import asyncio
import json
import os
import re
import subprocess
import sys
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple, Callable, TypedDict
from dataclasses import dataclass, field
import shutil

# --- Langchain and Chroma Imports ---
from langchain_core.documents import Document
from langchain_chroma import Chroma
from tongyiembedding import QwenEmbeddingFunction

# --- Configuration ---
CHROMA_PERSIST_DIRECTORY = "chroma_db_kag_recursive"
CHROMA_COLLECTION_NAME = "kag_recursive_documents"


# --- ChromaKnowledgeBase (ä¸æ‚¨æä¾›çš„ç‰ˆæœ¬åŸºæœ¬ä¸€è‡´) ---
class ChromaKnowledgeBase:
	def __init__(self, embedding_function: Callable, initial_documents: Optional[List[Document]] = None,
				 persist_directory: str = CHROMA_PERSIST_DIRECTORY, collection_name: str = CHROMA_COLLECTION_NAME,
				 force_rebuild: bool = False):
		print(f"  [ChromaKB] åˆå§‹åŒ–çŸ¥è¯†åº“: {persist_directory}, é›†åˆ: {collection_name}")
		self.embedding_function = embedding_function;
		self.persist_directory = persist_directory
		self.collection_name = collection_name;
		self.vectorstore: Optional[Chroma] = None
		if force_rebuild and os.path.exists(persist_directory):
			print(f"  [ChromaKB] force_rebuild=True, åˆ é™¤ç›®å½•: {persist_directory}")
			try:
				shutil.rmtree(persist_directory)
			except OSError as e:
				print(f"  [ChromaKB Error] åˆ é™¤ç›®å½•å¤±è´¥: {e}.")
		if os.path.exists(persist_directory) and not force_rebuild:
			print(f"  [ChromaKB] ä» '{persist_directory}' åŠ è½½å·²å­˜åœ¨å‘é‡åº“...")
			try:
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] æˆåŠŸåŠ è½½å‘é‡åº“ '{self.collection_name}'.")
			except Exception as e:
				print(f"  [ChromaKB Error] ä» '{persist_directory}' åŠ è½½å¤±è´¥: {e}. å°†å°è¯•æ–°å»ºã€‚")
				self.vectorstore = None
		if self.vectorstore is None:
			if initial_documents:
				print(f"  [ChromaKB] ä¸º {len(initial_documents)} ä¸ªæ–‡æ¡£æ„å»ºæ–°å‘é‡åº“...")
				self.vectorstore = Chroma.from_documents(documents=initial_documents,
														 embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] æ–°å‘é‡åº“æ„å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚")
			else:
				print(f"  [ChromaKB] æ— åˆå§‹æ–‡æ¡£ï¼Œåˆ›å»ºç©ºçš„æŒä¹…åŒ–é›†åˆã€‚")
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] ç©ºçš„æŒä¹…åŒ– Chroma é›†åˆ '{self.collection_name}' å·²å‡†å¤‡å°±ç»ªã€‚")

	def add_documents(self, documents: List[Document]):
		if not self.vectorstore:
			if documents:
				print(f"  [ChromaKB] Vectorstore ä¸ºç©º, å°è¯•ä»å½“å‰ {len(documents)} ä¸ªæ–‡æ¡£åˆ›å»º...")
				self.vectorstore = Chroma.from_documents(documents=documents, embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] åŸºäºæ–°æ–‡æ¡£åˆ›å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚");
				return
			else:
				print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–ä¸”æ— æ–‡æ¡£å¯æ·»åŠ ."); return
		if documents:
			print(f"  [ChromaKB] å‘é›†åˆ '{self.collection_name}' æ·»åŠ  {len(documents)} ä¸ªæ–°æ–‡æ¡£...")
			self.vectorstore.add_documents(documents);
			print(f"  [ChromaKB] æ–‡æ¡£æ·»åŠ å®Œæˆã€‚")

	def retrieve(self, query: str, top_k: int = 3, filter_dict: Optional[Dict] = None) -> List[Dict[str, Any]]:
		if not self.vectorstore: print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–."); return []
		try:
			if self.vectorstore._collection is None or self.vectorstore._collection.count() == 0:
				print("  [ChromaKB] çŸ¥è¯†åº“é›†åˆä¸ºç©ºæˆ–æœªæ­£ç¡®åŠ è½½ã€‚");
				return []
		except Exception as e:
			print(f"  [ChromaKB Warning] æ— æ³•è·å–é›†åˆè®¡æ•°: {e}")
		print(f"  [ChromaKB] æ£€ç´¢æŸ¥è¯¢ '{query}', top_k={top_k}, è¿‡æ»¤å™¨: {filter_dict}...")
		try:
			results_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k, filter=filter_dict)
		except Exception as e:
			print(f"  [ChromaKB Error] Chroma similarity search failed: {e}"); return []
		processed_results = [{"id": doc.metadata.get("id", f"retrieved_{i}"), "content": doc.page_content,
							  "metadata": doc.metadata, "score": float(score)}
							 for i, (doc, score) in enumerate(results_with_scores)]
		print(f"  [ChromaKB] æ£€ç´¢åˆ° {len(processed_results)} ä¸ªæ–‡æ¡£ã€‚")
		return processed_results


# --- LLM Client (OpenAIChatLLM - ä¸ä¹‹å‰ç›¸åŒ) ---
class OpenAIChatLLM:  # (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
	def __init__(self, model_name: Optional[str] = None, api_key: Optional[str] = None, base_url: Optional[str] = None):
		try:
			import openai
		except ImportError:
			raise ImportError("OpenAI library not found. `pip install openai`.")
		self.api_key = api_key or os.getenv("OPENAI_API_KEY")
		self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
		self.model_name = model_name or os.getenv("LLM_MODEL_NAME", "qwen-plus")
		if not self.api_key: raise ValueError("API key not found.")
		self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
		print(f"[OpenAIChatLLM] å®¢æˆ·ç«¯å°±ç»ª: æ¨¡å‹ {self.model_name}, URL {self.base_url or 'OpenAI default'}")

	async def _make_api_call(self, messages: List[Dict[str, str]], expect_json: bool = False, temperature: float = 0.1,
							 **kwargs) -> str:
		try:
			completion_params = {"model": self.model_name, "messages": messages, "temperature": temperature, **kwargs}
			if expect_json:
				if "dashscope" in (self.base_url or "").lower() and self.model_name.startswith("qwen"):
					completion_params["extra_body"] = {"result_format": "message"}
				else:
					completion_params["response_format"] = {"type": "json_object"}
			response = await asyncio.to_thread(self.client.chat.completions.create, **completion_params)
			content = response.choices[0].message.content;
			return content.strip() if content else ""
		except Exception as e:
			print(f"  [OpenAIChatLLM Error] APIè°ƒç”¨å¤±è´¥: {e}"); raise RuntimeError(f"LLM API call failed: {e}")

	async def generate(self, prompt_str: str, system_prompt_str: Optional[str] = None, temperature: float = 0.1,
					   **kwargs) -> str:
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		messages.append({"role": "user", "content": prompt_str})
		return await self._make_api_call(messages, expect_json=False, temperature=temperature, **kwargs)

	async def generate_structured_json(self, prompt_str: str, system_prompt_str: Optional[str] = None,
									   temperature: float = 0.0, **kwargs) -> Dict:  # Default temp 0.0 for planner
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		user_content = f"{prompt_str}\n\nè¯·ç¡®ä¿æ‚¨çš„å›å¤æ˜¯ä¸€ä¸ªåˆæ³•çš„ã€å•ç‹¬çš„JSONå¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡æœ¬æˆ–markdownæ ‡è®°ã€‚"
		messages.append({"role": "user", "content": user_content})
		response_str = await self._make_api_call(messages, expect_json=True, temperature=temperature, **kwargs)
		cleaned_response_str = response_str.strip()
		if cleaned_response_str.startswith("```json"): cleaned_response_str = cleaned_response_str[7:]
		if cleaned_response_str.endswith("```"): cleaned_response_str = cleaned_response_str[:-3]
		cleaned_response_str = cleaned_response_str.strip()
		try:
			return json.loads(cleaned_response_str)
		except json.JSONDecodeError as e:
			error_msg = f"LLM did not return valid JSON. Error: {e}. Raw: '{response_str}'";
			print(f"  [OpenAIChatLLM Error] {error_msg}");
			raise ValueError(error_msg)


# --- Prompts ---
class BasePrompt:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, template: str, variables: List[str]):
		self.template_str = template; self.variables = variables

	def format(self, **kwargs) -> str:
		formatted_prompt = self.template_str
		for var_name in self.variables:
			if var_name not in kwargs: raise ValueError(f"Missing variable: {var_name}")
			formatted_prompt = formatted_prompt.replace(f"${{{var_name}}}", str(kwargs[var_name]))
		return formatted_prompt


class PlannerPrompt(BasePrompt):  # REFINED: For deeper recursive-like planning
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½é«˜åº¦æ™ºèƒ½ä¸”æœ‰è¿œè§çš„AIä»»åŠ¡è§„åˆ’å’Œåæ€ä¸“å®¶ã€‚
    ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼šæ ¹æ®ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜ `${user_query}` å’Œå·²ç»æ‰§è¡Œçš„å†å²ä»»åŠ¡ `${task_history}`ï¼Œåˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
    ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„å†³ç­–å’Œè®¡åˆ’ï¼š

    **è¾“å‡ºæ ¼å¼:**
    ```json
    {
      "plan_status": "finished" | "cannot_proceed" | "requires_more_steps",
      "final_thought": "string (å¯¹å½“å‰æ•´ä½“è¿›å±•çš„æ¸…æ™°æ€è€ƒå’Œåˆ¤æ–­ï¼Œè§£é‡Šä½ çš„plan_statusã€‚ä¾‹å¦‚ï¼Œå¦‚æœrequires_more_stepsï¼Œè¯´æ˜è¿˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ä»¥åŠä¸‹ä¸€æ­¥çš„ç›®æ ‡)",
      "next_steps": [ // ä»…å½“ plan_status == "requires_more_steps" æ—¶éç©º
        {
          "id": "string (æ–°ä»»åŠ¡çš„å”¯ä¸€ID, ä¾‹å¦‚ task_iter2_step0, ç¡®ä¿ä¸å†å²IDä¸å†²çª)",
          "executor_name": "string (ä»å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©)",
          "task_description": "string (å¯¹æ­¤æ–°ä»»åŠ¡ç›®æ ‡çš„æ¸…æ™°ã€å…·ä½“ä¸­æ–‡æè¿°)",
          "logic_input": { /* å–å†³äº executor_name */ },
          "dependencies": ["string"] // ä¾èµ–çš„å†å²ä»»åŠ¡IDåˆ—è¡¨
        }
      ]
    }
    ```

    **æ ¸å¿ƒæŒ‡ä»¤ä¸æ€è€ƒé“¾ (Chain-of-Thought for Reflective Planning):**
    1.  **å›é¡¾ç›®æ ‡**: ç†è§£ç”¨æˆ·åŸå§‹é—®é¢˜ `${user_query}` çš„æœ€ç»ˆç›®æ ‡ã€‚
    2.  **å®¡è§†å†å² (`${task_history}`)**: 
        * å“ªäº›å­é—®é¢˜å·²å›ç­”ï¼Ÿç­”æ¡ˆ (`answer_summary`) æ˜¯ä»€ä¹ˆï¼Ÿ
        * **ã€å…³é”®æ£€æŸ¥ç‚¹ã€‘**: æœ€è¿‘çš„ `DeduceExecutor` æ­¥éª¤çš„ç»“æœä¸­ `is_sufficient` æ˜¯å¦ä¸º `false`ï¼Ÿå¦‚æœä¸º `false`ï¼Œå®ƒåˆ—å‡ºäº†å“ªäº›å…·ä½“çš„ `new_questions_or_entities`ï¼Ÿè¿™äº›æ˜¯è§£å†³é—®é¢˜çš„ã€æ ¸å¿ƒå¾…åŠçº¿ç´¢ã€‘ã€‚
        * `RetrievalExecutor` æ£€ç´¢åˆ°äº†ä»€ä¹ˆï¼Ÿè¿™äº›ä¿¡æ¯æ˜¯å¦å·²è¢«å……åˆ†åˆ†æï¼ˆä¾‹å¦‚ï¼Œæ˜¯å¦æ‰€æœ‰è¢«å¼•ç”¨çš„æ ‡å‡†ç¼–å·éƒ½å·²æŸ¥è¯¢å…¶ç»†èŠ‚ï¼‰ï¼Ÿ
    3.  **å·®è·è¯„ä¼°**: å¯¹æ¯”å·²çŸ¥ä¿¡æ¯å’Œç”¨æˆ·ç›®æ ‡ï¼Œè¿˜ç¼ºå°‘å“ªäº›ã€å…·ä½“ç»†èŠ‚ã€‘æˆ–ã€å¼•ç”¨çš„æ ‡å‡†/æ–‡ä»¶ï¼ˆå¦‚é€šåˆ™XXXXï¼‰çš„è¯¦ç»†å†…å®¹ã€‘ï¼Ÿ
    4.  **å†³ç­–åˆ¶å®š (`final_thought` å’Œ `plan_status`)**:
        * **å·²è§£å†³?** å¦‚æœæ‰€æœ‰å­é—®é¢˜éƒ½å·²è§£å†³ï¼Œä¿¡æ¯å……åˆ†ï¼Œå¹¶ä¸”æ‰€æœ‰åœ¨æ¨ç†ä¸­æåŠçš„å…³é”®æ ‡å‡†/æ–‡ä»¶ï¼ˆä¾‹å¦‚â€œé€šåˆ™0104â€ï¼Œâ€œé€šåˆ™1107â€ï¼‰éƒ½ã€å·²ç»è·å¾—äº†è¯¦ç»†å†…å®¹å¹¶è¢«åˆ†æè¿‡ã€‘ï¼Œåˆ™è§„åˆ’ä¸€ä¸ª `FinishAction` ä»»åŠ¡ï¼Œå¹¶å°† `plan_status: "finished"`ã€‚
        * **æ— æ³•è§£å†³?** è‹¥å…³é”®ä¿¡æ¯ç¼ºå¤±ï¼Œä¸”å†å²ä¸­çš„ `new_questions_or_entities` å·²å°è¯•æ£€ç´¢ä½†æ— æœï¼Œæˆ–å·¥å…·æ— æ³•è·å–ï¼Œåˆ™ `plan_status: "cannot_proceed"`ã€‚
        * **éœ€è¦æ›´å¤šæ­¥éª¤?** å¦åˆ™ `plan_status: "requires_more_steps"`ã€‚
    5.  **è¡ŒåŠ¨è§„åˆ’ (`next_steps` - å¦‚æœ `requires_more_steps`)**:
        * **ã€æœ€é«˜ä¼˜å…ˆçº§ï¼šå¤„ç†æ˜ç¡®çš„ `new_questions_or_entities`ã€‘**: å¦‚æœå†å²ä¸­æœ‰ `DeduceExecutor` çš„ç»“æœæŒ‡å‡ºäº† `new_questions_or_entities`ï¼ˆä¾‹å¦‚ "é€šåˆ™1107çš„è¯¦ç»†å†…å®¹"ï¼‰ï¼Œå¹¶ä¸”è¿™äº›æ¡ç›®ã€å°šæœªã€‘åœ¨åç»­æ­¥éª¤ä¸­è¢«ä¸“é—¨æ£€ç´¢å’Œåˆ†æè¿‡ï¼š
            * **ç«‹å³è§„åˆ’**ä¸€ä¸ª `RetrievalExecutor` ä»»åŠ¡æ¥æŸ¥æ‰¾è¿™äº›å…·ä½“çš„æ¡ç›®ã€‚`logic_input.query` åº”ç›´æ¥é’ˆå¯¹è¿™äº›æ¡ç›®ï¼Œä¾‹å¦‚ â€œè·å–ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107ä¸­å…³äºå¾®ç”Ÿç‰©é™åº¦çš„è¯¦ç»†è§„å®šâ€ã€‚
            * å¦‚æœå·²æœ‰åˆæ­¥æ£€ç´¢ç»“æœä½†éœ€è¦è¿›ä¸€æ­¥åˆ†æï¼Œå¯è§„åˆ’ `DeduceExecutor`ã€‚
        * **æ•´åˆä¿¡æ¯**: å¦‚æœä¸Šä¸€æ­¥æ˜¯æ£€ç´¢ï¼Œå¹¶ä¸”æ£€ç´¢åˆ°äº†æ–°ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥é€šå¸¸æ˜¯è§„åˆ’ `DeduceExecutor` ä»»åŠ¡ï¼Œå…¶ `logic_input.reasoning_goal` è®¾ä¸ºâ€œæ•´åˆæ–°æ£€ç´¢åˆ°çš„å…³äº[æ–°å®ä½“/æ ‡å‡†]çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆå…ˆå‰å…³äº[åŸå§‹ä¸»é¢˜]çš„ç»“è®º...â€ã€‚`logic_input.context_data` åº”ç²¾ç¡®å¼•ç”¨æ–°çš„æ£€ç´¢ç»“æœ (`{{new_retrieval_task_id.result}}`) å’Œç›¸å…³çš„å…ˆå‰ç»“è®ºã€‚
        * **åˆå§‹è§„åˆ’/æ¢ç´¢æ€§æ£€ç´¢**: å¦‚æœå†å²ä¸ºç©ºï¼Œæˆ–æ²¡æœ‰æ˜ç¡®çš„ `new_questions_or_entities`ï¼Œåˆ™æ ¹æ®å¯¹ç”¨æˆ·é—®é¢˜çš„ç†è§£è¿›è¡Œåˆæ­¥çš„ `RetrievalExecutor` è§„åˆ’ã€‚
        * **èšç„¦**: é€šå¸¸ä¸€æ¬¡åªè§„åˆ’1ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼ˆå¯èƒ½æ˜¯æ£€ç´¢+æ¼”ç»ï¼Œæˆ–åªæ˜¯ä¸€ä¸ªå…³é”®æ£€ç´¢ï¼‰ã€‚
    6.  **ID å’Œä¾èµ–**: ä¸ºæ–°ä»»åŠ¡åˆ†é…å”¯ä¸€çš„ `id`ã€‚æ­£ç¡®è®¾ç½® `dependencies`ã€‚
    7.  **ä¸¥æ ¼JSONè¾“å‡º**ã€‚
    """
	USER_TEMPLATE = """
    --- å¯ç”¨å·¥å…· ---
    ${available_executors_description}
    --- å¯ç”¨å·¥å…·ç»“æŸ ---

    --- å†å²ä»»åŠ¡åŠç»“æœ (æœ€è¿‘çš„æ­¥éª¤åœ¨æœ€å) ---
    ${task_history}
    --- å†å²ä»»åŠ¡åŠç»“æœç»“æŸ ---

    --- ç”¨æˆ·åŸå§‹é—®é¢˜ ---
    "${user_query}"
    --- ç”¨æˆ·åŸå§‹é—®é¢˜ç»“æŸ ---

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œåæ€å¹¶è§„åˆ’ã€‚è¾“å‡ºJSONå¯¹è±¡:
    """

	def __init__(self): super().__init__(self.USER_TEMPLATE,
										 ["user_query", "available_executors_description", "task_history"])


class DeducePrompt(BasePrompt):  # REFINED: Stronger guidance for new_questions
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„AIæ¨ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„â€œä¸Šä¸‹æ–‡ä¿¡æ¯â€ï¼Œç²¾ç¡®åœ°å›ç­”æˆ–å®Œæˆâ€œæ¨ç†ç›®æ ‡â€ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„ç»“è®ºå’Œè¯„ä¼°ï¼š
    ```json
    {
      "answer_summary": "string (å¯¹æ¨ç†ç›®æ ‡çš„ç›´æ¥ã€ç®€æ´çš„å›ç­”æˆ–æ€»ç»“ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜å½“å‰å·²çŸ¥ä»€ä¹ˆï¼Œã€å¹¶æ¸…æ™°æŒ‡å‡ºå…·ä½“è¿˜ç¼ºå°‘å“ªäº›ä¿¡æ¯æ‰èƒ½å®Œæ•´å›ç­”æ¨ç†ç›®æ ‡ã€‘)",
      "is_sufficient": boolean (true å¦‚æœä½ è®¤ä¸ºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€å·²åŒ…å«æ‰€æœ‰å¿…è¦çš„ç»†èŠ‚ï¼Œæ— éœ€ä»»ä½•è¡¥å……æŸ¥è¯¢ã€‘æ¥å®Œå…¨å›ç­”æ¨ç†ç›®æ ‡ï¼Œå¦åˆ™ false),
      "new_questions_or_entities": [
        "string" // ã€è‡³å…³é‡è¦ã€‘å¦‚æœ is_sufficient ä¸º falseï¼Œè¿™é‡Œã€å¿…é¡»ã€‘åˆ—å‡ºä¸ºäº†è·å¾—ã€ç¼ºå¤±çš„å…³é”®ç»†èŠ‚ã€‘æˆ–ã€å®Œæ•´ç†è§£ä¸Šä¸‹æ–‡ä¸­é¦–æ¬¡æåŠä¸”æœªå±•å¼€è¯´æ˜çš„æ ‡å‡†/æ–‡ä»¶ç¼–å·/æ ¸å¿ƒå®ä½“ã€‘ï¼Œéœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢çš„ã€éå¸¸å…·ä½“çš„ã€å¯ç›´æ¥ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢è¯æˆ–é—®é¢˜ã€‘ã€‚ä¾‹å¦‚ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¸­æåˆ°â€œå¾®ç”Ÿç‰©é™åº¦åº”ç¬¦åˆé€šåˆ™1107â€ï¼Œä½†æœªç»™å‡ºé€šåˆ™1107å†…å®¹ï¼Œåˆ™åº”åœ¨æ­¤åˆ—å‡ºâ€œé€šåˆ™1107ä¸­å…³äºå¾®ç”Ÿç‰©é™åº¦çš„å…·ä½“æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿâ€æˆ–â€œæŸ¥æ‰¾ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107çš„è¯¦ç»†å†…å®¹â€ã€‚å¦‚æœä¿¡æ¯å®Œå…¨å……åˆ†ï¼Œåˆ™ä¸ºç©ºåˆ—è¡¨[]ã€‚
      ]
    }
    ```
    - ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–markdownæ ‡è®°ï¼Œåªè¾“å‡ºJSONå¯¹è±¡ã€‚
    """
	USER_TEMPLATE = "æ¨ç†ç›®æ ‡:\n${reasoning_goal}\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n${context_data}\n\nè¯·è¾“å‡ºJSONæ ¼å¼çš„æ¨ç†ç»“æœ:"

	def __init__(self): super().__init__(self.USER_TEMPLATE, ["reasoning_goal", "context_data"])


class CodeExecutionPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ)
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ©æ‰‹ã€‚"
	USER_TEMPLATE = "è¯·æ ¹æ®ä»¥ä¸‹æŒ‡ä»¤å’Œç›¸å…³æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µPythonä»£ç æ¥è§£å†³é—®é¢˜ã€‚\nä»£ç å¿…é¡»é€šè¿‡ `print()` è¾“å‡ºå…¶æœ€ç»ˆè®¡ç®—ç»“æœã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¾“å‡ºçº¯ä»£ç ã€‚\n\næŒ‡ä»¤:\n${code_generation_prompt}\n\nç›¸å…³æ•°æ® (å¦‚æœæä¾›):\n${relevant_data}\n\nç”Ÿæˆçš„Pythonä»£ç  (è¯·ç¡®ä¿å®ƒåªåŒ…å«ä»£ç æœ¬èº«ï¼Œå¹¶ç”¨print()è¾“å‡ºç»“æœ):"

	def __init__(self): super().__init__(self.USER_TEMPLATE, ["code_generation_prompt", "relevant_data"])


class UserProvidedReferGeneratorPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, language: str = "zh"):
		try:
			from kag.common.utils import get_now
		except ImportError:
			def get_now(language='zh'):
				return "å½“å‰æ—¥æœŸ"
		# print("[UserProvidedReferGeneratorPrompt] Warning: kag.common.utils.get_now not found.")
		self.template_zh = (
					f"ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†æä¸“å®¶ï¼Œä»Šå¤©æ˜¯{get_now(language='zh')}ã€‚" + "åŸºäºç»™å®šçš„å¼•ç”¨ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\nè¾“å‡ºç­”æ¡ˆï¼Œå¦‚æœç­”æ¡ˆä¸­å­˜åœ¨å¼•ç”¨ä¿¡æ¯ï¼Œåˆ™éœ€è¦referenceçš„idå­—æ®µï¼Œå¦‚æœä¸æ˜¯æ£€ç´¢ç»“æœï¼Œåˆ™ä¸éœ€è¦æ ‡è®°å¼•ç”¨\nè¾“å‡ºæ—¶ï¼Œä¸éœ€è¦é‡å¤è¾“å‡ºå‚è€ƒæ–‡çŒ®\nå¼•ç”¨è¦æ±‚ï¼Œä½¿ç”¨ç±»ä¼¼<reference id=\"chunk:1_2\"></reference>è¡¨ç¤º\nå¦‚æœæ ¹æ®å¼•ç”¨ä¿¡æ¯æ— æ³•å›ç­”ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹å†…çš„çŸ¥è¯†å›ç­”ï¼Œä½†æ˜¯å¿…é¡»é€šè¿‡åˆé€‚çš„æ–¹å¼æç¤ºç”¨æˆ·ï¼Œæ˜¯åŸºäºæ£€ç´¢å†…å®¹è¿˜æ˜¯å¼•ç”¨æ–‡æ¡£\nç¤ºä¾‹1ï¼š\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ï¼š\næ ¹æ®å¸¸è¯†å²³çˆ¶æ˜¯å¦»å­çš„çˆ¸çˆ¸ï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆæ‰¾åˆ°å¼ ä¸‰çš„å¦»å­ï¼Œç„¶åæ‰¾åˆ°å¦»å­çš„çˆ¸çˆ¸\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'\nreferenceï¼š\n[\n{\n    \"content\": \"å¼ ä¸‰ å¦»å­ ç‹äº”\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_1\"\n},\n{\n    \"content\": \"ç‹äº” çˆ¶äº² ç‹å››\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_2\"\n}\n]'\né—®é¢˜ï¼š'å¼ ä¸‰çš„å²³çˆ¶æ˜¯è°ï¼Ÿ'\n\nå¼ ä¸‰çš„å¦»å­æ˜¯ç‹äº”<reference id=\"chunk:1_1\"></reference>ï¼Œè€Œç‹äº”çš„çˆ¶äº²æ˜¯ç‹å››<reference id=\"chunk:1_2\"></reference>ï¼Œæ‰€ä»¥å¼ ä¸‰çš„å²³çˆ¶æ˜¯ç‹å››\n\n\nè¾“å‡ºè¯­è°ƒè¦æ±‚é€šé¡ºï¼Œä¸è¦æœ‰æœºæ¢°æ„Ÿï¼Œè¾“å‡ºçš„è¯­è¨€è¦å’Œé—®é¢˜çš„è¯­è¨€ä¿æŒä¸€è‡´\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š'${summary_of_executed_steps}'\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'${formatted_references}'\né—®é¢˜ï¼š'${user_query}'")
		self.template_en = self.template_zh
		current_template = self.template_zh if language == "zh" else self.template_en
		super().__init__(current_template, ["summary_of_executed_steps", "user_query", "formatted_references"])

	def format(self, summary_of_executed_steps: str, user_query: str, retrieved_references: List[Dict]) -> str:
		ref_list_for_prompt = []
		for i, ref_item in enumerate(retrieved_references): ref_list_for_prompt.append(
			{"content": ref_item.get("content", ""),
			 "document_name": ref_item.get("metadata", {}).get("source_name", f"æ£€ç´¢æ–‡æ¡£{i + 1}"),
			 "id": ref_item.get("metadata", {}).get("id", f"retrieved_chunk_{i}")})
		formatted_references_str = json.dumps(ref_list_for_prompt, ensure_ascii=False, indent=2)
		return super().format(summary_of_executed_steps=summary_of_executed_steps, user_query=user_query,
							  formatted_references=formatted_references_str)


# --- DataStructures & ContextManager (ä¸ä¹‹å‰ç›¸åŒ) ---
LogicInput = Dict[str, Any]


@dataclass
class Task: id: str; executor_name: str; task_description: str; logic_input: LogicInput; dependencies: List[
	str] = field(default_factory=list); status: str = "pending"; result: Optional[Any] = None; thought: Optional[
	str] = None


class ContextManager:  # (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒï¼ŒåŒ…æ‹¬ get_task_history_for_prompt, get_summary_for_generator, collect_retrieved_references_for_generator)
	def __init__(self, user_query: str):
		self.user_query = user_query; self.tasks: Dict[str, Task] = {}; self.execution_order: List[str] = []

	def add_task_from_planner(self, task_data: Dict, base_id_prefix: str, iter_num: int, step_in_iter: int) -> Task:
		llm_id = task_data.get("id");
		task_id = llm_id if llm_id and isinstance(llm_id,
												  str) and llm_id.strip() else f"{base_id_prefix}_iter{iter_num}_step{step_in_iter}"
		orig_id = task_id;
		ctr = 0
		while task_id in self.tasks: ctr += 1; task_id = f"{orig_id}_v{ctr}"
		if ctr > 0: print(f"  [CM Warn] Task ID '{orig_id}' conflict. Renamed '{task_id}'.")
		task = Task(id=task_id, executor_name=task_data["executor_name"],
					task_description=task_data["task_description"], logic_input=task_data["logic_input"],
					dependencies=task_data.get("dependencies", []));
		self.tasks[task.id] = task;
		self.execution_order.append(task.id);
		return task

	def get_task(self, task_id: str) -> Optional[Task]:
		return self.tasks.get(task_id)

	def update_task_status(self, task_id: str, status: str, result: Optional[Any] = None,
						   thought: Optional[str] = None):
		task = self.get_task(task_id)
		if task:
			task.status = status; task.result = result if result is not None else task.result; task.thought = (
																														  task.thought or "") + f"\n{thought}".strip() if thought and (
						task.thought or "") else thought or task.thought
		else:
			print(f"  [CM Warn] Task ID {task_id} not found for status update.")

	def get_task_history_for_prompt(self) -> str:
		hist = [];
		truncate_len = 100
		for tid in self.execution_order:
			t = self.get_task(tid)
			if t and t.status in ["completed", "failed"]:
				res_disp = [];
				if t.status == "completed":
					if isinstance(t.result, dict) and t.executor_name == "DeduceExecutor":
						d_out = t.result;
						res_disp.append(
							f"    æ¨ç†æ€»ç»“: {str(d_out.get('answer_summary', 'N/A'))[:truncate_len]}{'...' if len(str(d_out.get('answer_summary', 'N/A'))) > truncate_len else ''}")
						res_disp.append(f"    ä¿¡æ¯æ˜¯å¦å……åˆ†: {d_out.get('is_sufficient', True)}")
						nq = d_out.get('new_questions_or_entities', []);
						if nq: res_disp.append(
							f"    å»ºè®®è¿›ä¸€æ­¥æŸ¥è¯¢: {', '.join(nq)[:truncate_len]}{'...' if len(', '.join(nq)) > truncate_len else ''}")
					elif isinstance(t.result, list) and t.executor_name == "RetrievalExecutor":
						res_disp.append(
							f"    æ£€ç´¢åˆ° {len(t.result)} ç‰‡æ®µ. å†…å®¹(æ‘˜è¦): {str(t.result[0].get('content') if t.result else 'ç©º')[:truncate_len - 20]}...")
					else:
						res_s = str(t.result); res_disp.append(
							f"    ç»“æœ: {res_s[:truncate_len]}{'...' if len(res_s) > truncate_len else ''}")
				else:
					res_disp.append(f"    æ‰§è¡Œå¤±è´¥: {str(t.result)[:truncate_len]}...")
				res_final_disp = "\n".join(res_disp);
				th_s = str(t.thought or "N/A");
				th_s = th_s[:truncate_len] + "..." if len(th_s) > truncate_len else th_s
				hist.append(
					f"  - Task ID: {t.id}\n    Desc: {t.task_description}\n    Exec: {t.executor_name}\n    Status: {t.status}\n{res_final_disp}\n    Thought: {th_s}")
		return "\n\n".join(hist) if hist else "å°šæœªæ‰§è¡Œä»»ä½•å†å²ä»»åŠ¡ã€‚"

	def get_summary_for_generator(self) -> str:  # (ä¸ä¹‹å‰ç›¸åŒ)
		summary_parts = []
		for i, task_id in enumerate(self.execution_order):
			task = self.get_task(task_id)
			if task:
				result_str = "N/A";
				thought_str = str(task.thought or 'æœªè®°å½•æ€è€ƒè¿‡ç¨‹')
				if task.status == 'completed':
					if isinstance(task.result, dict) and task.executor_name == "DeduceExecutor":
						result_str = f"æ¨ç†æ€»ç»“: {task.result.get('answer_summary', 'N/A')}, ä¿¡æ¯æ˜¯å¦å……åˆ†: {task.result.get('is_sufficient')}" + (
							f", å»ºè®®æ¢ç©¶: {task.result['new_questions_or_entities']}" if task.result.get(
								'new_questions_or_entities') else "")
					elif isinstance(task.result, list) and task.executor_name == "RetrievalExecutor":
						result_str = f"æ£€ç´¢åˆ° {len(task.result)} ä¸ªç›¸å…³ç‰‡æ®µã€‚"
					elif isinstance(task.result, str):
						result_str = task.result
					else:
						result_str = f"å¤æ‚ç±»å‹ç»“æœ (æ‘˜è¦: {str(task.result)[:100]}...)"
				elif task.status == 'failed':
					result_str = f"æ‰§è¡Œå¤±è´¥: {str(task.result)[:150]}..."
				elif task.status == 'skipped':
					result_str = "å› ä¾èµ–å¤±è´¥æˆ–æ¡ä»¶ä¸æ»¡è¶³è€Œè·³è¿‡ã€‚"
				else:
					result_str = f"å½“å‰çŠ¶æ€: {task.status}"
				result_str_summary = result_str[:250] + "..." if len(result_str) > 250 else result_str
				thought_str_summary = thought_str[:200] + "..." if len(thought_str) > 200 else thought_str
				summary_parts.append(
					f"æ­¥éª¤ {i + 1} (ID: {task.id}):\n  ç›®æ ‡: {task.task_description}\n  æ‰§è¡Œå·¥å…·: {task.executor_name}\n  æ‰§è¡Œæ€è€ƒ: {thought_str_summary}\n  äº§å‡º/çŠ¶æ€: {result_str_summary}")
		if not summary_parts: return "æœªèƒ½æ‰§è¡Œä»»ä½•æ­¥éª¤ï¼Œæˆ–æ²¡æœ‰å¯æ€»ç»“çš„äº§å‡ºã€‚"
		return "\n\n".join(summary_parts)

	def collect_retrieved_references_for_generator(self) -> List[Dict]:  # (ä¸ä¹‹å‰ç›¸åŒ)
		refs = [];
		for tid in self.execution_order:
			t = self.get_task(tid)
			if t and t.executor_name == "RetrievalExecutor" and t.status == "completed" and isinstance(t.result, list):
				for item in t.result:
					if isinstance(item, dict) and "content" in item: refs.append({"content": item["content"],
																				  "document_name": item.get("metadata",
																											{}).get(
																					  "source_name", f"æº_{t.id}"),
																				  "id": item.get("metadata", {}).get(
																					  "id", f"ref_{t.id}_{len(refs)}")})
		return refs


# --- Executors (DeduceExecutor.execute and _resolve_references slightly refined) ---
class ExecutorError(Exception): pass


class ExecutorBase(ABC):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: Optional[OpenAIChatLLM] = None):
		self.llm_client = llm_client

	@abstractmethod
	async def execute(self, task: Task, context: ContextManager) -> Any:
		pass

	@abstractmethod
	def get_schema(self) -> Dict[str, Any]:
		pass

	def _resolve_references(self, data_template: Any, context: ContextManager) -> Any:  # REFINED for structured result
		if isinstance(data_template, str):
			def replace_match(match):
				ref_full = match.group(1).strip();
				task_id_ref, attr_ref = ref_full.split('.', 1) if '.' in ref_full else (ref_full, "result")
				ref_task = context.get_task(task_id_ref)
				if ref_task and ref_task.status == "completed":
					target_obj = ref_task.result
					if attr_ref == "result":  # Default
						if isinstance(target_obj, dict) and "answer_summary" in target_obj: return str(
							target_obj["answer_summary"])  # For DeduceExecutor structured output
						if isinstance(target_obj, list): return "\n".join(
							[f"- {item}" for item in map(str, target_obj)])
						return str(target_obj)
					elif isinstance(target_obj,
									dict) and attr_ref in target_obj:  # Allow referencing specific keys in a dict result
						return str(target_obj[attr_ref])
					else:
						print(
							f"  [Exec Warn] Unsupported attr '{attr_ref}' or not found in result for {{ {ref_full} }}.")
				else:
					print(
						f"  [Exec Warn] Cannot resolve ref {{ {ref_full} }}. Task '{task_id_ref}' status: {ref_task.status if ref_task else 'not_found'}.")
				return f"{{å¼•ç”¨é”™è¯¯: {ref_full}}}"

			return re.sub(r"\{\{([\w_\-\d\.]+)\}\}", replace_match, data_template)
		elif isinstance(data_template, dict):
			return {k: self._resolve_references(v, context) for k, v in data_template.items()}
		elif isinstance(data_template, list):
			return [self._resolve_references(item, context) for item in data_template]
		return data_template


class RetrievalExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, kb: ChromaKnowledgeBase):
		super().__init__(); self.kb = kb

	async def execute(self, task: Task, context: ContextManager) -> List[Dict[str, Any]]:
		logic_input = task.logic_input;
		query_to_retrieve = logic_input.get("query");
		filter_param = logic_input.get("filter")
		if not query_to_retrieve or not isinstance(query_to_retrieve, str): raise ExecutorError(
			"RetrievalExecutor: 'query' (string) is required.")
		resolved_query = self._resolve_references(query_to_retrieve, context);
		actual_filter = None
		if filter_param and isinstance(filter_param, dict) and filter_param:
			actual_filter = filter_param
		elif filter_param:
			print(f"  [RetrievalExec Warn] Invalid filter for task '{task.id}': {filter_param}. Ignoring.")
		task.thought = (task.thought or "") + f"KBæ£€ç´¢æŸ¥è¯¢: '{resolved_query}', è¿‡æ»¤å™¨: {actual_filter}.".strip()
		retrieved_docs_with_meta = self.kb.retrieve(resolved_query, top_k=3, filter_dict=actual_filter)
		if not retrieved_docs_with_meta: task.thought += "\næœªæ£€ç´¢åˆ°ä»»ä½•åŒ¹é…æ–‡æ¡£."; return []
		task.thought += f"\næ£€ç´¢åˆ° {len(retrieved_docs_with_meta)} ä¸ªæ–‡æ¡£å¯¹è±¡.";
		return retrieved_docs_with_meta

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "RetrievalExecutor",
				"description": "ä»å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚å¯æŒ‡å®šå…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚",
				"logic_input_schema": {"query": "string (æ£€ç´¢æŸ¥è¯¢è¯­å¥, å¯å¼•ç”¨ {{task_id.result}})",
									   "filter": "dict (å¯é€‰, ChromaDBå…ƒæ•°æ®è¿‡æ»¤å™¨)"}}


class DeduceExecutorOutput(TypedDict, total=False):  # For type hinting
	answer_summary: str;
	is_sufficient: bool;
	new_questions_or_entities: List[str];
	raw_llm_response: str


class DeduceExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„DeducePrompt)
	def __init__(self, llm_client: OpenAIChatLLM, prompt_template: DeducePrompt,
				 specialized_prompts: Optional[Dict[str, BasePrompt]] = None):
		super().__init__(llm_client);
		self.default_prompt_template = prompt_template;
		self.specialized_prompts = specialized_prompts or {}

	async def execute(self, task: Task, context: ContextManager) -> DeduceExecutorOutput:
		logic_input = task.logic_input;
		goal = logic_input.get("reasoning_goal");
		raw_ctx_data = logic_input.get("context_data");
		op_type = logic_input.get("operation_type")
		if not goal or not isinstance(goal, str) or raw_ctx_data is None: raise ExecutorError(
			"DeduceExecutor: 'reasoning_goal'(str) & 'context_data' required.")
		res_ctx_data = self._resolve_references(raw_ctx_data, context)
		ctx_data_str = json.dumps(res_ctx_data, ensure_ascii=False, indent=2) if isinstance(res_ctx_data,
																							(list, dict)) else str(
			res_ctx_data)
		prompt_to_use = self.specialized_prompts.get(op_type,
													 self.default_prompt_template) if op_type else self.default_prompt_template
		sys_prompt = getattr(prompt_to_use, "SYSTEM_PROMPT", DeducePrompt.SYSTEM_PROMPT)
		if op_type and prompt_to_use != self.default_prompt_template: print(
			f"  [DeduceExecutor] Using specialized prompt for op_type: {op_type}")
		prompt_str = prompt_to_use.format(reasoning_goal=goal, context_data=ctx_data_str)
		task.thought = (
								   task.thought or "") + f"æ¼”ç»ç›®æ ‡({op_type or 'default'}): {goal}. ä¸Šä¸‹æ–‡(æ‘˜è¦): {ctx_data_str[:100]}...".strip()
		resp_json = await self.llm_client.generate_structured_json(prompt_str, system_prompt_str=sys_prompt,
																   temperature=0.0)
		summary = resp_json.get("answer_summary", "æœªèƒ½ä»LLMå“åº”ä¸­è§£æå‡ºç­”æ¡ˆæ€»ç»“ã€‚");
		is_suff = bool(resp_json.get("is_sufficient", False))
		new_qs_raw = resp_json.get("new_questions_or_entities", []);
		new_qs = [str(item).strip() for item in new_qs_raw if
				  isinstance(item, str) and str(item).strip()] if isinstance(new_qs_raw, list) else [
			str(new_qs_raw).strip()] if isinstance(new_qs_raw, str) and str(new_qs_raw).strip() else []
		task.thought += f"\nLLMæ¼”ç»å“åº”(ç»“æ„åŒ–): sufficient={is_suff}, new_qs={new_qs}, summary_preview='{summary[:50]}...'"
		output: DeduceExecutorOutput = {"answer_summary": summary, "is_sufficient": is_suff,
										"new_questions_or_entities": new_qs,
										"raw_llm_response": json.dumps(resp_json, ensure_ascii=False)};
		return output

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "DeduceExecutor",
				"description": "åŸºäºä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ã€æ€»ç»“ã€åˆ¤æ–­æˆ–æŠ½å–ã€‚ä¼šåˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……åˆ†å¹¶ç»™å‡ºä¸‹ä¸€æ­¥æŸ¥è¯¢å»ºè®®ã€‚",
				"logic_input_schema": {"reasoning_goal": "string (å…·ä½“æ¨ç†ç›®æ ‡)",
									   "context_data": "any (æ¨ç†æ‰€éœ€ä¸Šä¸‹æ–‡,å¯å¼•ç”¨ {{task_id.result}})",
									   "operation_type": "string (å¯é€‰, å¦‚ summarize, extract_info)"}}


class CodeExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: CodeExecutionPrompt):
		super().__init__(llm_client); self.prompt_template = prompt

	async def execute(self, task: Task, context: ContextManager) -> str:
		logic_input = task.logic_input;
		code_gen_prompt = logic_input.get("code_generation_prompt");
		rel_data = logic_input.get("relevant_data", "")
		if not code_gen_prompt or not isinstance(code_gen_prompt, str): raise ExecutorError(
			"CodeExecutor: 'code_generation_prompt' required.")
		res_code_prompt = self._resolve_references(code_gen_prompt, context);
		res_rel_data = self._resolve_references(rel_data, context)
		rel_data_prompt = json.dumps(res_rel_data, ensure_ascii=False, indent=2) if isinstance(res_rel_data,
																							   (list, dict)) else str(
			res_rel_data) if res_rel_data else ""
		llm_prompt = self.prompt_template.format(code_generation_prompt=res_code_prompt, relevant_data=rel_data_prompt)
		task.thought = (task.thought or "") + f"ä»£ç ç”Ÿæˆç›®æ ‡: {res_code_prompt}. ".strip()
		code_block = await self.llm_client.generate(llm_prompt, system_prompt_str=CodeExecutionPrompt.SYSTEM_PROMPT)
		code = code_block.strip();
		if code.startswith("```python"): code = code[9:]
		if code.startswith("```"): code = code[3:]
		if code.endswith("```"): code = code[:-3]
		code = code.strip()
		if not code: task.thought += "\nLLMæœªèƒ½ç”Ÿæˆä»£ç ."; raise ExecutorError("CodeExecutor: LLM no code.")
		task.thought += f"\nç”Ÿæˆçš„ä»£ç :\n---\n{code}\n---"
		try:
			with open("t.py", "w", encoding="utf-8") as f:
				f.write(code)
			p = await asyncio.to_thread(subprocess.run, [sys.executable, "t.py"], capture_output=True, text=True,
										timeout=10, check=False)
			if p.returncode != 0: task.thought += f"\nä»£ç é”™è¯¯ç {p.returncode}. stderr:\n{p.stderr or 'æ— '}"; raise ExecutorError(
				f"Code exec err {p.returncode}:\n{p.stderr or 'æ— '}")
			out = p.stdout.strip();
			task.thought += f"\nä»£ç è¾“å‡º: {out}";
			return out
		except subprocess.TimeoutExpired:
			task.thought += "\nä»£ç è¶…æ—¶."; raise ExecutorError("Code timeout.")
		except Exception as e:
			task.thought += f"\nä»£ç æœ¬åœ°æ‰§è¡Œé”™è¯¯: {e}"; raise ExecutorError(f"Code local exec err: {e}")
		finally:
			if os.path.exists("t.py"): os.remove("t.py")

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "CodeExecutor",
				"description": "ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ã€‚ä»£ç åº”print()ç»“æœã€‚è¾“å…¥å¯å¼•ç”¨ {{task_id.result}}ã€‚",
				"logic_input_schema": {"code_generation_prompt": "string (ä»£ç ç”ŸæˆæŒ‡ä»¤)",
									   "relevant_data": "any (å¯é€‰, ä»£ç æ‰€éœ€æ•°æ®)"}}


class FinishExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	async def execute(self, task: Task,
					  context: ContextManager) -> str: task.thought = "æ”¶åˆ°FinishæŒ‡ä»¤ï¼Œæµç¨‹ç»“æŸã€‚"; print(
		f"  [FinishExecutor] Task {task.id} executed."); return "å·²å®Œæˆæ‰€æœ‰å¿…è¦æ­¥éª¤ã€‚"

	def get_schema(self) -> Dict[str, Any]: return {"name": "FinishAction",
													"description": "å½“é—®é¢˜å·²è§£å†³æˆ–æ— æ³•ç»§ç»­æ—¶è°ƒç”¨æ­¤åŠ¨ä½œç»“æŸè§„åˆ’ã€‚",
													"logic_input_schema": {"reason": "string (å¯é€‰ï¼Œç»“æŸåŸå› )"}}


# --- Planner (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„PlannerPrompt) ---
class Planner:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: PlannerPrompt):
		self.llm_client = llm_client; self.prompt_template = prompt

	async def plan_next_steps(self, user_query: str, context: ContextManager, available_executors: List[Dict]) -> Tuple[
		str, str, List[Dict]]:
		exec_desc_parts = [
			f"  - åç§°: \"{s['name']}\"\n    æè¿°: \"{s['description']}\"\n    è¾“å…¥å‚æ•°æ¨¡å¼ (logic_input_schema): {json.dumps(s.get('logic_input_schema', 'N/A'), ensure_ascii=False)}"
			for s in available_executors]
		exec_desc = "\n".join(exec_desc_parts);
		history_str = context.get_task_history_for_prompt()
		user_prompt_str = self.prompt_template.format(user_query=user_query, available_executors_description=exec_desc,
													  task_history=history_str)
		response_json = await self.llm_client.generate_structured_json(user_prompt_str,
																	   system_prompt_str=PlannerPrompt.SYSTEM_PROMPT,
																	   temperature=0.0)
		plan_status = response_json.get("plan_status", "error");
		final_thought = response_json.get("final_thought", "LLMæœªèƒ½æä¾›è§„åˆ’æ€è€ƒã€‚");
		next_steps_data = response_json.get("next_steps", [])
		if not isinstance(next_steps_data, list): print(
			f"  [Planner Err] LLM 'next_steps' not list. Got: {next_steps_data}. Assuming none."); next_steps_data = []
		valid_steps = [td for td in next_steps_data if isinstance(td, dict) and all(
			k in td for k in ["id", "executor_name", "task_description", "logic_input"])]
		if len(valid_steps) != len(next_steps_data): print(f"  [Planner Warn] Some planned steps were invalid.")
		return plan_status, final_thought, valid_steps


# --- AnswerGenerator (ä¸ä¹‹å‰ç›¸åŒ) ---
class AnswerGenerator:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM,
				 prompt: UserProvidedReferGeneratorPrompt): self.llm_client = llm_client; self.prompt_template = prompt

	async def generate_final_answer(self, user_query: str, context: ContextManager) -> str:
		summary = context.get_summary_for_generator();
		refs = context.collect_retrieved_references_for_generator()
		prompt_str = self.prompt_template.format(summary_of_executed_steps=summary, user_query=user_query,
												 retrieved_references=refs)
		return await self.llm_client.generate(prompt_str)


# --- Pipeline (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„è¿­ä»£é€»è¾‘) ---
class IterativePipeline:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, planner: Planner, executors: Dict[str, ExecutorBase], generator: AnswerGenerator,
				 max_iterations: int = 5):
		self.planner = planner;
		self.executors = executors;
		self.generator = generator;
		self.max_iterations = max_iterations

	async def _execute_task_dag_segment(self, tasks_data: List[Dict], ctx: ContextManager, iter_num: int) -> bool:
		if not tasks_data: return True
		prefix = re.sub(r'[^\w\s-]', '', ctx.user_query[:10]).strip().replace(' ', '_') or "q"
		added_ids = [ctx.add_task_from_planner(td, prefix, iter_num, i).id for i, td in enumerate(tasks_data)]
		cache = set();
		success = True
		for tid in added_ids:
			task = ctx.get_task(tid)
			if task and task.status == "pending":
				await self._execute_task_with_dependencies(tid, ctx, cache)
				if ctx.get_task(tid).status != "completed": success = False
		return success

	async def _execute_task_with_dependencies(self, task_id: str, ctx: ContextManager, cache: set):
		task = ctx.get_task(task_id)
		if not task: print(f"  [Pipe Err] Task {task_id} def not found for exec."); return
		if task.status != "pending": return
		cache.add(task_id)
		for dep_id in task.dependencies:
			dep_task = ctx.get_task(dep_id)
			if not dep_task: emsg = f"T {task.id} undef dep ID: {dep_id}. Failed."; ctx.update_task_status(task.id,
																										   "failed",
																										   result=emsg,
																										   thought=emsg); print(
				f"  [Pipe Err] {emsg}"); return
			if dep_task.status == "pending": await self._execute_task_with_dependencies(dep_id, ctx, cache)
		deps_met = True
		if task.dependencies:
			for dep_id in task.dependencies:
				dep_task = ctx.get_task(dep_id)
				if not dep_task or dep_task.status != "completed":
					deps_met = False;
					ts = (
									 task.thought or "") + f"Dep {dep_id} not met (status: {dep_task.status if dep_task else 'N/A'}). Skip."
					ctx.update_task_status(task.id, "skipped", thought=ts);
					print(f"  [Pipe] Task {task.id} skip, dep {dep_id} not met.");
					break
			if not deps_met: return
		if task.status != "pending": return
		executor = self.executors.get(task.executor_name)
		if not executor: emsg = f"Exec '{task.executor_name}' not found for task '{task.task_description}'."; ctx.update_task_status(
			task.id, "failed", result=emsg, thought=emsg); print(f"  [Pipe Err] {emsg}"); return
		ctx.update_task_status(task.id, "running", thought=f"Start: {task.task_description}")
		print(f"\nâ–¶ï¸ Iter Exec Task: {task.id} - \"{task.task_description}\" ({task.executor_name})")
		try:
			result = await executor.execute(task, ctx)
			ctx.update_task_status(task.id, "completed", result=result, thought=task.thought)
		except ExecutorError as e:
			emsg = f"ExecErr T {task.id}: {e}"; ft = (task.thought or "") + f"\nExecErr: {e}"; ctx.update_task_status(
				task.id, "failed", result=emsg, thought=ft); print(f"ğŸ›‘ {emsg}")
		except Exception as e:
			emsg = f"UnexpectedErr T {task.id}: {e}"; import traceback; tb = traceback.format_exc(); print(
				f"ğŸ›‘ {emsg}\n{tb}"); ft = (task.thought or "") + f"\nUnexpectedErr: {e}"; ctx.update_task_status(task.id,
																												"failed",
																												result=emsg,
																												thought=ft)

	async def run(self, user_query: str) -> str:
		print(f"\nğŸš€ IterativePipeline for query: \"{user_query}\"")
		ctx = ContextManager(user_query);
		schemas = [ex.get_schema() for ex in self.executors.values()]
		final_ans = "å¤„ç†ä¸­é‡åˆ°é—®é¢˜ï¼Œæœªèƒ½å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚";
		planner_overall_thought = "";
		current_plan_status = "requires_more_steps"

		for i_iter in range(self.max_iterations):
			iter_num_for_log = i_iter + 1
			print(f"\n--- Iteration {iter_num_for_log} / {self.max_iterations} ---")
			print(f"ğŸ“ Planning phase (Iteration {iter_num_for_log})...")
			try:
				status, thought, steps_data = await self.planner.plan_next_steps(user_query, ctx, schemas)
				planner_overall_thought += f"\nIter {iter_num_for_log} Planner Thought: {thought}"
				current_plan_status = status  # Update overall status
				print(f"  [Planner Out] Status: {current_plan_status}, Thought: {thought}")
				if steps_data:
					print(
						f"  [Planner Out] Next Steps Planned ({len(steps_data)}): {[s.get('task_description', 'N/A') for s in steps_data]}")
				else:
					print("  [Planner Out] No new steps planned.")

				if current_plan_status == "finished": print("  [Pipe] Planner: finished."); break
				if current_plan_status == "cannot_proceed": print(
					"  [Pipe] Planner: cannot_proceed."); final_ans = f"æ— æ³•ç»§ç»­ï¼š{thought}"; break
				if not steps_data:
					if i_iter > 0:
						print("  [Pipe] No new steps & not finished explicitly. Assuming completion."); break
					else:
						print(
							"  [Pipe Err] Planner: no initial steps."); return f"æ— æ³•åˆ¶å®šåˆæ­¥è®¡åˆ’ã€‚Planner Thought: {thought}"
			except Exception as e:
				print(f"  [Pipe Err] Planning error: {e}"); import \
					traceback; traceback.print_exc(); final_ans = f"è§„åˆ’é˜¶æ®µæ„å¤–é”™è¯¯ï¼š{e}"; break

			print(f"\nâš™ï¸ Execution phase (Iteration {iter_num_for_log})...")
			await self._execute_task_dag_segment(steps_data, ctx, iter_num_for_log)

			finish_executed = any(
				t.executor_name == "FinishAction" and t.status == "completed" for t in ctx.tasks.values() if
				t.id in [s["id"] for s in steps_data])
			if finish_executed: print(
				f"  [Pipe] FinishAction completed. Ending iterations."); current_plan_status = "finished"; break

		print(f"\nğŸ’¬ Generation phase after {iter_num_for_log} iteration(s)...")
		try:
			if current_plan_status == "cannot_proceed" and "æ— æ³•ç»§ç»­" in final_ans:
				pass  # Use planner's cannot_proceed thought
			else:
				final_ans = await self.generator.generate_final_answer(user_query, ctx)
		except Exception as e:
			print(f"  [Pipe Err] Generation error: {e}"); import \
				traceback; traceback.print_exc(); final_ans = f"ç”Ÿæˆç­”æ¡ˆæ„å¤–é”™è¯¯ï¼š{e}"
		print(f"\nğŸ’¡ Final Answer: {final_ans}");
		return final_ans


# --- ä¸»ç¨‹åºå…¥å£ ---
async def run_main_logic_with_user_data_deep_recursive():
	# --- LLM and Embedding Setup ---
	api_key = 'sk-af4423da370c478abaf68b056f547c6e'
	base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
	model_name = os.getenv("LLM_MODEL_NAME", "qwen-plus")
	embedding_api_key = 'sk-af4423da370c478abaf68b056f547c6e'

	if not api_key or not embedding_api_key:
		print("é”™è¯¯ï¼šè¯·ç¡®ä¿ OPENAI_API_KEY å’Œ DASHSCOPE_API_KEY_FOR_EMBEDDING (æˆ–å…¶å›é€€ OPENAI_API_KEY) å·²è®¾ç½®ã€‚")
		return
	llm_client = OpenAIChatLLM(model_name=model_name, api_key=api_key, base_url=base_url)
	try:
		embedding_function = QwenEmbeddingFunction(api_key=embedding_api_key)
	except Exception as e:
		print(f"  [Embedding Error] QwenEmbeddingFunction åˆå§‹åŒ–å¤±è´¥: {e}"); return

	# --- REFINED Knowledge Base Data for Multi-Hop Retrieval Test ---
	initial_user_docs_as_dicts = [
		{
			"page_content": "ã€æ­£æŸ´èƒ¡é¥®é¢—ç²’ã€‘è¯å“è¯´æ˜ä¹¦ï¼ˆæ‘˜è¦ï¼‰\nã€æ£€æŸ¥ã€‘åº”ç¬¦åˆé¢—ç²’å‰‚é¡¹ä¸‹æœ‰å…³çš„å„é¡¹è§„å®šï¼ˆè¯¦è§ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104ï¼‰ã€‚å…¶ä½™æŒ‰å“ç§æ ‡å‡†æ‰§è¡Œã€‚",
			"metadata": {"id": "zchy_spec_main", "source_name": "æ­£æŸ´èƒ¡é¥®é¢—ç²’è¯´æ˜ä¹¦æ‘˜è¦"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104 - é¢—ç²’å‰‚ï¼ˆæ¦‚è¿°ï¼‰\næœ¬é€šåˆ™ä¸ºé¢—ç²’å‰‚çš„é€šç”¨è´¨é‡æ§åˆ¶è¦æ±‚ã€‚å…·ä½“æ£€æŸ¥é¡¹ç›®åŒ…æ‹¬ï¼šã€æ€§çŠ¶ã€‘ã€ã€é‰´åˆ«ã€‘ã€ã€æ£€æŸ¥ã€‘ï¼ˆå¦‚ç²’åº¦ã€æ°´åˆ†ã€æº¶åŒ–æ€§ã€è£…é‡å·®å¼‚ã€å¾®ç”Ÿç‰©é™åº¦ç­‰ï¼‰ã€ã€å«é‡æµ‹å®šã€‘ç­‰ã€‚\nå…³äºã€å¾®ç”Ÿç‰©é™åº¦ã€‘ï¼Œé¢—ç²’å‰‚åº”ç¬¦åˆç°è¡Œç‰ˆã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1105ï¼ˆéæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ï¼šå¾®ç”Ÿç‰©è®¡æ•°æ³•ï¼‰ã€é€šåˆ™1106ï¼ˆéæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ï¼šæ§åˆ¶èŒæ£€æŸ¥æ³•ï¼‰å’Œé€šåˆ™1107ï¼ˆéæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ï¼‰çš„ç›¸å…³è§„å®šã€‚",
			"metadata": {"id": "tg0104_overview_v2", "source_name": "è¯å…¸é€šåˆ™0104æ¦‚è¿°"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107 - éæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ï¼ˆèŠ‚é€‰ï¼‰\næœ¬æ ‡å‡†è§„å®šäº†å„ç±»éæ— èŒè¯å“æ‰€éœ€æ§åˆ¶çš„å¾®ç”Ÿç‰©é™åº¦ã€‚\nå¯¹äºå£æœå›ºä½“åˆ¶å‰‚ï¼ˆå¦‚é¢—ç²’å‰‚ï¼‰ï¼š\n1. éœ€æ°§èŒæ€»æ•°ï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—è¿‡1000 cfuã€‚\n2. éœ‰èŒå’Œé…µæ¯èŒæ€»æ•°ï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—è¿‡100 cfuã€‚\n3. æ§åˆ¶èŒï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒï¼›å¯¹äºå«åŠ¨ç‰©è„å™¨ã€ç»„ç»‡æˆ–è¡€æ¶²æˆåˆ†çš„åˆ¶å‰‚ï¼Œæ¯10gï¼ˆæˆ–10mlï¼‰ä¸å¾—æ£€å‡ºæ²™é—¨èŒã€‚",
			"metadata": {"id": "tg1107_details_v2", "source_name": "è¯å…¸é€šåˆ™1107-å¾®ç”Ÿç‰©é™åº¦ç»†èŠ‚"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104 - é¢—ç²’å‰‚ï¼ˆç²’åº¦ä¸æ°´åˆ†ç»†èŠ‚ï¼‰\nã€ç²’åº¦ã€‘ï¼ˆé€šåˆ™0982ç¬¬äºŒæ³•ï¼‰ä¸èƒ½é€šè¿‡ä¸€å·ç­›ï¼ˆ2.00mmï¼‰ä¸èƒ½é€šè¿‡äº”å·ç­›ï¼ˆ0.250mmï¼‰çš„è¯ç²‰æ€»å’Œä¸å¾—è¶…è¿‡æ€»é‡é‡çš„15ï¼…ã€‚\nã€æ°´åˆ†ã€‘ï¼ˆé€šåˆ™0832ç¬¬ä¸€æ³•ï¼‰ä¸­è¯é¢—ç²’å‰‚ä¸å¾—è¿‡8.0ï¼…ã€‚",
			"metadata": {"id": "tg0104_sizewater_v2", "source_name": "è¯å…¸é€šåˆ™0104-ç²’åº¦ä¸æ°´åˆ†ç»†èŠ‚"}}
	]
	initial_langchain_docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in
							  initial_user_docs_as_dicts]
	try:
		chroma_kb = ChromaKnowledgeBase(initial_documents=initial_langchain_docs, embedding_function=embedding_function,
										force_rebuild=False,
										persist_directory = 'chroma_db_kag_recursive_1')
	except Exception as e:
		print(f"åˆ›å»ºChromaçŸ¥è¯†åº“å¤±è´¥: {e}"); import traceback; traceback.print_exc(); return

	# --- Initialize Components (using refined prompts) ---
	planner_prompt = PlannerPrompt()
	deduce_prompt_template = DeducePrompt()
	code_exec_prompt = CodeExecutionPrompt()
	refer_generator_prompt = UserProvidedReferGeneratorPrompt(language="zh")

	retrieval_executor = RetrievalExecutor(kb=chroma_kb)
	deduce_executor = DeduceExecutor(llm_client, deduce_prompt_template)
	code_executor = CodeExecutor(llm_client, code_exec_prompt)
	finish_executor = FinishExecutor()

	executors_map = {
		"RetrievalExecutor": retrieval_executor, "DeduceExecutor": deduce_executor,
		"CodeExecutor": code_executor, "FinishAction": finish_executor
	}
	planner = Planner(llm_client, planner_prompt)
	generator = AnswerGenerator(llm_client, refer_generator_prompt)
	pipeline = IterativePipeline(planner, executors_map, generator,
								 max_iterations=5)  # Allow more iterations for deep recursion

	# --- Run Query Designed to Force Multi-Hop Retrieval ---
	user_query_to_run = "æ­£æŸ´èƒ¡é¥®é¢—ç²’çš„å…·ä½“æ£€æŸ¥æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"

	print(f"\nğŸš€ Running DEEP RECURSIVE optimized query: \"{user_query_to_run}\"")
	final_answer = await pipeline.run(user_query_to_run)
	print(f"\nğŸğŸğŸğŸğŸ DEEP RECURSIVE FINAL ANSWER (for query: '{user_query_to_run}') ğŸğŸğŸğŸğŸ\n{final_answer}")


if __name__ == "__main__":
	print("å¼€å§‹æ‰§è¡Œâ€œæ·±åº¦ç±»é€’å½’â€ä¼˜åŒ–ç‰ˆä¸»é€»è¾‘...")
	# (ç¯å¢ƒå˜é‡å’Œä¾èµ–æç¤º - ä¸ä¹‹å‰ç›¸åŒ)
	# ...
	# Ensure API keys are set before running
	asyncio.run(run_main_logic_with_user_data_deep_recursive())
