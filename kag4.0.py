import asyncio
import json
import os
import re
import subprocess
import sys
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple, Callable, TypedDict
from dataclasses import dataclass, field
import shutil

# --- Langchain and Chroma Imports ---
from langchain_core.documents import Document
from langchain_chroma import Chroma
from tongyiembedding import QwenEmbeddingFunction

# --- Configuration ---
CHROMA_PERSIST_DIRECTORY = "chroma_db_kag_recursive"
CHROMA_COLLECTION_NAME = "kag_recursive_documents"


# --- ChromaKnowledgeBase (åŸºæœ¬ä¸å˜) ---
class ChromaKnowledgeBase:
	def __init__(self, embedding_function: Callable, initial_documents: Optional[List[Document]] = None,
				 persist_directory: str = CHROMA_PERSIST_DIRECTORY, collection_name: str = CHROMA_COLLECTION_NAME,
				 force_rebuild: bool = False):
		print(f"  [ChromaKB] åˆå§‹åŒ–çŸ¥è¯†åº“: {persist_directory}, é›†åˆ: {collection_name}")
		self.embedding_function = embedding_function
		self.persist_directory = persist_directory
		self.collection_name = collection_name
		self.vectorstore: Optional[Chroma] = None
		if force_rebuild and os.path.exists(persist_directory):
			print(f"  [ChromaKB] force_rebuild=True, åˆ é™¤ç›®å½•: {persist_directory}")
			try:
				shutil.rmtree(persist_directory)
			except OSError as e:
				print(f"  [ChromaKB Error] åˆ é™¤ç›®å½•å¤±è´¥: {e}.")
		if os.path.exists(persist_directory) and not force_rebuild:
			print(f"  [ChromaKB] ä» '{persist_directory}' åŠ è½½å·²å­˜åœ¨å‘é‡åº“...")
			try:
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] æˆåŠŸåŠ è½½å‘é‡åº“ '{self.collection_name}'.")
			except Exception as e:
				print(f"  [ChromaKB Error] ä» '{persist_directory}' åŠ è½½å¤±è´¥: {e}. å°†å°è¯•æ–°å»ºã€‚")
				self.vectorstore = None
		if self.vectorstore is None:
			if initial_documents:
				print(f"  [ChromaKB] ä¸º {len(initial_documents)} ä¸ªæ–‡æ¡£æ„å»ºæ–°å‘é‡åº“...")
				self.vectorstore = Chroma.from_documents(documents=initial_documents,
														 embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] æ–°å‘é‡åº“æ„å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚")
			else:
				print(f"  [ChromaKB] æ— åˆå§‹æ–‡æ¡£ï¼Œåˆ›å»ºç©ºçš„æŒä¹…åŒ–é›†åˆã€‚")
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] ç©ºçš„æŒä¹…åŒ– Chroma é›†åˆ '{self.collection_name}' å·²å‡†å¤‡å°±ç»ªã€‚")

	def add_documents(self, documents: List[Document]):  # (ä¸ä¹‹å‰å®ç°ç›¸åŒ)
		if not self.vectorstore:
			if documents:
				print(f"  [ChromaKB] Vectorstore ä¸ºç©º, å°è¯•ä»å½“å‰ {len(documents)} ä¸ªæ–‡æ¡£åˆ›å»º...")
				self.vectorstore = Chroma.from_documents(documents=documents, embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] åŸºäºæ–°æ–‡æ¡£åˆ›å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚");
				return
			else:
				print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–ä¸”æ— æ–‡æ¡£å¯æ·»åŠ ."); return
		if documents:
			print(f"  [ChromaKB] å‘é›†åˆ '{self.collection_name}' æ·»åŠ  {len(documents)} ä¸ªæ–°æ–‡æ¡£...")
			self.vectorstore.add_documents(documents);
			print(f"  [ChromaKB] æ–‡æ¡£æ·»åŠ å®Œæˆã€‚")

	def retrieve(self, query: str, top_k: int = 3, filter_dict: Optional[Dict] = None) -> List[
		Dict[str, Any]]:  # (ä¸ä¹‹å‰å®ç°ç›¸åŒ)
		if not self.vectorstore: print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–."); return []
		try:
			if self.vectorstore._collection is None or self.vectorstore._collection.count() == 0:
				print("  [ChromaKB] çŸ¥è¯†åº“é›†åˆä¸ºç©ºæˆ–æœªæ­£ç¡®åŠ è½½ã€‚");
				return []
		except Exception as e:
			print(f"  [ChromaKB Warning] æ— æ³•è·å–é›†åˆè®¡æ•°: {e}")
		print(f"  [ChromaKB] æ£€ç´¢æŸ¥è¯¢ '{query}', top_k={top_k}, è¿‡æ»¤å™¨: {filter_dict}...")
		try:
			results_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k, filter=filter_dict)
		except Exception as e:
			print(f"  [ChromaKB Error] Chroma similarity search failed: {e}"); return []
		processed_results = [{"id": doc.metadata.get("id", f"retrieved_{i}"), "content": doc.page_content,
							  "metadata": doc.metadata, "score": float(score)}
							 for i, (doc, score) in enumerate(results_with_scores)]
		print(f"  [ChromaKB] æ£€ç´¢åˆ° {len(processed_results)} ä¸ªæ–‡æ¡£ã€‚")
		return processed_results


# --- LLM Client (OpenAIChatLLM - ä¸ä¹‹å‰ç›¸åŒ) ---
class OpenAIChatLLM:
	def __init__(self, model_name: Optional[str] = None, api_key: Optional[str] = None, base_url: Optional[str] = None):
		try:
			import openai
		except ImportError:
			raise ImportError("OpenAI library not found. `pip install openai`.")
		self.api_key = api_key or os.getenv("OPENAI_API_KEY")
		self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
		self.model_name = model_name or os.getenv("LLM_MODEL_NAME", "qwen-plus")
		if not self.api_key: raise ValueError("API key not found.")
		self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
		print(f"[OpenAIChatLLM] å®¢æˆ·ç«¯å°±ç»ª: æ¨¡å‹ {self.model_name}, URL {self.base_url or 'OpenAI default'}")

	async def _make_api_call(self, messages: List[Dict[str, str]], expect_json: bool = False, temperature: float = 0.1,
							 **kwargs) -> str:
		try:
			completion_params = {"model": self.model_name, "messages": messages, "temperature": temperature, **kwargs}
			if expect_json:
				if "dashscope" in (self.base_url or "").lower() and self.model_name.startswith("qwen"):
					completion_params["extra_body"] = {"result_format": "message"}
				else:
					completion_params["response_format"] = {"type": "json_object"}
			response = await asyncio.to_thread(self.client.chat.completions.create, **completion_params)
			content = response.choices[0].message.content
			return content.strip() if content else ""
		except Exception as e:
			print(f"  [OpenAIChatLLM Error] APIè°ƒç”¨å¤±è´¥: {e}"); raise RuntimeError(f"LLM API call failed: {e}")

	async def generate(self, prompt_str: str, system_prompt_str: Optional[str] = None, temperature: float = 0.1,
					   **kwargs) -> str:
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		messages.append({"role": "user", "content": prompt_str})
		return await self._make_api_call(messages, expect_json=False, temperature=temperature, **kwargs)

	async def generate_structured_json(self, prompt_str: str, system_prompt_str: Optional[str] = None,
									   temperature: float = 0.1, **kwargs) -> Dict:
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		user_content = f"{prompt_str}\n\nè¯·ç¡®ä¿æ‚¨çš„å›å¤æ˜¯ä¸€ä¸ªåˆæ³•çš„ã€å•ç‹¬çš„JSONå¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡æœ¬æˆ–markdownæ ‡è®°ã€‚"
		messages.append({"role": "user", "content": user_content})
		response_str = await self._make_api_call(messages, expect_json=True, temperature=temperature, **kwargs)
		cleaned_response_str = response_str.strip()
		if cleaned_response_str.startswith("```json"): cleaned_response_str = cleaned_response_str[7:]
		if cleaned_response_str.endswith("```"): cleaned_response_str = cleaned_response_str[:-3]
		cleaned_response_str = cleaned_response_str.strip()
		try:
			return json.loads(cleaned_response_str)
		except json.JSONDecodeError as e:
			error_msg = f"LLM did not return valid JSON. Error: {e}. Raw: '{response_str}'"
			print(f"  [OpenAIChatLLM Error] {error_msg}");
			raise ValueError(error_msg)


# --- Prompts ---
class BasePrompt:
	def __init__(self, template: str, variables: List[str]):
		self.template_str = template;
		self.variables = variables

	def format(self, **kwargs) -> str:
		formatted_prompt = self.template_str
		for var_name in self.variables:
			if var_name not in kwargs: raise ValueError(f"Missing variable: {var_name}")
			formatted_prompt = formatted_prompt.replace(f"${{{var_name}}}", str(kwargs[var_name]))
		return formatted_prompt


class PlannerPrompt(BasePrompt):  # REFINED: Enhanced for iterative, reflective planning
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½é«˜åº¦æ™ºèƒ½çš„AIä»»åŠ¡è§„åˆ’å’Œåæ€ä¸“å®¶ã€‚
    ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼šæ ¹æ®ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜ `${user_query}` å’Œå·²ç»æ‰§è¡Œçš„å†å²ä»»åŠ¡ `${task_history}`ï¼Œåˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
    è¿™ä¸ªè¡ŒåŠ¨å¯èƒ½æ˜¯ï¼š
    a) è§„åˆ’æ–°çš„ã€å…·ä½“çš„å­ä»»åŠ¡æ¥è·å–ç¼ºå¤±ä¿¡æ¯æˆ–è¿›è¡Œå¿…è¦æ¨ç†ã€‚
    b) åˆ¤æ–­é—®é¢˜å·²ç»å®Œå…¨è§£å†³ã€‚
    c) åˆ¤æ–­é—®é¢˜å› ä¿¡æ¯ä¸è¶³æˆ–å·¥å…·é™åˆ¶è€Œæ— æ³•è¿›ä¸€æ­¥è§£å†³ã€‚

    ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„å†³ç­–å’Œè®¡åˆ’ï¼š

    **è¾“å‡ºæ ¼å¼:**
    ```json
    {
      "plan_status": "finished" | "cannot_proceed" | "requires_more_steps",
      "final_thought": "string (å¯¹å½“å‰æ•´ä½“è¿›å±•çš„æ¸…æ™°æ€è€ƒå’Œåˆ¤æ–­ï¼Œè§£é‡Šä½ çš„plan_status)",
      "next_steps": [ // ä»…å½“ plan_status == "requires_more_steps" æ—¶éç©º
        {
          "id": "string (æ–°ä»»åŠ¡çš„å”¯ä¸€ID, ä¾‹å¦‚ task_iter2_step0, ç¡®ä¿ä¸å†å²IDä¸å†²çª)",
          "executor_name": "string (ä»å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¾‹å¦‚ 'RetrievalExecutor', 'DeduceExecutor', 'FinishAction')",
          "task_description": "string (å¯¹æ­¤æ–°ä»»åŠ¡ç›®æ ‡çš„æ¸…æ™°ã€å…·ä½“ä¸­æ–‡æè¿°)",
          "logic_input": {
            // å­—æ®µä¾èµ–äº executor_name, å‚è€ƒå·¥å…·æè¿°ä¸­çš„ 'logic_input_schema'
            // ç‰¹åˆ«æ³¨æ„ï¼šä¸º DeduceExecutor çš„ 'context_data' å­—æ®µæä¾›é«˜åº¦ç›¸å…³ä¸”ç®€æ´çš„ä¸Šä¸‹æ–‡ã€‚
            // å¯ä»¥ç›´æ¥ä» `${user_query}` æˆ– `${task_history}` ä¸­æŸä¸ªç‰¹å®šä»»åŠ¡çš„ `answer_summary` æˆ– `retrieved_content` ä¸­æå–ã€‚
            // å¦‚æœéœ€è¦ä¹‹å‰ä»»åŠ¡çš„å®Œæ•´ç»“æœï¼Œä½¿ç”¨ "{{task_id.result}}" å ä½ç¬¦ã€‚
          },
          "dependencies": ["string"] // ä¾èµ–çš„å†å²ä»»åŠ¡IDåˆ—è¡¨
        }
        // ... é€šå¸¸ä¸€æ¬¡åªè§„åˆ’1-2ä¸ªæœ€å…³é”®çš„åç»­æ­¥éª¤ ...
      ]
    }
    ```

    **æ ¸å¿ƒæŒ‡ä»¤ä¸æ€è€ƒé“¾ (Chain-of-Thought for Reflective Planning):**
    1.  **å›é¡¾ç›®æ ‡ (Recall Goal)**: æ¸…æ™°ç†è§£ç”¨æˆ·åŸå§‹é—®é¢˜ `${user_query}` çš„æœ€ç»ˆç›®æ ‡æ˜¯ä»€ä¹ˆã€‚
    2.  **å®¡è§†å†å² (Analyze History - `${task_history}`ä¸­æ¯ä¸ªä»»åŠ¡çš„ `Result Details`)**:
        * å“ªäº›å­é—®é¢˜å·²ç»è¢«å›ç­”äº†ï¼Ÿç­”æ¡ˆæ˜¯ä»€ä¹ˆ (`answer_summary`)ï¼Ÿ
        * ä¿¡æ¯æ˜¯å¦å……åˆ† (`is_sufficient`)ï¼Ÿ
        * æ˜¯å¦è¯†åˆ«å‡ºäº†æ–°çš„æŸ¥è¯¢ç‚¹ (`new_questions_or_entities`)ï¼Ÿ
        * `RetrievalExecutor` æ£€ç´¢åˆ°äº†å“ªäº›å…³é”®ä¿¡æ¯ï¼Ÿ
        * ä¹‹å‰çš„ `Thought` ä¸­æœ‰å“ªäº›æœªè§£å†³çš„çº¿ç´¢ï¼Ÿ
    3.  **å·®è·è¯„ä¼° (Gap Assessment)**: å¯¹æ¯”å½“å‰å·²çŸ¥ä¿¡æ¯å’Œç”¨æˆ·åŸå§‹é—®é¢˜çš„æœ€ç»ˆç›®æ ‡ï¼Œè¿˜ç¼ºå°‘å“ªäº›æ ¸å¿ƒä¿¡æ¯ç‰‡æ®µæˆ–é€»è¾‘æ­¥éª¤ï¼Ÿ
    4.  **å†³ç­–åˆ¶å®š (Decision Making & Justification -> `final_thought` and `plan_status`)**:
        * **å·²è§£å†³?** å¦‚æœæ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½å·²åœ¨å†å²ä¸­ç¡®è®¤ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ `is_sufficient: true` çš„Deduceæ­¥éª¤ï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œåˆ™ `plan_status: "finished"`ã€‚`final_thought` åº”æ€»ç»“æ˜¯å¦‚ä½•è§£å†³çš„ã€‚
        * **æ— æ³•è§£å†³?** å¦‚æœå…³é”®ä¿¡æ¯é€šè¿‡å†å²ä¸­çš„æ£€ç´¢å’Œæ¼”ç»æ­¥éª¤éƒ½æœªèƒ½è·å–ï¼ˆä¾‹å¦‚ï¼Œå¤šæ¬¡ç›¸å…³çš„ `RetrievalExecutor` å¤±è´¥æˆ–è¿”å›ç©ºï¼Œæˆ–è€… `DeduceExecutor` æŒç»­æŠ¥å‘Š `is_sufficient: false` ä¸”æ²¡æœ‰æ–°çš„æœ‰æ•ˆæŸ¥è¯¢ç‚¹ï¼‰ï¼Œå¹¶ä¸”ä½ åˆ¤æ–­æ²¡æœ‰å…¶ä»–å¯ç”¨å·¥å…·èƒ½è§£å†³ï¼Œåˆ™ `plan_status: "cannot_proceed"`ã€‚`final_thought` åº”è§£é‡ŠåŸå› ã€‚
        * **éœ€è¦æ›´å¤šæ­¥éª¤?** å¦‚æœä¸Šè¿°ä¸¤è€…éƒ½ä¸æ˜¯ï¼Œåˆ™ `plan_status: "requires_more_steps"`ã€‚`final_thought` åº”æ˜ç¡®æŒ‡å‡ºå½“å‰å·²å®Œæˆä»€ä¹ˆã€è¿˜ç¼ºå°‘ä»€ä¹ˆï¼Œä»¥åŠä¸‹ä¸€æ­¥è®¡åˆ’ï¼ˆ`next_steps`ï¼‰çš„ç›®æ ‡æ˜¯ä»€ä¹ˆã€‚
    5.  **è¡ŒåŠ¨è§„åˆ’ (Action Formulation for `next_steps` - å¦‚æœ `requires_more_steps`)**:
        * **é’ˆå¯¹æ€§**: è§„åˆ’1-2ä¸ªæœ€ç›´æ¥è§£å†³å½“å‰æ ¸å¿ƒå·®è·çš„æ­¥éª¤ã€‚
        * **å·¥å…·é€‰æ‹©**:
            * å¦‚æœå†å²æç¤º `new_questions_or_entities` æˆ–ä½ çš„åˆ†æè¡¨æ˜éœ€è¦æŸ¥æ‰¾æ–°ä¿¡æ¯ï¼Œä¼˜å…ˆè§„åˆ’ `RetrievalExecutor`ã€‚å…¶ `logic_input.query` åº”å°½å¯èƒ½å…·ä½“ï¼Œå¯ä»¥åŸºäºè¿™äº› `new_questions_or_entities` æˆ–å¯¹å†å²ç»“æœçš„åˆ†ææ¥æ„é€ ã€‚
            * å¦‚æœå·²æœ‰ä¸€äº›ä¿¡æ¯ç‰‡æ®µï¼Œéœ€è¦æ•´åˆã€åˆ†æã€æ€»ç»“ã€åˆ¤æ–­æˆ–æŠ½å–ï¼Œè§„åˆ’ `DeduceExecutor`ã€‚å…¶ `logic_input.reasoning_goal` è¦æ˜ç¡®ï¼Œ`logic_input.context_data` åº”ç²¾ç¡®æä¾›å¿…è¦çš„ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå¼•ç”¨åˆšæ£€ç´¢åˆ°çš„ `{{retrieval_task_id.result}}`ï¼Œæˆ–å†å²ä¸­æŸä¸ª `DeduceExecutor` çš„ `answer_summary`ï¼‰ã€‚
            * å¦‚æœåˆ¤æ–­æ‰€æœ‰å­é—®é¢˜éƒ½å·²è§£å†³ï¼Œä¿¡æ¯å·²å®Œæ•´ï¼Œè§„åˆ’ä¸€ä¸ª `FinishAction` ä»»åŠ¡ï¼Œ`logic_input` ä¸­å¯ä»¥ç®€å•è¯´æ˜ç†ç”±ã€‚
        * **ID å’Œä¾èµ–**: ä¸ºæ–°ä»»åŠ¡åˆ†é…å”¯ä¸€çš„ `id`ã€‚æ­£ç¡®è®¾ç½® `dependencies`ï¼ŒæŒ‡å‘æä¾›è¾“å…¥çš„å†å²ä»»åŠ¡IDã€‚
    6.  **ä¸¥æ ¼JSONè¾“å‡º**: ä½ çš„å…¨éƒ¨è¾“å‡ºå¿…é¡»æ˜¯åˆæ³•çš„ã€å•ä¸€çš„JSONå¯¹è±¡ã€‚
    """
	USER_TEMPLATE = """
    --- å¯ç”¨å·¥å…· ---
    ${available_executors_description}
    --- å¯ç”¨å·¥å…·ç»“æŸ ---

    --- å†å²ä»»åŠ¡åŠç»“æœ (æœ€è¿‘çš„æ­¥éª¤åœ¨æœ€å) ---
    ${task_history}
    --- å†å²ä»»åŠ¡åŠç»“æœç»“æŸ ---

    --- ç”¨æˆ·åŸå§‹é—®é¢˜ ---
    "${user_query}"
    --- ç”¨æˆ·åŸå§‹é—®é¢˜ç»“æŸ ---

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œåæ€å¹¶è§„åˆ’ã€‚è¾“å‡ºJSONå¯¹è±¡:
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["user_query", "available_executors_description", "task_history"])


class DeducePrompt(BasePrompt):  # REFINED: To guide LLM for structured output
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„AIæ¨ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„â€œä¸Šä¸‹æ–‡ä¿¡æ¯â€ï¼Œç²¾ç¡®åœ°å›ç­”æˆ–å®Œæˆâ€œæ¨ç†ç›®æ ‡â€ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„ç»“è®ºå’Œè¯„ä¼°ï¼š
    ```json
    {
      "answer_summary": "string (å¯¹æ¨ç†ç›®æ ‡çš„ç›´æ¥ã€ç®€æ´çš„å›ç­”æˆ–æ€»ç»“)",
      "is_sufficient": boolean (true å¦‚æœä½ è®¤ä¸ºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¶³ä»¥å®Œå…¨å›ç­”æ¨ç†ç›®æ ‡ï¼Œå¦åˆ™ false),
      "new_questions_or_entities": [
        "string" // å¦‚æœ is_sufficient ä¸º falseï¼Œåˆ—å‡ºéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥æˆ–æ£€ç´¢çš„å…·ä½“é—®é¢˜ã€æœ¯è¯­æˆ–å®ä½“åç§°ã€‚å¦‚æœä¿¡æ¯å……åˆ†åˆ™ä¸ºç©ºåˆ—è¡¨[]ã€‚
      ]
    }
    ```
    - å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸åŒ…å«å›ç­”æ¨ç†ç›®æ ‡æ‰€éœ€çš„å†…å®¹ï¼Œè¯·åœ¨ `answer_summary` ä¸­æ˜ç¡®æŒ‡å‡ºä¿¡æ¯ä¸è¶³ï¼Œå¹¶å°† `is_sufficient` è®¾ä¸º `false`ã€‚
    - `new_questions_or_entities` å¯¹äºå¼•å¯¼åç»­æ­¥éª¤è‡³å…³é‡è¦ï¼Œè¯·å°½å¯èƒ½å…·ä½“ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸­æåˆ°â€œé€šåˆ™XXXXâ€ï¼Œä½†ä¸åŒ…å«å…¶ç»†èŠ‚ï¼Œé‚£ä¹ˆâ€œé€šåˆ™XXXXçš„è¯¦ç»†å†…å®¹â€å°±æ˜¯ä¸€ä¸ªå¥½çš„`new_questions_or_entities`ã€‚
    - ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–markdownæ ‡è®°ï¼Œåªè¾“å‡ºJSONå¯¹è±¡ã€‚
    """
	USER_TEMPLATE = """
    æ¨ç†ç›®æ ‡:
    ${reasoning_goal}

    ä¸Šä¸‹æ–‡ä¿¡æ¯:
    ${context_data}

    è¯·è¾“å‡ºJSONæ ¼å¼çš„æ¨ç†ç»“æœ:
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["reasoning_goal", "context_data"])


class CodeExecutionPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ)
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ©æ‰‹ã€‚"
	USER_TEMPLATE = "è¯·æ ¹æ®ä»¥ä¸‹æŒ‡ä»¤å’Œç›¸å…³æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µPythonä»£ç æ¥è§£å†³é—®é¢˜ã€‚\nä»£ç å¿…é¡»é€šè¿‡ `print()` è¾“å‡ºå…¶æœ€ç»ˆè®¡ç®—ç»“æœã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¾“å‡ºçº¯ä»£ç ã€‚\n\næŒ‡ä»¤:\n${code_generation_prompt}\n\nç›¸å…³æ•°æ® (å¦‚æœæä¾›):\n${relevant_data}\n\nç”Ÿæˆçš„Pythonä»£ç  (è¯·ç¡®ä¿å®ƒåªåŒ…å«ä»£ç æœ¬èº«ï¼Œå¹¶ç”¨print()è¾“å‡ºç»“æœ):"

	def __init__(self): super().__init__(self.USER_TEMPLATE, ["code_generation_prompt", "relevant_data"])


class UserProvidedReferGeneratorPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ, ç¡®ä¿get_nowå¯ç”¨)
	def __init__(self, language: str = "zh"):
		try:
			from kag.common.utils import get_now
		except ImportError:
			def get_now(language='zh'):
				return "å½“å‰æ—¥æœŸ"

			print("[UserProvidedReferGeneratorPrompt] Warning: kag.common.utils.get_now not found.")
		self.template_zh = (
					f"ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†æä¸“å®¶ï¼Œä»Šå¤©æ˜¯{get_now(language='zh')}ã€‚" + "åŸºäºç»™å®šçš„å¼•ç”¨ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\nè¾“å‡ºç­”æ¡ˆï¼Œå¦‚æœç­”æ¡ˆä¸­å­˜åœ¨å¼•ç”¨ä¿¡æ¯ï¼Œåˆ™éœ€è¦referenceçš„idå­—æ®µï¼Œå¦‚æœä¸æ˜¯æ£€ç´¢ç»“æœï¼Œåˆ™ä¸éœ€è¦æ ‡è®°å¼•ç”¨\nè¾“å‡ºæ—¶ï¼Œä¸éœ€è¦é‡å¤è¾“å‡ºå‚è€ƒæ–‡çŒ®\nå¼•ç”¨è¦æ±‚ï¼Œä½¿ç”¨ç±»ä¼¼<reference id=\"chunk:1_2\"></reference>è¡¨ç¤º\nå¦‚æœæ ¹æ®å¼•ç”¨ä¿¡æ¯æ— æ³•å›ç­”ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹å†…çš„çŸ¥è¯†å›ç­”ï¼Œä½†æ˜¯å¿…é¡»é€šè¿‡åˆé€‚çš„æ–¹å¼æç¤ºç”¨æˆ·ï¼Œæ˜¯åŸºäºæ£€ç´¢å†…å®¹è¿˜æ˜¯å¼•ç”¨æ–‡æ¡£\nç¤ºä¾‹1ï¼š\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ï¼š\næ ¹æ®å¸¸è¯†å²³çˆ¶æ˜¯å¦»å­çš„çˆ¸çˆ¸ï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆæ‰¾åˆ°å¼ ä¸‰çš„å¦»å­ï¼Œç„¶åæ‰¾åˆ°å¦»å­çš„çˆ¸çˆ¸\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'\nreferenceï¼š\n[\n{\n    \"content\": \"å¼ ä¸‰ å¦»å­ ç‹äº”\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_1\"\n},\n{\n    \"content\": \"ç‹äº” çˆ¶äº² ç‹å››\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_2\"\n}\n]'\né—®é¢˜ï¼š'å¼ ä¸‰çš„å²³çˆ¶æ˜¯è°ï¼Ÿ'\n\nå¼ ä¸‰çš„å¦»å­æ˜¯ç‹äº”<reference id=\"chunk:1_1\"></reference>ï¼Œè€Œç‹äº”çš„çˆ¶äº²æ˜¯ç‹å››<reference id=\"chunk:1_2\"></reference>ï¼Œæ‰€ä»¥å¼ ä¸‰çš„å²³çˆ¶æ˜¯ç‹å››\n\n\nè¾“å‡ºè¯­è°ƒè¦æ±‚é€šé¡ºï¼Œä¸è¦æœ‰æœºæ¢°æ„Ÿï¼Œè¾“å‡ºçš„è¯­è¨€è¦å’Œé—®é¢˜çš„è¯­è¨€ä¿æŒä¸€è‡´\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š'${summary_of_executed_steps}'\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'${formatted_references}'\né—®é¢˜ï¼š'${user_query}'")
		self.template_en = self.template_zh  # ç®€åŒ–ï¼Œå®é™…åº”æœ‰è‹±æ–‡ç‰ˆ
		self.template_zh = (
		f"ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†æä¸“å®¶ï¼Œä»Šå¤©æ˜¯{get_now(language='zh')}ã€‚"
		"ä½ çš„ä»»åŠ¡æ˜¯åŸºäºã€ä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ä¸­å„æ­¥éª¤çš„ã€äº‹å®æ€§äº§å‡ºã€‘ï¼ˆå°¤å…¶æ˜¯æˆåŠŸçš„æ¨ç†ç»“è®ºå’Œæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼‰ä»¥åŠã€ç»™å®šçš„å¼•ç”¨ä¿¡æ¯ã€‘ï¼Œæ¥å…¨é¢ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š'${user_query}'ã€‚"
		"è¯·ç¡®ä¿ä½ çš„å›ç­”ç›´æ¥ã€å‡†ç¡®ï¼Œå¹¶èƒ½ä½“ç°å‡ºå¤šæ–¹é¢çš„ä¿¡æ¯ç»¼åˆã€‚ä¸è¦é‡å¤æˆ–è¿‡å¤šé˜è¿°ä»»åŠ¡è§„åˆ’çš„æ€è€ƒè¿‡ç¨‹ï¼Œè€Œæ˜¯èšç„¦äºå®é™…è·å¾—çš„ç»“æœã€‚"
		# ... åç»­å…³äºå¼•ç”¨æ ¼å¼ã€ä¿¡æ¯ä¸è¶³å¤„ç†ç­‰æŒ‡ä»¤ä¿æŒä¸å˜ ...
		"\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š'${summary_of_executed_steps}'"
		"\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'${formatted_references}'\né—®é¢˜ï¼š'${user_query}'"
		)
		current_template = self.template_zh if language == "zh" else self.template_en
		super().__init__(current_template, ["summary_of_executed_steps", "user_query", "formatted_references"])

	def format(self, summary_of_executed_steps: str, user_query: str, retrieved_references: List[Dict]) -> str:
		ref_list_for_prompt = []
		for i, ref_item in enumerate(retrieved_references):
			ref_list_for_prompt.append({"content": ref_item.get("content", ""),
										"document_name": ref_item.get("metadata", {}).get("source_name",
																						  f"æ£€ç´¢æ–‡æ¡£{i + 1}"),
										"id": ref_item.get("metadata", {}).get("id", f"retrieved_chunk_{i}")})
		formatted_references_str = json.dumps(ref_list_for_prompt, ensure_ascii=False, indent=2)
		return super().format(summary_of_executed_steps=summary_of_executed_steps, user_query=user_query,
							  formatted_references=formatted_references_str)


# --- DataStructures & ContextManager (ContextManager slightly adapted) ---
LogicInput = Dict[str, Any]


@dataclass
class Task:
	id: str;
	executor_name: str;
	task_description: str;
	logic_input: LogicInput
	dependencies: List[str] = field(default_factory=list);
	status: str = "pending"
	result: Optional[Any] = None;
	thought: Optional[str] = None  # result can now be a Dict for DeduceExecutor


class ContextManager:
	def __init__(self, user_query: str):
		self.user_query = user_query;
		self.tasks: Dict[str, Task] = {}
		self.execution_order: List[str] = []  # ä¿æŒä»»åŠ¡æ·»åŠ /è§„åˆ’é¡ºåº

	def add_task_from_planner(self, task_data: Dict, base_id_prefix: str, current_iteration: int,
							  step_in_iter: int) -> Task:
		llm_provided_id = task_data.get("id")
		if llm_provided_id and isinstance(llm_provided_id, str) and llm_provided_id.strip():
			task_id = llm_provided_id
		else:
			task_id = f"{base_id_prefix}_iter{current_iteration}_step{step_in_iter}"
		original_task_id = task_id;
		counter = 0
		while task_id in self.tasks: counter += 1; task_id = f"{original_task_id}_v{counter}"
		if counter > 0: print(f"  [CM Warn] Task ID '{original_task_id}' conflict. Renamed to '{task_id}'.")
		task = Task(id=task_id, executor_name=task_data["executor_name"],
					task_description=task_data["task_description"],
					logic_input=task_data["logic_input"], dependencies=task_data.get("dependencies", []))
		self.tasks[task.id] = task;
		self.execution_order.append(task.id);
		return task

	def get_task(self, task_id: str) -> Optional[Task]:
		return self.tasks.get(task_id)

	def update_task_status(self, task_id: str, status: str, result: Optional[Any] = None,
						   thought: Optional[str] = None):
		task = self.get_task(task_id)
		if task:
			task.status = status
			if result is not None: task.result = result
			current_thought = task.thought or "";
			task.thought = f"{current_thought}\n{thought}".strip() if current_thought and thought else thought or current_thought
		else:
			print(f"  [CM Warn] Task ID {task_id} not found for status update.")

	def get_task_history_for_prompt(self) -> str:  # REFINED: To show structured DeduceExecutor result
		history = []
		for task_id in self.execution_order:
			task = self.tasks.get(task_id)
			if task and task.status in ["completed", "failed"]:
				result_display_parts = []
				if task.status == "completed":
					if isinstance(task.result, dict) and task.executor_name == "DeduceExecutor":  # Structured result
						deduce_out = task.result
						result_display_parts.append(
							f"    æ¨ç†æ€»ç»“: {str(deduce_out.get('answer_summary', 'N/A'))[:100]}{'...' if len(str(deduce_out.get('answer_summary', 'N/A'))) > 100 else ''}")
						result_display_parts.append(f"    ä¿¡æ¯æ˜¯å¦å……åˆ†: {deduce_out.get('is_sufficient', True)}")
						new_q = deduce_out.get('new_questions_or_entities', [])
						if new_q: result_display_parts.append(
							f"    å»ºè®®è¿›ä¸€æ­¥æŸ¥è¯¢: {', '.join(new_q)[:100]}{'...' if len(', '.join(new_q)) > 100 else ''}")
					elif isinstance(task.result, list) and task.executor_name == "RetrievalExecutor":
						result_display_parts.append(
							f"    æ£€ç´¢åˆ° {len(task.result)} ä¸ªç‰‡æ®µã€‚å†…å®¹(æ‘˜è¦): {str(task.result[0].get('content') if task.result else 'ç©º')[:80]}...")
					else:  # Other executors or simple string result
						res_str = str(task.result);
						result_display_parts.append(f"    ç»“æœ: {res_str[:150]}{'...' if len(res_str) > 150 else ''}")
				else:  # Failed task
					result_display_parts.append(f"    æ‰§è¡Œå¤±è´¥: {str(task.result)[:100]}...")

				result_final_display = "\n".join(result_display_parts)
				thought_str = str(task.thought or "N/A");
				thought_str = thought_str[:100] + "..." if len(thought_str) > 100 else thought_str

				history.append(
					f"  - Task ID: {task.id}\n    Desc: {task.task_description}\n    Exec: {task.executor_name}\n    Status: {task.status}\n{result_final_display}\n    Thought: {thought_str}")
		return "\n\n".join(history) if history else "å°šæœªæ‰§è¡Œä»»ä½•å†å²ä»»åŠ¡ã€‚"

	# In class ContextManager:
	def get_summary_for_generator(self) -> str:
		summary_parts = []
		print("  [ContextManager] Generating summary for AnswerGenerator...")
		for i, task_id in enumerate(self.execution_order):  # æŒ‰ä»»åŠ¡è§„åˆ’/æ·»åŠ é¡ºåº
			task = self.get_task(task_id)
			if task:
				result_str = "N/A"
				thought_str = str(task.thought or 'æœªè®°å½•æ€è€ƒè¿‡ç¨‹')

				if task.status == 'completed':
					if isinstance(task.result, dict) and task.executor_name == "DeduceExecutor":
						# è¿™æ˜¯ DeduceExecutor çš„ç»“æ„åŒ–è¾“å‡º
						answer_summary = task.result.get('answer_summary', 'æœªèƒ½æå–æ€»ç»“')
						is_sufficient = task.result.get('is_sufficient', False)
						new_qs = task.result.get('new_questions_or_entities', [])
						result_str = f"æ¨ç†ç»“è®º: \"{answer_summary}\" (ä¿¡æ¯æ˜¯å¦å……åˆ†: {is_sufficient})"
						if new_qs:
							result_str += f" å»ºè®®è¿›ä¸€æ­¥æ¢ç©¶: {', '.join(new_qs)}"
					elif isinstance(task.result, list) and task.executor_name == "RetrievalExecutor":
						# task.result æ˜¯ List[Dict[str,Any]]
						num_retrieved = len(task.result)
						if num_retrieved > 0:
							# æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£çš„IDæˆ–åç§°ï¼Œè€Œä¸æ˜¯å®Œæ•´å†…å®¹ï¼Œé¿å…æ‘˜è¦è¿‡é•¿
							# å®Œæ•´å†…å®¹ä¼šé€šè¿‡ collect_retrieved_references_for_generator å•ç‹¬æä¾›ç»™ ReferGeneratorPrompt
							doc_ids_or_names = [
								item.get("metadata", {}).get("id", f"æ£€ç´¢é¡¹{idx + 1}")
								for idx, item in enumerate(task.result)
							]
							result_str = f"æ£€ç´¢åˆ° {num_retrieved} ä¸ªç›¸å…³æ–‡æ¡£/ç‰‡æ®µ (ID/åç§°: {', '.join(doc_ids_or_names)})ã€‚"
						else:
							result_str = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
					elif isinstance(task.result, str):  # ä¾‹å¦‚ CodeExecutor æˆ–æ—§ç‰ˆ DeduceExecutor
						result_str = task.result
					else:  # å…¶ä»–å¤æ‚ç±»å‹
						result_str = f"å¤æ‚ç±»å‹ç»“æœ (æ‘˜è¦: {str(task.result)[:100]}...)"
				elif task.status == 'failed':
					result_str = f"æ‰§è¡Œå¤±è´¥: {str(task.result)[:150]}..."
				elif task.status == 'skipped':
					result_str = "å› ä¾èµ–å¤±è´¥æˆ–æ¡ä»¶ä¸æ»¡è¶³è€Œè·³è¿‡ã€‚"
				else:  # pending, running
					result_str = f"å½“å‰çŠ¶æ€: {task.status}"

				# æˆªæ–­ï¼Œä»¥é˜²Promptè¿‡é•¿
				result_str_summary = result_str[:250] + "..." if len(result_str) > 250 else result_str
				thought_str_summary = thought_str[:200] + "..." if len(thought_str) > 200 else thought_str

				summary_parts.append(
					f"æ­¥éª¤ {i + 1} (ID: {task.id}):\n"
					f"  ç›®æ ‡: {task.task_description}\n"
					f"  æ‰§è¡Œå·¥å…·: {task.executor_name}\n"
					f"  æ‰§è¡Œæ€è€ƒ: {thought_str_summary}\n"
					f"  äº§å‡º/çŠ¶æ€: {result_str_summary}"
				)
		if not summary_parts:
			return "æœªèƒ½æ‰§è¡Œä»»ä½•æ­¥éª¤ï¼Œæˆ–æ²¡æœ‰å¯æ€»ç»“çš„äº§å‡ºã€‚"
		return "\n\n".join(summary_parts)

	def collect_retrieved_references_for_generator(self) -> List[Dict]:  # (ä¸ä¹‹å‰ç›¸åŒ)
		references = []
		for task_id in self.execution_order:
			task = self.get_task(task_id)
			if task and task.executor_name == "RetrievalExecutor" and task.status == "completed" and isinstance(
					task.result, list):
				for retrieved_item in task.result:
					if isinstance(retrieved_item, dict) and "content" in retrieved_item:
						references.append({"content": retrieved_item["content"],
										   "document_name": retrieved_item.get("metadata", {}).get("source_name",
																								   f"æ¥æºæ–‡æ¡£_{task.id}"),
										   "id": retrieved_item.get("metadata", {}).get("id",
																						f"ref_{task.id}_{len(references)}")})
		return references


# --- Executors (DeduceExecutor modified, FinishExecutor added) ---
class ExecutorError(Exception): pass


class ExecutorBase(ABC):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: Optional[OpenAIChatLLM] = None):
		self.llm_client = llm_client

	@abstractmethod
	async def execute(self, task: Task, context: ContextManager) -> Any:
		pass

	@abstractmethod
	def get_schema(self) -> Dict[str, Any]:
		pass

	def _resolve_references(self, data_template: Any, context: ContextManager) -> Any:
		if isinstance(data_template, str):
			def replace_match(match):
				ref_full = match.group(1).strip();
				task_id_ref, attr_ref = ref_full.split('.', 1) if '.' in ref_full else (ref_full, "result")
				ref_task = context.get_task(task_id_ref)
				if ref_task and ref_task.status == "completed":
					if attr_ref == "result":
						# Special handling for structured DeduceExecutor result if referenced directly
						if isinstance(ref_task.result, dict) and "answer_summary" in ref_task.result:
							return str(ref_task.result["answer_summary"])  # Use the summary part
						if isinstance(ref_task.result, list): return "\n".join(
							[f"- {item}" for item in map(str, ref_task.result)])
						return str(ref_task.result)
					else:
						print(f"  [Exec Warn] Unsupported attr '{attr_ref}' in {{ {ref_full} }}.")
				else:
					print(
						f"  [Exec Warn] Cannot resolve ref {{ {ref_full} }}. Task '{task_id_ref}' status: {ref_task.status if ref_task else 'not_found'}.")
				return f"{{å¼•ç”¨é”™è¯¯: {ref_full}}}"

			return re.sub(r"\{\{([\w_\-\d\.]+)\}\}", replace_match, data_template)
		elif isinstance(data_template, dict):
			return {k: self._resolve_references(v, context) for k, v in data_template.items()}
		elif isinstance(data_template, list):
			return [self._resolve_references(item, context) for item in data_template]
		return data_template


class RetrievalExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, kb: ChromaKnowledgeBase):
		super().__init__(); self.kb = kb

	async def execute(self, task: Task, context: ContextManager) -> List[Dict[str, Any]]:
		logic_input = task.logic_input;
		query_to_retrieve = logic_input.get("query");
		filter_param = logic_input.get("filter")
		if not query_to_retrieve or not isinstance(query_to_retrieve, str): raise ExecutorError(
			"RetrievalExecutor: 'query' (string) is required.")
		resolved_query = self._resolve_references(query_to_retrieve, context);
		actual_filter = None
		if filter_param and isinstance(filter_param, dict) and filter_param:
			actual_filter = filter_param
		elif filter_param:
			print(f"  [RetrievalExec Warn] Invalid filter for task '{task.id}': {filter_param}. Ignoring.")
		current_thought = task.thought or "";
		task.thought = f"{current_thought}KBæ£€ç´¢æŸ¥è¯¢: '{resolved_query}', è¿‡æ»¤å™¨: {actual_filter}.".strip()
		retrieved_docs_with_meta = self.kb.retrieve(resolved_query, top_k=3, filter_dict=actual_filter)
		if not retrieved_docs_with_meta: task.thought += "\næœªæ£€ç´¢åˆ°ä»»ä½•åŒ¹é…æ–‡æ¡£."; return []
		task.thought += f"\næ£€ç´¢åˆ° {len(retrieved_docs_with_meta)} ä¸ªæ–‡æ¡£å¯¹è±¡.";
		return retrieved_docs_with_meta

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "RetrievalExecutor",
				"description": "ä»å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚å¯æŒ‡å®šå…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚",
				"logic_input_schema": {"query": "string (æ£€ç´¢æŸ¥è¯¢è¯­å¥, å¯å¼•ç”¨ {{task_id.result}})",
									   "filter": "dict (å¯é€‰, ChromaDBå…ƒæ•°æ®è¿‡æ»¤å™¨)"}}


# REFINED: DeduceExecutor to return structured output
class DeduceExecutorOutput(TypedDict, total=False):
	answer_summary: str
	is_sufficient: bool
	new_questions_or_entities: List[str]
	raw_llm_response: str


class DeduceExecutor(ExecutorBase):
	def __init__(self, llm_client: OpenAIChatLLM, prompt_template: DeducePrompt,
				 specialized_prompts: Optional[Dict[str, BasePrompt]] = None):  # Renamed prompt to prompt_template
		super().__init__(llm_client)
		self.default_prompt_template = prompt_template  # Use the renamed variable
		self.specialized_prompts = specialized_prompts if specialized_prompts else {}

	async def execute(self, task: Task, context: ContextManager) -> DeduceExecutorOutput:
		logic_input = task.logic_input;
		reasoning_goal = logic_input.get("reasoning_goal")
		raw_context_data = logic_input.get("context_data");
		operation_type = logic_input.get("operation_type")

		if not reasoning_goal or not isinstance(reasoning_goal, str) or raw_context_data is None:
			raise ExecutorError("DeduceExecutor: 'reasoning_goal' (string) and 'context_data' are required.")

		resolved_context_data = self._resolve_references(raw_context_data, context)
		context_data_str = json.dumps(resolved_context_data, ensure_ascii=False, indent=2) if isinstance(
			resolved_context_data, (list, dict)) else str(resolved_context_data)

		prompt_to_use = self.default_prompt_template
		system_prompt_to_use = DeducePrompt.SYSTEM_PROMPT  # Default system prompt
		if operation_type and operation_type in self.specialized_prompts:
			prompt_to_use = self.specialized_prompts[operation_type]
			system_prompt_to_use = getattr(prompt_to_use, "SYSTEM_PROMPT",
										   system_prompt_to_use)  # Specialized system prompt if available
			print(f"  [DeduceExecutor] Using specialized prompt for op_type: {operation_type}")

		prompt_str = prompt_to_use.format(reasoning_goal=reasoning_goal, context_data=context_data_str)
		current_thought = task.thought or ""
		task.thought = f"{current_thought}æ¼”ç»ç›®æ ‡({operation_type or 'default'}): {reasoning_goal}. ä¸Šä¸‹æ–‡(æ‘˜è¦): {context_data_str[:100]}...".strip()

		# DeduceExecutor's LLM now needs to output JSON
		response_json = await self.llm_client.generate_structured_json(prompt_str,
																	   system_prompt_str=system_prompt_to_use,
																	   temperature=0.0)  # Low temp for structured

		# Validate and structure the output
		answer_summary = response_json.get("answer_summary", "æœªèƒ½ä»LLMå“åº”ä¸­è§£æå‡ºç­”æ¡ˆã€‚")
		is_sufficient = response_json.get("is_sufficient", False)  # Default to False if not specified
		new_qs = response_json.get("new_questions_or_entities", [])
		if not isinstance(new_qs, list): new_qs = [str(new_qs)] if new_qs else []  # Ensure it's a list of strings

		task.thought += f"\nLLMæ¼”ç»å“åº”(ç»“æ„åŒ–): sufficient={is_sufficient}, new_qs={new_qs}, summary={answer_summary[:50]}..."

		return {
			"answer_summary": answer_summary,
			"is_sufficient": is_sufficient,
			"new_questions_or_entities": new_qs,
			"raw_llm_response": json.dumps(response_json)  # Store the full JSON response string
		}

	def get_schema(self) -> Dict[str, Any]:  # (ä¸ä¹‹å‰ç›¸åŒ)
		return {"name": "DeduceExecutor",
				"description": "åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ¨ç†ã€æ€»ç»“ã€åˆ¤æ–­æˆ–æŠ½å–ã€‚ä¸Šä¸‹æ–‡å¯å¼•ç”¨å…ˆå‰æ­¥éª¤çš„ç»“æœã€‚ä¼šåˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……åˆ†å¹¶ç»™å‡ºä¸‹ä¸€æ­¥æŸ¥è¯¢å»ºè®®ã€‚",
				"logic_input_schema": {"reasoning_goal": "string (å…·ä½“æ¨ç†ç›®æ ‡)",
									   "context_data": "any (æ¨ç†æ‰€éœ€ä¸Šä¸‹æ–‡ï¼Œå¯å¼•ç”¨ {{task_id.result}})",
									   "operation_type": "string (å¯é€‰, å¦‚ summarize, extract_info, judge, refine_query)"}}


class CodeExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: CodeExecutionPrompt):
		super().__init__(llm_client); self.prompt_template = prompt

	async def execute(self, task: Task, context: ContextManager) -> str:
		logic_input = task.logic_input;
		code_gen_prompt = logic_input.get("code_generation_prompt");
		rel_data = logic_input.get("relevant_data", "")
		if not code_gen_prompt or not isinstance(code_gen_prompt, str): raise ExecutorError(
			"CodeExecutor: 'code_generation_prompt' required.")
		res_code_prompt = self._resolve_references(code_gen_prompt, context);
		res_rel_data = self._resolve_references(rel_data, context)
		rel_data_prompt = json.dumps(res_rel_data, ensure_ascii=False, indent=2) if isinstance(res_rel_data,
																							   (list, dict)) else str(
			res_rel_data) if res_rel_data else ""
		llm_prompt = self.prompt_template.format(code_generation_prompt=res_code_prompt, relevant_data=rel_data_prompt)
		task.thought = (task.thought or "") + f"ä»£ç ç”Ÿæˆç›®æ ‡: {res_code_prompt}. ".strip()
		code_block = await self.llm_client.generate(llm_prompt, system_prompt_str=CodeExecutionPrompt.SYSTEM_PROMPT)
		code = code_block.strip();
		if code.startswith("```python"): code = code[9:]
		if code.startswith("```"): code = code[3:]
		if code.endswith("```"): code = code[:-3]
		code = code.strip()
		if not code: task.thought += "\nLLMæœªèƒ½ç”Ÿæˆä»£ç ."; raise ExecutorError("CodeExecutor: LLM no code.")
		task.thought += f"\nç”Ÿæˆçš„ä»£ç :\n---\n{code}\n---"
		try:
			with open("t.py", "w", encoding="utf-8") as f:
				f.write(code)
			p = await asyncio.to_thread(subprocess.run, [sys.executable, "t.py"], capture_output=True, text=True,
										timeout=10, check=False)
			if p.returncode != 0: task.thought += f"\nä»£ç é”™è¯¯ç {p.returncode}. stderr:\n{p.stderr or 'æ— '}"; raise ExecutorError(
				f"Code exec err {p.returncode}:\n{p.stderr or 'æ— '}")
			out = p.stdout.strip();
			task.thought += f"\nä»£ç è¾“å‡º: {out}";
			return out
		except subprocess.TimeoutExpired:
			task.thought += "\nä»£ç è¶…æ—¶."; raise ExecutorError("Code timeout.")
		except Exception as e:
			task.thought += f"\nä»£ç æœ¬åœ°æ‰§è¡Œé”™è¯¯: {e}"; raise ExecutorError(f"Code local exec err: {e}")
		finally:
			if os.path.exists("t.py"): os.remove("t.py")

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "CodeExecutor",
				"description": "ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ã€‚ä»£ç åº”print()ç»“æœã€‚è¾“å…¥å¯å¼•ç”¨ {{task_id.result}}ã€‚",
				"logic_input_schema": {"code_generation_prompt": "string (ä»£ç ç”ŸæˆæŒ‡ä»¤)",
									   "relevant_data": "any (å¯é€‰, ä»£ç æ‰€éœ€æ•°æ®)"}}


class FinishExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	async def execute(self, task: Task,
					  context: ContextManager) -> str: task.thought = "æ”¶åˆ°FinishæŒ‡ä»¤ï¼Œæµç¨‹ç»“æŸã€‚"; print(
		f"  [FinishExecutor] Task {task.id} executed."); return "å·²å®Œæˆæ‰€æœ‰å¿…è¦æ­¥éª¤ã€‚"

	def get_schema(self) -> Dict[str, Any]: return {"name": "FinishAction",
													"description": "å½“é—®é¢˜å·²è§£å†³æˆ–æ— æ³•ç»§ç»­æ—¶è°ƒç”¨æ­¤åŠ¨ä½œç»“æŸè§„åˆ’ã€‚",
													"logic_input_schema": {"reason": "string (å¯é€‰ï¼Œç»“æŸåŸå› )"}}


# --- Planner (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„PlannerPrompt) ---
class Planner:
	def __init__(self, llm_client: OpenAIChatLLM, prompt: PlannerPrompt):
		self.llm_client = llm_client; self.prompt_template = prompt

	async def plan_next_steps(self, user_query: str, context: ContextManager, available_executors: List[Dict]) -> Tuple[
		str, str, List[Dict]]:
		exec_desc_parts = [
			f"  - åç§°: \"{s['name']}\"\n    æè¿°: \"{s['description']}\"\n    è¾“å…¥å‚æ•°æ¨¡å¼ (logic_input_schema): {json.dumps(s.get('logic_input_schema', 'N/A'), ensure_ascii=False)}"
			for s in available_executors]
		exec_desc = "\n".join(exec_desc_parts)
		history_str = context.get_task_history_for_prompt()
		user_prompt_str = self.prompt_template.format(user_query=user_query, available_executors_description=exec_desc,
													  task_history=history_str)
		# print(f"  [Planner] Sending planning prompt (history length: {len(history_str)} chars)...")
		response_json = await self.llm_client.generate_structured_json(user_prompt_str,
																	   system_prompt_str=PlannerPrompt.SYSTEM_PROMPT,
																	   temperature=0.0)  # Planner needs to be deterministic
		plan_status = response_json.get("plan_status", "error");
		final_thought = response_json.get("final_thought", "LLMæœªèƒ½æä¾›è§„åˆ’æ€è€ƒã€‚");
		next_steps_data = response_json.get("next_steps", [])
		if not isinstance(next_steps_data, list): print(
			f"  [Planner Err] LLM 'next_steps' not list. Got: {next_steps_data}. Assuming none."); next_steps_data = []
		valid_steps = [td for td in next_steps_data if isinstance(td, dict) and all(
			k in td for k in ["id", "executor_name", "task_description", "logic_input"])]
		if len(valid_steps) != len(next_steps_data): print(f"  [Planner Warn] Some planned steps were invalid.")
		return plan_status, final_thought, valid_steps


# --- AnswerGenerator (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨UserProvidedReferGeneratorPrompt) ---
class AnswerGenerator:
	def __init__(self, llm_client: OpenAIChatLLM,
				 prompt: UserProvidedReferGeneratorPrompt): self.llm_client = llm_client; self.prompt_template = prompt

	async def generate_final_answer(self, user_query: str, context: ContextManager) -> str:
		summary = context.get_summary_for_generator();
		refs = context.collect_retrieved_references_for_generator()
		prompt_str = self.prompt_template.format(summary_of_executed_steps=summary, user_query=user_query,
												 retrieved_references=refs)
		# print(f"  [Generator] Sending generation prompt (summary length: {len(summary)}, refs: {len(refs)})...")
		return await self.llm_client.generate(
			prompt_str)  # System prompt might be part of UserProvidedReferGeneratorPrompt's template


# --- Pipeline (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„è¿­ä»£é€»è¾‘) ---
class IterativePipeline:
	def __init__(self, planner: Planner, executors: Dict[str, ExecutorBase], generator: AnswerGenerator,
				 max_iterations: int = 5):
		self.planner = planner;
		self.executors = executors;
		self.generator = generator;
		self.max_iterations = max_iterations

	async def _execute_task_dag_segment(self, tasks_data: List[Dict], ctx: ContextManager, iter_num: int) -> bool:
		if not tasks_data: return True
		prefix = re.sub(r'[^\w\s-]', '', ctx.user_query[:10]).strip().replace(' ', '_') or "q"
		added_ids = [ctx.add_task_from_planner(td, prefix, iter_num, i).id for i, td in enumerate(tasks_data)]
		# print(f"  [Pipeline Iter {iter_num}] Added tasks: {added_ids}")
		# for tid in added_ids: task = ctx.get_task(tid); print(f"    - ID:{task.id}, Exec:{task.executor_name}, Desc:\"{task.task_description}\", Deps:{task.dependencies}")
		cache = set();
		success = True
		for tid in added_ids:
			task = ctx.get_task(tid)
			if task and task.status == "pending":
				await self._execute_task_with_dependencies(tid, ctx, cache)
				if ctx.get_task(tid).status != "completed": success = False
		return success

	async def _execute_task_with_dependencies(self, task_id: str, ctx: ContextManager, cache: set):
		task = ctx.get_task(task_id)
		if not task: print(f"  [Pipe Err] Task {task_id} def not found."); return
		if task.status != "pending": return
		cache.add(task_id)
		for dep_id in task.dependencies:
			dep_task = ctx.get_task(dep_id)
			if not dep_task: emsg = f"T {task.id} undef dep ID: {dep_id}. Failed."; ctx.update_task_status(task.id,
																										   "failed",
																										   result=emsg,
																										   thought=emsg); print(
				f"  [Pipe Err] {emsg}"); return
			if dep_task.status == "pending": await self._execute_task_with_dependencies(dep_id, ctx, cache)
		deps_met = True
		if task.dependencies:
			for dep_id in task.dependencies:
				dep_task = ctx.get_task(dep_id)
				if not dep_task or dep_task.status != "completed":
					deps_met = False;
					ts = (
									 task.thought or "") + f"Dep {dep_id} not met (status: {dep_task.status if dep_task else 'N/A'}). Skip."
					ctx.update_task_status(task.id, "skipped", thought=ts);
					print(f"  [Pipe] Task {task.id} skip, dep {dep_id} not met.");
					break
			if not deps_met: return
		if task.status != "pending": return
		executor = self.executors.get(task.executor_name)
		if not executor: emsg = f"Exec '{task.executor_name}' not found for task '{task.task_description}'."; ctx.update_task_status(
			task.id, "failed", result=emsg, thought=emsg); print(f"  [Pipe Err] {emsg}"); return
		print(f"\nâ–¶ï¸ Iter Exec Task: {task.id} - \"{task.task_description}\" ({task.executor_name})")
		ctx.update_task_status(task.id, "running", thought=f"Start: {task.task_description}")
		try:
			result = await executor.execute(task, ctx)
			ctx.update_task_status(task.id, "completed", result=result, thought=task.thought)
		# print(f"âœ… Task {task.id} Result (short): {str(result)[:100]}{'...' if result and len(str(result)) > 100 else ''}")
		except ExecutorError as e:
			emsg = f"ExecErr T {task.id}: {e}"; ft = (task.thought or "") + f"\nExecErr: {e}"; ctx.update_task_status(
				task.id, "failed", result=emsg, thought=ft); print(f"ğŸ›‘ {emsg}")
		except Exception as e:
			emsg = f"UnexpectedErr T {task.id}: {e}"; import traceback; tb = traceback.format_exc(); print(
				f"ğŸ›‘ {emsg}\n{tb}"); ft = (task.thought or "") + f"\nUnexpectedErr: {e}"; ctx.update_task_status(task.id,
																												"failed",
																												result=emsg,
																												thought=ft)

	# In class IterativePipeline:
	async def run(self, user_query: str) -> str:
		print(f"\nğŸš€ IterativePipeline starting for query: \"{user_query}\"")
		context = ContextManager(user_query)
		available_executors_schemas = [ex.get_schema() for ex in self.executors.values()]

		current_plan_status = "requires_more_steps"  # Initial status
		i_iter = 0  # Initialize iteration counter

		for i_iter in range(self.max_iterations):
			current_iteration_num = i_iter + 1
			print(f"\n--- Iteration {current_iteration_num} / {self.max_iterations} ---")

			print(f"ğŸ“ Planning phase (Iteration {current_iteration_num})...")
			try:
				plan_status, planner_thought, next_steps_data = await self.planner.plan_next_steps(
					user_query, context, available_executors_schemas
				)
				print(f"  [Planner Output] Status: {plan_status}, Thought: {planner_thought}")
				if next_steps_data:
					print(
						f"  [Planner Output] Next Steps Planned ({len(next_steps_data)}): {[s.get('task_description', 'N/A') for s in next_steps_data]}")
				else:
					print("  [Planner Output] No new steps planned for this iteration.")

				current_plan_status = plan_status  # Update overall plan status

			# Log planner's thought, but don't let it become the final answer directly
			# context.update_task_status(f"planner_thought_iter_{current_iteration_num}", "info", thought=planner_thought) # Optional logging

			# ... (error handling for planning as before) ...
			except Exception as e:
				# ... (error handling) ...
				break  # Break from loop on planning error

			if current_plan_status == "finished":
				print("  [Pipeline] Planner determined the process is finished. Proceeding to generation.")
				break
			if current_plan_status == "cannot_proceed":
				print("  [Pipeline] Planner determined it cannot proceed.")
				# final_answer will be set by generator based on current context and this thought
				# We can pass this thought to the generator via context if needed, or generator just uses task history
				# For now, let generator synthesize based on available task results.
				break

			if not next_steps_data:
				print(
					"  [Pipeline] No new steps planned by Planner, and not explicitly finished. Will proceed to generation with current context.")
				break

			print(f"\nâš™ï¸ Execution phase (Iteration {current_iteration_num})...")
			segment_successful = await self._execute_task_dag_segment(next_steps_data, context, current_iteration_num)

			if not segment_successful:
				print(
					f"  [Pipeline Warning] Iteration {current_iteration_num} encountered errors. Planner will try to adapt.")

			finish_task_executed = any(
				task.executor_name == "FinishAction" and task.status == "completed"
				for task in context.tasks.values() if task.id in [data["id"] for data in next_steps_data]
			)
			if finish_task_executed:
				print(f"  [Pipeline] FinishAction task completed. Ending iterations.")
				current_plan_status = "finished"  # Ensure status reflects finish
				break

		# --- Generation Phase ---
		# This phase is ALWAYS called after the loop finishes or breaks,
		# unless a critical unrecoverable error occurred earlier.
		print(f"\nğŸ’¬ Generation phase after {i_iter + 1} iteration(s) (or fewer if loop broke early)...")
		final_answer = "æœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"  # Default if generator fails
		try:
			# If planner said "cannot_proceed", its thought is valuable context for the generator
			if current_plan_status == "cannot_proceed" and planner_thought:
				# We can prepend this to the summary or pass it specially
				# For now, let the generator work with the task history; planner_thought is mainly for control flow.
				print(f"  [Generator] Note: Planner indicated 'cannot_proceed'. Reason: {planner_thought}")

			final_answer = await self.generator.generate_final_answer(user_query, context)
		except RuntimeError as rte:
			print(
				f"  [Pipeline Error] Generation LLM call failed: {rte}"); final_answer = f"ç”Ÿæˆç­”æ¡ˆæ—¶è¿æ¥LLMå¤±è´¥ï¼š{rte}"
		except Exception as e:
			print(f"  [Pipeline Error] Unexpected error during generation: {e}"); import \
				traceback; traceback.print_exc(); final_answer = f"ç”Ÿæˆç­”æ¡ˆé˜¶æ®µæ„å¤–é”™è¯¯ï¼š{e}"

		print(f"\nğŸ’¡ Final Answer: {final_answer}")
		return final_answer


# _execute_task_dag_segment å’Œ _execute_task_with_dependencies ä¿æŒä¸å˜
# ... (rest of the IterativePipeline class as before) ...

# --- ä¸»ç¨‹åºå…¥å£ ---
async def run_main_logic_with_user_data_recursive_optimized():
	# --- LLM and Embedding Setup ---
	# (Same as previous, ensure env vars are set: OPENAI_API_KEY, OPENAI_BASE_URL, LLM_MODEL_NAME, DASHSCOPE_API_KEY_FOR_EMBEDDING)
	api_key = 'sk-af4423da370c478abaf68b056f547c6e'
	base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
	model_name = os.getenv("LLM_MODEL_NAME", "qwen-plus")
	if "YOUR_API_KEY" in api_key: print("é”™è¯¯ï¼šè¯·è®¾ç½®æœ‰æ•ˆçš„ OPENAI_API_KEYã€‚"); return
	llm_client = OpenAIChatLLM(model_name=model_name, api_key=api_key, base_url=base_url)
	try:
		from tongyiembedding import QwenEmbeddingFunction

		embedding_function = QwenEmbeddingFunction(api_key='sk-af4423da370c478abaf68b056f547c6e')
	except Exception as e:
		print(f"  [Embedding Error] QwenEmbeddingFunction åˆå§‹åŒ–å¤±è´¥: {e}"); return

	# --- Knowledge Base Data ---
	initial_user_docs_as_dicts = [
		{"page_content": "ã€æ­£æŸ´èƒ¡é¥®é¢—ç²’ã€‘æ£€æŸ¥: åº”ç¬¦åˆé¢—ç²’å‰‚é¡¹ä¸‹æœ‰å…³çš„å„é¡¹è§„å®šï¼ˆé€šåˆ™0104)ã€‚è¿™æ˜¯ä¸»è¦æ£€æŸ¥ä¾æ®ã€‚",
		 "metadata": {"id": "doc_zchyk_check", "source_name": "æ­£æŸ´èƒ¡é¥®é¢—ç²’è¯´æ˜ä¹¦-æ£€æŸ¥ç« èŠ‚"}},
		{
			"page_content": "è¯å…¸é€šåˆ™0104 - é¢—ç²’å‰‚æ£€æŸ¥è¦ç‚¹ï¼šã€ç²’åº¦ã€‘è¦æ±‚ä¸èƒ½é€šè¿‡ä¸€å·ç­›ä¸èƒ½é€šè¿‡äº”å·ç­›çš„æ€»å’Œä¸å¾—è¶…è¿‡15ï¼…ã€‚ã€æ°´åˆ†ã€‘ä¸­è¯é¢—ç²’å‰‚æ°´åˆ†ä¸å¾—è¶…è¿‡8.0ï¼…ã€‚ã€æº¶åŒ–æ€§ã€‘å¯æº¶é¢—ç²’5åˆ†é’Ÿå†…å…¨éƒ¨æº¶åŒ–æˆ–å‘ˆè½»å¾®æµ‘æµŠï¼›æ³¡è…¾é¢—ç²’5åˆ†é’Ÿå†…å®Œå…¨åˆ†æ•£æˆ–æº¶è§£ã€‚å‡ä¸å¾—æœ‰å¼‚ç‰©ï¼Œä¸­è¯é¢—ç²’è¿˜ä¸å¾—æœ‰ç„¦å±‘ã€‚ã€è£…é‡å·®å¼‚ã€‘å•å‰‚é‡åŒ…è£…ä¸å¹³å‡è£…é‡æ¯”è¾ƒï¼Œå·®å¼‚éœ€åœ¨é™åº¦å†…ï¼Œä¾‹å¦‚1gä»¥ä¸‹ä¸ºÂ±10%ã€‚ã€å¾®ç”Ÿç‰©é™åº¦ã€‘éœ€ç¬¦åˆéæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ã€‚",
			"metadata": {"id": "doc_tongze0104_summary_v2", "source_name": "è¯å…¸é€šåˆ™0104æ ¸å¿ƒæ‘˜è¦"}},
		{
			"page_content": "é€šåˆ™0104è¯¦ç»†è¯´æ˜ä¹‹ã€æ°´åˆ†æµ‹å®šã€‘ï¼šé‡‡ç”¨ç”²è‹¯æ³•æˆ–å‡å‹å¹²ç‡¥æ³•ã€‚å¯¹äºå«ç³–æˆ–æ˜“ç†”åŒ–è¾…æ–™çš„é¢—ç²’ï¼Œå®œåœ¨è¾ƒä½æ¸©åº¦ï¼ˆå¦‚60-80â„ƒï¼‰å‡å‹å¹²ç‡¥è‡³æ’é‡ã€‚",
			"metadata": {"id": "doc_tongze0104_water", "source_name": "è¯å…¸é€šåˆ™0104-æ°´åˆ†æµ‹å®šç»†èŠ‚"}},
		{
			"page_content": "é€šåˆ™0104è¯¦ç»†è¯´æ˜ä¹‹ã€ç²’åº¦åˆ†å¸ƒã€‘ï¼šä½¿ç”¨æ ‡å‡†è¯ç­›è¿›è¡Œç­›åˆ†ï¼Œè®°å½•å„ç­›ä¸Šç‰©åŠç­›ä¸‹ç‰©çš„é‡é‡ç™¾åˆ†æ¯”ã€‚å¯¹äºéš¾æº¶æ€§è¯ç‰©ï¼Œéœ€æ³¨æ„å…¶åœ¨ç‰¹å®šä»‹è´¨ä¸­çš„åˆ†æ•£æ€§ã€‚",
			"metadata": {"id": "doc_tongze0104_size", "source_name": "è¯å…¸é€šåˆ™0104-ç²’åº¦åˆ†å¸ƒç»†èŠ‚"}}
	]
	initial_langchain_docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in
							  initial_user_docs_as_dicts]
	from md2Document import read_md_file, create_document_from_md
	initial_langchain_docs = []
	folder_path = r'D:\Master\llm\database\kag\æµ‹è¯•æ•°æ®é›†'  # Windowsè·¯å¾„
	# è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶
	files = os.listdir(folder_path)
	# ç­›é€‰å‡ºæ‰€æœ‰ .md æ–‡ä»¶
	md_files = [file for file in files if file.endswith('.md')]
	# åˆ›å»ºDocumentå¯¹è±¡åˆ—è¡¨

	for file in md_files:
		file_path = os.path.join(folder_path, file)  # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
		document = create_document_from_md(file_path)  # åˆ›å»ºDocumentå¯¹è±¡
		initial_langchain_docs.append(document)
	try:
		chroma_kb = ChromaKnowledgeBase(initial_documents=initial_langchain_docs, embedding_function=embedding_function,
										force_rebuild=False,
										persist_directory = 'chroma_db_kag_recursive_1')
	except Exception as e:
		print(f"åˆ›å»ºChromaçŸ¥è¯†åº“å¤±è´¥: {e}"); import traceback; traceback.print_exc(); return

	# --- Initialize Components ---
	planner_prompt = PlannerPrompt()  # Uses the new reflective prompt
	deduce_prompt_template = DeducePrompt()  # The one that asks for structured output
	code_exec_prompt = CodeExecutionPrompt()
	refer_generator_prompt = UserProvidedReferGeneratorPrompt(language="zh")

	retrieval_executor = RetrievalExecutor(kb=chroma_kb)
	deduce_executor = DeduceExecutor(llm_client, deduce_prompt_template)  # Default deduce prompt
	code_executor = CodeExecutor(llm_client, code_exec_prompt)
	finish_executor = FinishExecutor()

	executors_map = {
		"RetrievalExecutor": retrieval_executor, "DeduceExecutor": deduce_executor,
		"CodeExecutor": code_executor, "FinishAction": finish_executor
	}

	planner = Planner(llm_client, planner_prompt)
	generator = AnswerGenerator(llm_client, refer_generator_prompt)
	pipeline = IterativePipeline(planner, executors_map, generator, max_iterations=4)  # Max 4 iterations

	# --- Run Query ---
	# user_query_to_run = "æ­£æŸ´èƒ¡é¥®é¢—ç²’çš„æ£€æŸ¥å†…å®¹æœ‰å“ªäº›æ–¹é¢ï¼Ÿè¯·è¯¦ç»†è¯´æ˜ã€‚"
	user_query_to_run = "æ­£æŸ´èƒ¡é¥®é¢—ç²’çš„ä¸»è¦æ£€æŸ¥æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"

	print(f"\nğŸš€ Running RECURSIVE-LIKE optimized query: \"{user_query_to_run}\"")
	final_answer = await pipeline.run(user_query_to_run)
	print(f"\nğŸğŸğŸğŸğŸ RECURSIVE-LIKE FINAL ANSWER (for query: '{user_query_to_run}') ğŸğŸğŸğŸğŸ\n{final_answer}")


if __name__ == "__main__":
	print("å¼€å§‹æ‰§è¡Œâ€œç±»é€’å½’â€ä¼˜åŒ–ç‰ˆä¸»é€»è¾‘...")
	# ... (ç¯å¢ƒå˜é‡å’Œä¾èµ–æç¤º) ...
	# Forcing API Key for this test run in case environment variables are not set by user.
	# USER SHOULD REPLACE THIS WITH THEIR ACTUAL KEY OR ENV VARS.
	# THIS IS NOT SAFE FOR PRODUCTION.

	asyncio.run(run_main_logic_with_user_data_recursive_optimized())
