import asyncio
import json
import os
import re
import subprocess  # ç”¨äºå®‰å…¨æ‰§è¡Œä»£ç 
import sys
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
import shutil  # ç”¨äºåˆ é™¤ç›®å½•

# --- OpenAI/LLM Client Configuration ---
# (ä¸ä¸Šä¸€è½®ç›¸åŒï¼Œç”¨æˆ·éœ€è®¾ç½®ç¯å¢ƒå˜é‡)

# --- Langchain and Chroma Imports (æ¥è‡ªæ‚¨çš„ä»£ç ) ---
from langchain_core.documents import Document
from langchain_chroma import Chroma
# Embedding function - ä½¿ç”¨æ‚¨æä¾›çš„ DashScope (QwenEmbeddingFunction)
# from langchain_community.embeddings import DashScopeEmbeddings # ç›´æ¥ç”¨ä¸‹é¢çš„åŒ…è£…ç±»
from tongyiembedding import QwenEmbeddingFunction  # æ¥è‡ªæ‚¨çš„ä»£ç 

# --- ChromaKnowledgeBase Class (ç›´æ¥ä»æ‚¨çš„ä»£ç ä¸­å¤åˆ¶è¿‡æ¥) ---
CHROMA_PERSIST_DIRECTORY = "chroma_db_kag_concrete"  # æ”¹ä¸ªåå­—ä»¥é˜²å†²çª
CHROMA_COLLECTION_NAME = "kag_concrete_documents"


class ChromaKnowledgeBase:
	def __init__(self,
				 embedding_function: Callable,
				 initial_documents: Optional[List[Document]] = None,
				 persist_directory: str = CHROMA_PERSIST_DIRECTORY,
				 collection_name: str = CHROMA_COLLECTION_NAME,
				 force_rebuild: bool = False):

		print(f"  [ChromaKB] åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ŒæŒä¹…åŒ–ç›®å½•: {persist_directory}, é›†åˆ: {collection_name}")
		self.embedding_function = embedding_function
		self.persist_directory = persist_directory
		self.collection_name = collection_name
		self.vectorstore: Optional[Chroma] = None

		if force_rebuild and os.path.exists(persist_directory):
			print(f"  [ChromaKB] force_rebuild=True, æ­£åœ¨åˆ é™¤å·²å­˜åœ¨çš„æŒä¹…åŒ–ç›®å½•: {persist_directory}")
			try:
				shutil.rmtree(persist_directory)
			except OSError as e:
				print(f"  [ChromaKB Error] åˆ é™¤ç›®å½•å¤±è´¥: {e}. å¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ é™¤ã€‚")

		if os.path.exists(persist_directory) and not force_rebuild:
			print(f"  [ChromaKB] æ­£åœ¨ä» '{persist_directory}' åŠ è½½å·²å­˜åœ¨çš„ Chroma å‘é‡åº“...")
			try:
				self.vectorstore = Chroma(
					persist_directory=self.persist_directory,
					embedding_function=self.embedding_function,
					collection_name=self.collection_name
				)
				print(f"  [ChromaKB] æˆåŠŸåŠ è½½å‘é‡åº“ã€‚é›†åˆ '{self.collection_name}'.")
			except Exception as e:
				print(f"  [ChromaKB Error] ä» '{persist_directory}' åŠ è½½å¤±è´¥: {e}")
				print(f"  [ChromaKB] å°†å°è¯•åŸºäºæä¾›çš„ initial_documents (å¦‚æœå­˜åœ¨) åˆ›å»ºæ–°çš„å‘é‡åº“ã€‚")
				self.vectorstore = None

		if self.vectorstore is None:
			if initial_documents:
				print(f"  [ChromaKB] æ­£åœ¨ä¸º {len(initial_documents)} ä¸ªæ–‡æ¡£æ„å»ºæ–°çš„ Chroma å‘é‡åº“...")
				self.vectorstore = Chroma.from_documents(
					documents=initial_documents,
					embedding=self.embedding_function,
					persist_directory=self.persist_directory,
					collection_name=self.collection_name
				)
				print(f"  [ChromaKB] æ–°å‘é‡åº“æ„å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚")
			else:
				print(f"  [ChromaKB] æ²¡æœ‰æä¾›åˆå§‹æ–‡æ¡£ï¼Œä¸”æœªåŠ è½½ç°æœ‰å‘é‡åº“ã€‚çŸ¥è¯†åº“å°†ä¸ºç©ºã€‚")
				self.vectorstore = Chroma(  # åˆ›å»ºä¸€ä¸ªç©ºçš„ï¼Œä½†å¯æŒä¹…åŒ–çš„
					persist_directory=self.persist_directory,
					embedding_function=self.embedding_function,
					collection_name=self.collection_name
				)
				print(f"  [ChromaKB] ç©ºçš„æŒä¹…åŒ– Chroma é›†åˆ '{self.collection_name}' å·²å‡†å¤‡å°±ç»ªã€‚")

	def add_documents(self, documents: List[Document]):
		if not self.vectorstore:
			print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£ã€‚")
			# è€ƒè™‘åœ¨è¿™é‡Œåˆ›å»ºï¼Œå¦‚æœ initial_documents ä¸ºç©ºæ—¶æ²¡æœ‰åˆ›å»º
			if documents:
				print(f"  [ChromaKB] Vectorstore ä¸ºç©º, å°è¯•ä»å½“å‰æ–‡æ¡£åˆ›å»º...")
				self.vectorstore = Chroma.from_documents(
					documents=documents,
					embedding=self.embedding_function,
					persist_directory=self.persist_directory,
					collection_name=self.collection_name
				)
				print(f"  [ChromaKB] åŸºäºæ–°æ–‡æ¡£åˆ›å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚")
				return  # å·²ç»æ·»åŠ äº†
			else:  # æ²¡æœ‰æ–‡æ¡£å¯æ·»åŠ ï¼Œä¹Ÿæ²¡æœ‰vectorstore
				return

		if documents:
			print(f"  [ChromaKB] æ­£åœ¨å‘é›†åˆ '{self.collection_name}' æ·»åŠ  {len(documents)} ä¸ªæ–°æ–‡æ¡£...")
			self.vectorstore.add_documents(documents)
			print(f"  [ChromaKB] æ–‡æ¡£æ·»åŠ å®Œæˆã€‚")

	def retrieve(self, query: str, top_k: int = 3, filter_dict: Optional[Dict] = None) -> List[Dict[str, Any]]:
		# è¿”å› List[Dict[str, Any]]ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« "content", "metadata", "score"
		if not self.vectorstore:
			print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ£€ç´¢ã€‚")
			return []
		try:
			# æ£€æŸ¥é›†åˆæ˜¯å¦çœŸçš„å­˜åœ¨å¹¶ä¸”å¯ä»¥è®¡æ•°
			if self.vectorstore._collection is None or self.vectorstore._collection.count() == 0:
				print("  [ChromaKB] çŸ¥è¯†åº“é›†åˆä¸ºç©ºæˆ–æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•æ£€ç´¢ã€‚")
				return []
		except Exception as e:  # æ•è·å¯èƒ½çš„ Chroma client æˆ– collection è®¿é—®é”™è¯¯
			print(f"  [ChromaKB Warning] æ— æ³•è·å–é›†åˆè®¡æ•°æˆ–é›†åˆæ— æ•ˆï¼Œå°†å°è¯•æ£€ç´¢: {e}")
		# å³ä½¿æ— æ³•è®¡æ•°ï¼Œä¹Ÿå°è¯•æ£€ç´¢ï¼Œè®©similarity_searchè‡ªå·±æŠ¥é”™ï¼ˆå¦‚æœåº•å±‚æœ‰é—®é¢˜ï¼‰

		print(f"  [ChromaKB] æ­£åœ¨ä¸ºæŸ¥è¯¢ '{query}' æ£€ç´¢æœ€ç›¸å…³çš„ {top_k} ä¸ªæ–‡æ¡£ (è¿‡æ»¤å™¨: {filter_dict})...")
		try:
			results_with_scores = self.vectorstore.similarity_search_with_score(
				query, k=top_k, filter=filter_dict
			)
		except Exception as e:
			print(f"  [ChromaKB Error] Chroma similarity search failed: {e}")
			return []

		processed_results = []
		if results_with_scores:
			for doc, score in results_with_scores:
				processed_results.append({
					"id": doc.metadata.get("id", None),
					"content": doc.page_content,
					"metadata": doc.metadata,
					"score": float(score)
				})
		print(f"  [ChromaKB] æ£€ç´¢åˆ° {len(processed_results)} ä¸ªæ–‡æ¡£ã€‚")
		return processed_results


# --- LLMå®¢æˆ·ç«¯ (OpenAIChatLLM from previous response) ---
class OpenAIChatLLM:
	def __init__(self, model_name: Optional[str] = None, api_key: Optional[str] = None, base_url: Optional[str] = None):
		try:
			import openai
		except ImportError:
			raise ImportError("OpenAI library not found. Please install it with `pip install openai`.")

		self.api_key = 'sk-af4423da370c478abaf68b056f547c6e'
		self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
		self.model_name = model_name or os.getenv("LLM_MODEL_NAME", "qwen-plus")

		if not self.api_key:
			raise ValueError(
				"API key not found. Please set OPENAI_API_KEY environment variable or pass it to the constructor.")

		self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
		print(f"[OpenAIChatLLM] å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†ä½¿ç”¨æ¨¡å‹: {self.model_name} at {self.base_url or 'OpenAI default'}")

	async def _make_api_call(self, messages: List[Dict[str, str]], expect_json: bool = False, **kwargs) -> str:
		try:
			completion_params = {
				"model": self.model_name,
				"messages": messages,
				"temperature": 0.1,
				**kwargs
			}
			if expect_json:
				if "dashscope" in (self.base_url or "").lower() and self.model_name.startswith("qwen"):
					completion_params["extra_body"] = {"result_format": "message"}
					print("  [OpenAIChatLLM] DashScope Qwenæ¨¡å‹ï¼Œè®¾ç½® result_format: message ä»¥æœŸå¾…JSON")
				else:
					completion_params["response_format"] = {"type": "json_object"}
					print("  [OpenAIChatLLM] è®¾ç½® response_format: json_object")

			# print(f"  [OpenAIChatLLM] Calling model with params: model={completion_params['model']}, temp={completion_params['temperature']}")
			# print(f"  [OpenAIChatLLM] Messages (brief): Role {messages[-1]['role']}, Content: {messages[-1]['content'][:100]}...")

			response = await asyncio.to_thread(self.client.chat.completions.create, **completion_params)
			content = response.choices[0].message.content
			if not content:
				print("  [OpenAIChatLLM Error] æœªèƒ½ä»APIå“åº”ä¸­è·å–å†…å®¹.")
				return ""
			return content.strip()
		except Exception as e:
			print(f"  [OpenAIChatLLM Error] APIè°ƒç”¨å¤±è´¥: {e}")
			import traceback
			traceback.print_exc()
			raise RuntimeError(f"LLM API call failed: {e}")

	async def generate(self, prompt_str: str, system_prompt_str: Optional[str] = None, **kwargs) -> str:
		messages = []
		if system_prompt_str:
			messages.append({"role": "system", "content": system_prompt_str})
		messages.append({"role": "user", "content": prompt_str})
		return await self._make_api_call(messages, expect_json=False, **kwargs)

	async def generate_structured_json(self, prompt_str: str, system_prompt_str: Optional[str] = None,
									   **kwargs) -> Dict:
		messages = []
		if system_prompt_str:
			messages.append({"role": "system", "content": system_prompt_str})
		user_content = f"{prompt_str}\n\nè¯·ç¡®ä¿æ‚¨çš„å›å¤æ˜¯ä¸€ä¸ªåˆæ³•çš„ã€å•ç‹¬çš„JSONå¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡æœ¬æˆ–markdownæ ‡è®°ã€‚"
		messages.append({"role": "user", "content": user_content})

		response_str = await self._make_api_call(messages, expect_json=True, **kwargs)

		cleaned_response_str = response_str.strip()
		if cleaned_response_str.startswith("```json"):
			cleaned_response_str = cleaned_response_str[7:]
		if cleaned_response_str.endswith("```"):
			cleaned_response_str = cleaned_response_str[:-3]
		cleaned_response_str = cleaned_response_str.strip()

		try:
			return json.loads(cleaned_response_str)
		except json.JSONDecodeError as e:
			error_msg = f"LLM did not return valid JSON. Error: {e}. Raw response: '{response_str}'"
			print(f"  [OpenAIChatLLM Error] {error_msg}")
			raise ValueError(error_msg)


# --- Prompts (from previous response, ensure PlannerPrompt is updated) ---
class BasePrompt:
	def __init__(self, template: str, variables: List[str]):
		self.template_str = template
		self.variables = variables

	def format(self, **kwargs) -> str:
		formatted_prompt = self.template_str
		for var_name in self.variables:
			if var_name not in kwargs:
				raise ValueError(f"Missing variable: {var_name} for prompt template.")
			formatted_prompt = formatted_prompt.replace(f"${{{var_name}}}", str(kwargs[var_name]))
		return formatted_prompt


class PlannerPrompt(BasePrompt):
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä¸ªé«˜åº¦æ™ºèƒ½çš„AIä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚
    ä½ çš„ç›®æ ‡æ˜¯æ ¹æ®ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜ï¼Œå°†å…¶åˆ†è§£ä¸ºä¸€ç³»åˆ—é€»è¾‘æ¸…æ™°ã€å¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚
    ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä»»åŠ¡è®¡åˆ’ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ•´ä¸ªè®¡åˆ’æ˜¯ä¸€ä¸ªJSONåˆ—è¡¨ã€‚

    ä»»åŠ¡å¯¹è±¡ç»“æ„:
    {
      "id": "string (ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦, ä¾‹å¦‚ task_0, task_userquery_1)",
      "executor_name": "string (æ‰§è¡Œè¯¥ä»»åŠ¡çš„å·¥å…·åç§°, ä»æä¾›çš„å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©)",
      "task_description": "string (å¯¹è¿™ä¸ªå­ä»»åŠ¡çš„ç®€çŸ­ä¸­æ–‡æè¿°)",
      "logic_input": {
        // è¿™ä¸ªå¯¹è±¡çš„å…·ä½“å­—æ®µå–å†³äºæ‰€é€‰çš„ executor_name
        // è¯·å‚è€ƒä¸‹é¢å¯ç”¨å·¥å…·æè¿°ä¸­æ¯ä¸ªå·¥å…·çš„ 'logic_input_schema'
      },
      "dependencies": ["string"] (ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å½“å‰ä»»åŠ¡æ‰€ä¾èµ–çš„å…¶ä»–ä»»åŠ¡çš„IDã€‚åˆå§‹ä»»åŠ¡æ­¤åˆ—è¡¨ä¸ºç©º)
    }

    å¤„ç†é€»è¾‘:
    1.  ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜å’Œå·²ç»æ‰§è¡Œçš„å†å²ä»»åŠ¡ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
    2.  å°†ç”¨æˆ·é—®é¢˜åˆ†è§£ä¸ºè‹¥å¹²ä¸ªåŸå­æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½åº”æ˜ç¡®å¯¹åº”ä¸€ä¸ªå¯ç”¨å·¥å…·ã€‚
    3.  ä¸ºæ¯ä¸ªæ­¥éª¤é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œå¹¶æ ¹æ®å·¥å…·çš„`logic_input_schema`å‡†å¤‡å…¶è¾“å…¥å‚æ•°ã€‚
    4.  **å…³é”®**ï¼šå¦‚æœä¸€ä¸ªä»»åŠ¡çš„è¾“å…¥ï¼ˆ`logic_input`ä¸­çš„å­—æ®µï¼‰éœ€è¦ä¾èµ–ä¹‹å‰ä»»åŠ¡çš„è¾“å‡ºï¼Œè¯·ä½¿ç”¨å ä½ç¬¦ `{{task_id.result}}` æ¥è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœ `task_1` çš„æŸä¸ªè¾“å…¥éœ€è¦ `task_0` çš„ç»“æœï¼Œåˆ™è¯¥è¾“å…¥å€¼åº”ä¸º `{{task_0.result}}`ã€‚è¿™é‡Œçš„ `task_id` å¿…é¡»æ˜¯å…ˆå‰æ­¥éª¤ä¸­å®šä¹‰çš„ `id`ã€‚
    5.  ç¡®ä¿æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªç¬¦åˆä¸Šè¿°ç»“æ„çš„JSONåˆ—è¡¨ã€‚ä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€æ³¨é‡Šæˆ–markdownæ ‡è®°ã€‚åªè¾“å‡ºJSONã€‚
    """
	USER_TEMPLATE = """
    å¯ç”¨å·¥å…·å¦‚ä¸‹:
    ---BEGIN EXECUTOR DESCRIPTIONS---
    ${available_executors_description}
    ---END EXECUTOR DESCRIPTIONS---

    å†å²ä»»åŠ¡åŠç»“æœ (å¦‚æœå­˜åœ¨):
    ---BEGIN TASK HISTORY---
    ${task_history}
    ---END TASK HISTORY---

    å½“å‰ç”¨æˆ·é—®é¢˜: "${user_query}"

    è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯ï¼Œä¸ºè§£å†³å½“å‰ç”¨æˆ·é—®é¢˜åˆ¶å®šä»»åŠ¡è®¡åˆ’ã€‚
    è¾“å‡ºä»»åŠ¡è®¡åˆ’ (ä¸€ä¸ªJSONå¯¹è±¡åˆ—è¡¨):
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["user_query", "available_executors_description", "task_history"])


class DeducePrompt(BasePrompt):
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„AIæ¨ç†åŠ©æ‰‹ã€‚"
	USER_TEMPLATE = """
    è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”æˆ–è€…å®ŒæˆæŒ‡å®šçš„æ¨ç†ç›®æ ‡ã€‚
    è¯·ä¸¥æ ¼ä¾æ®æ‰€æä¾›çš„ä¸Šä¸‹æ–‡ä½œç­”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è¿›è¡Œä¸åˆç†çš„å‡è®¾ã€‚
    å¦‚æœä¿¡æ¯ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºâ€œä¿¡æ¯ä¸è¶³â€ã€‚

    æ¨ç†ç›®æ ‡:
    ${reasoning_goal}

    ä¸Šä¸‹æ–‡ä¿¡æ¯:
    ${context_data}

    ä½ çš„å›ç­”:
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["reasoning_goal", "context_data"])


class CodeExecutionPrompt(BasePrompt):
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ©æ‰‹ã€‚"
	USER_TEMPLATE = """
    è¯·æ ¹æ®ä»¥ä¸‹æŒ‡ä»¤å’Œç›¸å…³æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µPythonä»£ç æ¥è§£å†³é—®é¢˜ã€‚
    ä»£ç å¿…é¡»é€šè¿‡ `print()` è¾“å‡ºå…¶æœ€ç»ˆè®¡ç®—ç»“æœã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¾“å‡ºçº¯ä»£ç ã€‚

    æŒ‡ä»¤:
    ${code_generation_prompt}

    ç›¸å…³æ•°æ® (å¦‚æœæä¾›):
    ${relevant_data}

    ç”Ÿæˆçš„Pythonä»£ç  (è¯·ç¡®ä¿å®ƒåªåŒ…å«ä»£ç æœ¬èº«ï¼Œå¹¶ç”¨print()è¾“å‡ºç»“æœ):
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["code_generation_prompt", "relevant_data"])


class GenerationPrompt(BasePrompt):
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„AIæ²Ÿé€šåŠ©æ‰‹ã€‚"
	USER_TEMPLATE = """
    ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä»¥ä¸‹è§£å†³ç”¨æˆ·é—®é¢˜çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹å’Œå„æ­¥éª¤çš„æœ€ç»ˆç»“æœï¼Œç”Ÿæˆä¸€ä¸ªæµç•…ã€ç®€æ´ã€ç”¨æˆ·å‹å¥½çš„æœ€ç»ˆç­”æ¡ˆã€‚

    ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ˜¯: "${user_query}"

    è§£å†³è¿‡ç¨‹å’Œå„æ­¥éª¤ç»“æœ:
    ---BEGIN REASONING STEPS---
    ${reasoning_steps_summary}
    ---END REASONING STEPS---

    è¯·æ•´åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚å¦‚æœæŸäº›æ­¥éª¤æœªèƒ½æˆåŠŸæˆ–ä¿¡æ¯ä¸è¶³ï¼Œä¹Ÿè¯·åœ¨ç­”æ¡ˆä¸­æ°å½“è¯´æ˜ã€‚
    è¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦æ·»åŠ å¦‚â€œæœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šâ€è¿™æ ·çš„å‰ç¼€ã€‚
    æœ€ç»ˆç­”æ¡ˆ:
    """

	def __init__(self):
		super().__init__(self.USER_TEMPLATE, ["user_query", "reasoning_steps_summary"])


# --- DataStructures (from previous response) ---
LogicInput = Dict[str, Any]


@dataclass
class Task:
	id: str
	executor_name: str
	task_description: str
	logic_input: LogicInput
	dependencies: List[str] = field(default_factory=list)
	status: str = "pending"
	result: Optional[Any] = None
	thought: Optional[str] = None


# --- ContextManager (from previous response) ---
class ContextManager:
	def __init__(self, user_query: str):
		self.user_query = user_query
		self.tasks: Dict[str, Task] = {}
		self.execution_order: List[str] = []

	def add_task_from_planner(self, task_data: Dict, base_id_prefix: str, existing_task_count: int) -> Task:
		task_id = task_data.get("id")
		if not task_id or not isinstance(task_id, str):
			task_id = f"{base_id_prefix}_task_{existing_task_count}"
			print(
				f"  [ContextManager] Warning: Planner did not provide a valid string ID for a task. Generated ID: {task_id}")

		original_task_id = task_id
		counter = 0  # Start counter at 0 for the first potential conflict
		while task_id in self.tasks:
			counter += 1
			task_id = f"{original_task_id}_v{counter}"
		if counter > 0:
			print(
				f"  [ContextManager] Warning: Task ID '{original_task_id}' from planner already exists or was generated multiple times. Renamed to '{task_id}'.")

		task = Task(
			id=task_id,
			executor_name=task_data["executor_name"],
			task_description=task_data["task_description"],
			logic_input=task_data["logic_input"],
			dependencies=task_data.get("dependencies", [])
		)
		self.tasks[task.id] = task
		self.execution_order.append(task.id)
		return task

	def get_task(self, task_id: str) -> Optional[Task]:
		return self.tasks.get(task_id)

	def update_task_status(self, task_id: str, status: str, result: Optional[Any] = None,
						   thought: Optional[str] = None):
		task = self.get_task(task_id)
		if task:
			task.status = status
			if result is not None:
				task.result = result
			current_thought = task.thought or ""
			if thought:  # Append new thought to existing, if any
				task.thought = f"{current_thought}\n{thought}".strip() if current_thought else thought
		else:
			print(f"  [ContextManager] Warning: Task ID {task_id} not found for status update.")

	def get_task_history_for_prompt(self) -> str:
		history = []
		for task_id in self.execution_order:
			task = self.tasks.get(task_id)
			if task and task.status in ["completed", "failed"]:
				result_str = str(task.result)
				if len(result_str) > 200:
					result_str = result_str[:200] + "..."
				history.append(
					f"  - Task ID: {task.id}\n"
					f"    Description: {task.task_description}\n"
					f"    Executor: {task.executor_name}\n"
					f"    Status: {task.status}\n"
					f"    Result: {result_str}\n"
					f"    Thought: {task.thought or 'N/A'}"
				)
		return "\n".join(history) if history else "æ— å·²æ‰§è¡Œçš„å†å²ä»»åŠ¡ã€‚"

	def get_summary_for_generator(self) -> str:
		summary = []
		for i, task_id in enumerate(self.execution_order):
			task = self.tasks.get(task_id)
			if task:
				result_str = str(task.result)
				if len(result_str) > 300:
					result_str = result_str[:300] + "..."
				summary.append(
					f"æ­¥éª¤ {i + 1} (Task ID: {task.id}):\n"
					f"  æè¿°: {task.task_description}\n"
					f"  å·¥å…·: {task.executor_name}\n"
					f"  æ€è€ƒè¿‡ç¨‹: {task.thought or 'æœªè®°å½•'}\n"
					f"  çŠ¶æ€: {task.status}\n"
					f"  ç»“æœ: {result_str if task.status == 'completed' else 'æœªå®Œæˆæˆ–å¤±è´¥'}"
				)
		return "\n\n".join(summary)


# --- Executors (from previous response, RetrievalExecutor modified) ---
class ExecutorError(Exception): pass


class ExecutorBase(ABC):
	def __init__(self, llm_client: Optional[OpenAIChatLLM] = None):
		self.llm_client = llm_client

	@abstractmethod
	async def execute(self, task: Task, context: ContextManager) -> Any:
		pass

	@abstractmethod
	def get_schema(self) -> Dict[str, Any]:
		pass

	def _resolve_references(self, data_template: Any, context: ContextManager) -> Any:
		if isinstance(data_template, str):
			def replace_match(match):
				ref_full = match.group(1).strip()
				task_id_ref, attr_ref = ref_full.split('.', 1) if '.' in ref_full else (
				ref_full, "result")  # Default to result

				ref_task = context.get_task(task_id_ref)
				if ref_task and ref_task.status == "completed":
					if attr_ref == "result":
						if isinstance(ref_task.result, list):
							# å°†åˆ—è¡¨ç»“æœæ ¼å¼åŒ–ä¸ºå¤šè¡Œå­—ç¬¦ä¸²ï¼Œä¾›LLMæ¶ˆè´¹
							return "\n".join([f"- {item}" for item in map(str, ref_task.result)])
						return str(ref_task.result)
					else:
						print(f"  [Executor Warning] ä¸æ”¯æŒå¼•ç”¨ä»»åŠ¡å±æ€§ '{attr_ref}' in {{ {ref_full} }}.")
						return match.group(0)
				else:
					status_msg = f"(Task '{task_id_ref}' not found or not completed: status={ref_task.status if ref_task else 'not_found'})"
					print(f"  [Executor Warning] æ— æ³•è§£æå¼•ç”¨ {{ {ref_full} }}. {status_msg}")
					return f"{{å¼•ç”¨é”™è¯¯: {ref_full} {status_msg}}}"

			return re.sub(r"\{\{([\w_\-\d\.]+)\}\}", replace_match, data_template)
		elif isinstance(data_template, dict):
			return {k: self._resolve_references(v, context) for k, v in data_template.items()}
		elif isinstance(data_template, list):
			return [self._resolve_references(item, context) for item in data_template]
		return data_template


# (è¯·å°†æ­¤ä¿®æ”¹åçš„ RetrievalExecutor æ›¿æ¢æ‰ä¹‹å‰ä»£ç ä¸­çš„ç‰ˆæœ¬)

# --- Executors (éƒ¨åˆ†ä¿®æ”¹) ---
# class ExecutorError(Exception): pass # å‡è®¾å·²å®šä¹‰
# class ExecutorBase(ABC): ... # å‡è®¾å·²å®šä¹‰, åŒ…å« _resolve_references
# from .knowledge_base import ChromaKnowledgeBase # ç¡®ä¿æ­£ç¡®å¯¼å…¥

class RetrievalExecutor(ExecutorBase):
	"""ä½¿ç”¨ ChromaKnowledgeBase è¿›è¡Œæ£€ç´¢çš„æ‰§è¡Œå™¨"""

	def __init__(self, kb: ChromaKnowledgeBase):  # æ”¹ä¸ºæ¥æ”¶ChromaKnowledgeBase
		super().__init__()  # RetrievalExecutor é€šå¸¸ä¸éœ€è¦ LLM
		self.kb = kb

	async def execute(self, task: Task, context: ContextManager) -> List[str]:  # è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨
		logic_input = task.logic_input
		query_to_retrieve = logic_input.get("query")
		filter_param_from_planner = logic_input.get("filter")  # å¯èƒ½ä¸º None, {}, æˆ–æœ‰æ•ˆè¿‡æ»¤å™¨

		if not query_to_retrieve or not isinstance(query_to_retrieve, str):
			raise ExecutorError("RetrievalExecutor: 'query' (string) is required in logic_input.")

		resolved_query = self._resolve_references(query_to_retrieve, context)

		# ã€é”™è¯¯ä¿®æ­£æ ¸å¿ƒä»£ç ã€‘
		# ç¡®ä¿ä¼ é€’ç»™ ChromaDB çš„è¿‡æ»¤å™¨æ˜¯æœ‰æ•ˆçš„ï¼Œæˆ–è€…ä¸º None
		actual_filter_for_chroma: Optional[Dict[str, Any]] = None
		if filter_param_from_planner and isinstance(filter_param_from_planner, dict):
			# åªæœ‰å½“ filter_param_from_planner æ˜¯ä¸€ä¸ªéç©ºå­—å…¸æ—¶ï¼Œæ‰å°†å…¶ç”¨ä½œè¿‡æ»¤å™¨
			if filter_param_from_planner:  # æ£€æŸ¥å­—å…¸æ˜¯å¦ä¸ºç©º
				actual_filter_for_chroma = filter_param_from_planner
			else:  # å¦‚æœ filter_param_from_planner æ˜¯ä¸€ä¸ªç©ºå­—å…¸ {}
				print(
					f"  [RetrievalExecutor] è­¦å‘Š: Planner ä¸ºä»»åŠ¡ '{task.id}' çš„ 'filter' å‚æ•°æä¾›äº†ä¸€ä¸ªç©ºå­—å…¸ã€‚å°†è§†ä½œæ— è¿‡æ»¤å™¨å¤„ç†ã€‚")
				actual_filter_for_chroma = None
		elif filter_param_from_planner is not None:
			# å¦‚æœ filter_param ä¸æ˜¯å­—å…¸ä¹Ÿä¸æ˜¯ None (ä¾‹å¦‚ï¼Œå¯èƒ½æ˜¯é”™è¯¯ç±»å‹çš„å­—ç¬¦ä¸²ç­‰)ï¼Œåˆ™è­¦å‘Šå¹¶è§†ä¸ºæ— è¿‡æ»¤å™¨
			print(
				f"  [RetrievalExecutor] è­¦å‘Š: Planner ä¸ºä»»åŠ¡ '{task.id}' çš„ 'filter' å‚æ•°æä¾›äº†æ— æ•ˆç±»å‹: {type(filter_param_from_planner)}ã€‚å°†è§†ä½œæ— è¿‡æ»¤å™¨å¤„ç†ã€‚")
			actual_filter_for_chroma = None
		# å¦‚æœ filter_param_from_planner æœ¬èº«å°±æ˜¯ Noneï¼Œåˆ™ actual_filter_for_chroma ä¿æŒä¸º None

		current_thought = task.thought or ""
		task.thought = f"{current_thought}çŸ¥è¯†åº“æ£€ç´¢æŸ¥è¯¢: '{resolved_query}', åº”ç”¨çš„è¿‡æ»¤å™¨: {actual_filter_for_chroma}.".strip()

		# ChromaKnowledgeBase.retrieve è¿”å› List[Dict[str, Any]]
		# æ¯ä¸ªå­—å…¸: {"id": ..., "content": ..., "metadata": ..., "score": ...}
		retrieved_docs_with_meta = self.kb.retrieve(
			resolved_query,
			top_k=3,
			filter_dict=actual_filter_for_chroma  # ä½¿ç”¨ä¿®æ­£åçš„è¿‡æ»¤å™¨
		)

		# æå– page_content åˆ—è¡¨ä¾›ä¸‹æ¸¸ä½¿ç”¨
		retrieved_contents = [doc["content"] for doc in retrieved_docs_with_meta if
							  doc and "content" in doc]  # å¢åŠ å¯¹docæ˜¯å¦ä¸ºNoneçš„æ£€æŸ¥

		if not retrieved_contents:
			task.thought += "\næœªæ£€ç´¢åˆ°ä»»ä½•åŒ¹é…æ–‡æ¡£ã€‚"
			return ["æŠ±æ­‰ï¼Œæœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸æ‚¨æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚"]  # ä¿æŒè¿”å›åˆ—è¡¨å½¢å¼

		task.thought += f"\næ£€ç´¢åˆ° {len(retrieved_contents)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚"
		return retrieved_contents

	def get_schema(self) -> Dict[str, Any]:  # (ä¸ä¹‹å‰ç›¸åŒ)
		return {
			"name": "RetrievalExecutor",
			"description": "ä»é…ç½®çš„å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚å¯ä»¥æŒ‡å®šå…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚",
			"logic_input_schema": {
				"query": "string (è¦åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢çš„æŸ¥è¯¢è¯­å¥ï¼Œå¯å¼•ç”¨å…ˆå‰æ­¥éª¤ {{task_id.result}})",
				"filter": "dict (å¯é€‰, ç”¨äºChromaDBçš„å…ƒæ•°æ®è¿‡æ»¤å™¨, e.g., {\"year\": 2020})"
			}
		}

class DeduceExecutor(ExecutorBase):
	def __init__(self, llm_client: OpenAIChatLLM, prompt: DeducePrompt):
		super().__init__(llm_client)
		self.prompt_template = prompt

	async def execute(self, task: Task, context: ContextManager) -> str:
		logic_input = task.logic_input
		reasoning_goal = logic_input.get("reasoning_goal")
		raw_context_data = logic_input.get("context_data")

		if not reasoning_goal or not isinstance(reasoning_goal, str) or \
				raw_context_data is None:  # context_data can be various types after resolution
			raise ExecutorError("DeduceExecutor: 'reasoning_goal' (string) and 'context_data' are required.")

		resolved_context_data = self._resolve_references(raw_context_data, context)

		context_data_str = ""
		if isinstance(resolved_context_data, list):
			context_data_str = "\n".join([f"- {item}" for item in resolved_context_data])
		elif isinstance(resolved_context_data, dict):
			context_data_str = json.dumps(resolved_context_data, ensure_ascii=False, indent=2)
		else:
			context_data_str = str(resolved_context_data)

		prompt_str = self.prompt_template.format(reasoning_goal=reasoning_goal, context_data=context_data_str)
		task.thought = f"æ¼”ç»ç›®æ ‡: {reasoning_goal}. ä½¿ç”¨çš„ä¸Šä¸‹æ–‡ (æ‘˜è¦): {context_data_str[:200]}..."

		response = await self.llm_client.generate(prompt_str, system_prompt_str=DeducePrompt.SYSTEM_PROMPT)
		return response

	def get_schema(self) -> Dict[str, Any]:
		return {
			"name": "DeduceExecutor",
			"description": "åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ¨ç†ã€æ€»ç»“ã€åˆ¤æ–­æˆ–æŠ½å–ã€‚ä¸Šä¸‹æ–‡å¯å¼•ç”¨å…ˆå‰æ­¥éª¤çš„ç»“æœã€‚",
			"logic_input_schema": {
				"reasoning_goal": "string (æœ¬æ¬¡æ¨ç†çš„å…·ä½“ç›®æ ‡æˆ–å­é—®é¢˜)",
				"context_data": "any (è¿›è¡Œæ¨ç†æ‰€å¿…éœ€çš„æ‰€æœ‰èƒŒæ™¯çŸ¥è¯†æˆ–æ•°æ®ï¼Œå¯åŒ…å« {{task_id.result}})",
				"operation_type": "string (å¯é€‰ï¼Œå¦‚ summarize, extract_info, judge)"
			}
		}


class CodeExecutor(ExecutorBase):  # (ä¸ä¸Šä¸€è½®å›å¤ä¸­çš„ç‰ˆæœ¬ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: CodeExecutionPrompt):
		super().__init__(llm_client)
		self.prompt_template = prompt

	async def execute(self, task: Task, context: ContextManager) -> str:
		logic_input = task.logic_input
		code_generation_prompt_str = logic_input.get("code_generation_prompt")
		relevant_data = logic_input.get("relevant_data", "")
		if not code_generation_prompt_str or not isinstance(code_generation_prompt_str, str):
			raise ExecutorError("CodeExecutor: 'code_generation_prompt' (string) is required.")
		resolved_code_gen_prompt = self._resolve_references(code_generation_prompt_str, context)
		resolved_relevant_data = self._resolve_references(relevant_data, context)
		relevant_data_for_prompt = ""
		if isinstance(resolved_relevant_data, (list, dict)):
			relevant_data_for_prompt = json.dumps(resolved_relevant_data, ensure_ascii=False, indent=2)
		elif resolved_relevant_data:
			relevant_data_for_prompt = str(resolved_relevant_data)
		llm_prompt_str = self.prompt_template.format(
			code_generation_prompt=resolved_code_gen_prompt,
			relevant_data=relevant_data_for_prompt
		)
		task.thought = f"ä»£ç ç”Ÿæˆç›®æ ‡: {resolved_code_gen_prompt}. "
		generated_code_with_markers = await self.llm_client.generate(llm_prompt_str,
																	 system_prompt_str=CodeExecutionPrompt.SYSTEM_PROMPT)
		generated_code = generated_code_with_markers.strip()
		if generated_code.startswith("```python"): generated_code = generated_code[9:]
		if generated_code.startswith("```"): generated_code = generated_code[3:]
		if generated_code.endswith("```"): generated_code = generated_code[:-3]
		generated_code = generated_code.strip()
		if not generated_code:
			task.thought += "\nLLMæœªèƒ½ç”Ÿæˆä»»ä½•å¯æ‰§è¡Œä»£ç ã€‚"
			raise ExecutorError("CodeExecutor: LLM did not generate any Python code.")
		task.thought += f"\nç”Ÿæˆçš„ä»£ç :\n---\n{generated_code}\n---"
		try:
			with open("temp_code_to_execute.py", "w", encoding="utf-8") as f:
				f.write(generated_code)
			process = await asyncio.to_thread(
				subprocess.run, [sys.executable, "temp_code_to_execute.py"],
				capture_output=True, text=True, timeout=10, check=False
			)
			if process.returncode != 0:
				error_output = process.stderr or "Unknown execution error"
				task.thought += f"\nä»£ç æ‰§è¡Œè¿”å›éé›¶é€€å‡ºç  {process.returncode}. Stderr:\n{error_output}"
				raise ExecutorError(
					f"Generated code execution failed with exit code {process.returncode}:\n{error_output}")
			output_value = process.stdout.strip()
			task.thought += f"\nä»£ç æ‰§è¡Œæ ‡å‡†è¾“å‡º: {output_value}"
			return output_value
		except subprocess.TimeoutExpired:
			task.thought += "\nä»£ç æ‰§è¡Œè¶…æ—¶ã€‚"
			raise ExecutorError("Generated code execution timed out.")
		except Exception as e:
			task.thought += f"\næ‰§è¡Œç”Ÿæˆçš„ä»£ç æ—¶å‘ç”Ÿæœ¬åœ°é”™è¯¯: {str(e)}"
			raise ExecutorError(f"Error during local setup/execution of generated code: {e}")
		finally:
			if os.path.exists("temp_code_to_execute.py"): os.remove("temp_code_to_execute.py")

	def get_schema(self) -> Dict[str, Any]:
		return {
			"name": "CodeExecutor",
			"description": "ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç æ¥è§£å†³è®¡ç®—æˆ–æ•°æ®å¤„ç†é—®é¢˜ã€‚ä»£ç åº”é€šè¿‡print()è¾“å‡ºç»“æœã€‚è¾“å…¥å¯ä»¥å¼•ç”¨å…ˆå‰æ­¥éª¤çš„ç»“æœã€‚",
			"logic_input_schema": {
				"code_generation_prompt": "string (ç”Ÿæˆä»£ç çš„ç›®æ ‡å’ŒæŒ‡ä»¤ï¼Œå¯åŒ…å« {{task_id.result}})",
				"relevant_data": "any (å¯é€‰, ä»£ç æ‰§è¡Œå¯èƒ½éœ€è¦çš„æ•°æ®ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€å­—å…¸æˆ–å¼•ç”¨ {{task_id.result}})"
			}
		}


# --- Planner (from previous response) ---
class Planner:
	def __init__(self, llm_client: OpenAIChatLLM, prompt: PlannerPrompt):
		self.llm_client = llm_client
		self.prompt_template = prompt

	async def create_plan(self, user_query: str, context: ContextManager, available_executors: List[Dict]) -> List[
		Dict]:
		executors_description_parts = []
		for ex_schema in available_executors:
			input_schema_str = ex_schema.get('logic_input_schema', 'æœªå®šä¹‰')
			if isinstance(input_schema_str, dict):
				input_schema_str = json.dumps(input_schema_str, ensure_ascii=False)
			executors_description_parts.append(
				f"  - åç§°: \"{ex_schema['name']}\"\n"
				f"    æè¿°: \"{ex_schema['description']}\"\n"
				f"    è¾“å…¥å‚æ•°æ¨¡å¼ (logic_input_schema): {input_schema_str}"
			)
		executors_description = "\n".join(executors_description_parts)
		task_history_str = context.get_task_history_for_prompt()
		user_prompt_str = self.prompt_template.format(
			user_query=user_query,
			available_executors_description=executors_description,
			task_history=task_history_str
		)
		print(f"  [Planner] Sending planning prompt to LLM (length: {len(user_prompt_str)} chars)...")
		planned_task_data_list = await self.llm_client.generate_structured_json(
			user_prompt_str, system_prompt_str=PlannerPrompt.SYSTEM_PROMPT
		)
		if not isinstance(planned_task_data_list, list):
			print(f"  [Planner Error] LLM did not return a list for the plan. Response: {planned_task_data_list}")
			if isinstance(planned_task_data_list, dict):
				for key in ["tasks", "plan", "steps", "actions"]:  # å¸¸è§åŒ…è£…é”®
					if isinstance(planned_task_data_list.get(key), list):
						planned_task_data_list = planned_task_data_list[key]
						print(f"  [Planner] Extracted task list from key '{key}'.")
						break
				else:
					raise ValueError("Planner LLM response, even after checking common keys, is not a list.")
			else:
				raise ValueError("Planner LLM response is not a list or a dict containing a list.")
		validated_tasks_data = []
		for i, task_data in enumerate(planned_task_data_list):
			if not isinstance(task_data, dict):
				print(f"  [Planner Warning] Plan item {i} is not a dictionary: {task_data}. Skipping.")
				continue
			if not all(k in task_data for k in ["executor_name", "task_description", "logic_input"]):
				print(
					f"  [Planner Warning] Plan item {i} missing required fields (executor_name, task_description, logic_input): {task_data}. Skipping.")
				continue
			if "id" not in task_data or not isinstance(task_data["id"], str) or not task_data["id"].strip():
				auto_id = f"{context.user_query[:10].replace(' ', '_').replace('?', '')}_plan_step_{i}"  # ä½¿ç”¨ context.tasks è®¡æ•°ä¸å‡†ç¡®ï¼Œå› ä¸ºcontextæ­¤æ—¶å¯èƒ½ä¸ºç©º
				print(
					f"  [Planner Warning] Plan item {i} missing or invalid ID. Assigning '{auto_id}'. Original: {task_data.get('id')}")
				task_data["id"] = auto_id
			validated_tasks_data.append(task_data)
		return validated_tasks_data


# --- AnswerGenerator (from previous response) ---
class AnswerGenerator:
	def __init__(self, llm_client: OpenAIChatLLM, prompt: GenerationPrompt):
		self.llm_client = llm_client
		self.prompt_template = prompt

	async def generate_final_answer(self, user_query: str, context: ContextManager) -> str:
		reasoning_summary = context.get_summary_for_generator()
		user_prompt_str = self.prompt_template.format(
			user_query=user_query,
			reasoning_steps_summary=reasoning_summary
		)
		print(f"  [Generator] Sending generation prompt to LLM (summary length: {len(reasoning_summary)} chars)...")
		final_answer = await self.llm_client.generate(user_prompt_str, system_prompt_str=GenerationPrompt.SYSTEM_PROMPT)
		return final_answer


# --- Pipeline (from previous response, minor logging/error improvements) ---
class IterativePipeline:  # (åå­—å¯ä»¥ä¿ç•™ï¼Œä½†è¡Œä¸ºæ˜¯é™æ€DAGæ‰§è¡Œ)
	def __init__(self,
				 planner: Planner,
				 executors: Dict[str, ExecutorBase],
				 generator: AnswerGenerator,
				 max_iterations: int = 1):
		self.planner = planner
		self.executors = executors
		self.generator = generator
		self.max_iterations = max_iterations

	async def _execute_task_with_dependencies(self, task_id: str, context: ContextManager, executed_tasks_cache: set):
		if task_id in executed_tasks_cache:
			task_obj = context.get_task(task_id)
			# if task_obj and task_obj.status != "pending": # å¦‚æœä¸æ˜¯pendingï¼Œè¯´æ˜å·²ç»è¢«å¤„ç†æˆ–æ­£åœ¨å¤„ç†
			# print(f"  [Pipeline] Task {task_id} already processed or in queue (status: {task_obj.status if task_obj else 'N/A'}). Skipping.")
			return

		task_to_run = context.get_task(task_id)
		if not task_to_run:
			print(f"  [Pipeline Error] Task {task_id} definition not found in context. Cannot execute.")
			return  # Should not happen if plan is consistent

		# å…ˆå°†ä»»åŠ¡IDåŠ å…¥ç¼“å­˜ï¼Œè¡¨ç¤ºå¼€å§‹å¤„ç†ï¼ˆåŒ…æ‹¬ä¾èµ–æ£€æŸ¥ï¼‰
		executed_tasks_cache.add(task_id)

		# 1. è§£å†³ä¾èµ–
		print(f"  [Pipeline] Checking dependencies for task {task_id}: {task_to_run.dependencies}")
		for dep_id in task_to_run.dependencies:
			if dep_id not in executed_tasks_cache:  # åªæœ‰å½“ä¾èµ–é¡¹æœ¬èº«è¿˜æœªè¢«â€œå¤„ç†è¿‡â€æ—¶æ‰é€’å½’
				dep_task_obj = context.get_task(dep_id)
				if not dep_task_obj:  # ä¾èµ–çš„ä»»åŠ¡IDåœ¨è®¡åˆ’ä¸­å°±ä¸å­˜åœ¨
					error_msg = f"Task {task_to_run.id} has an undefined dependency ID: {dep_id} not found in plan. Marking task as failed."
					context.update_task_status(task_to_run.id, "failed", result=error_msg, thought=error_msg)
					print(f"  [Pipeline Error] {error_msg}")
					return
				await self._execute_task_with_dependencies(dep_id, context, executed_tasks_cache)

		# 2. æ£€æŸ¥ä¾èµ–æ˜¯å¦éƒ½æˆåŠŸå®Œæˆ
		dependencies_met = True
		if task_to_run.dependencies:  # Only check if there are dependencies
			for dep_id in task_to_run.dependencies:
				dep_task = context.get_task(dep_id)
				if not dep_task or dep_task.status != "completed":
					dependencies_met = False
					current_thought = task_to_run.thought or ""
					task_thought = f"{current_thought}ä¾èµ–ä»»åŠ¡ {dep_id} æœªæˆåŠŸå®Œæˆ (çŠ¶æ€: {dep_task.status if dep_task else 'ä¸å­˜åœ¨æˆ–æœªå®šä¹‰'})ï¼Œè·³è¿‡æ‰§è¡Œæœ¬ä»»åŠ¡ã€‚"
					context.update_task_status(task_to_run.id, "skipped", thought=task_thought)
					print(
						f"  [Pipeline] Task {task_to_run.id} ('{task_to_run.task_description}') skipped, dependency {dep_id} not met (status: {dep_task.status if dep_task else 'N/A'}).")
					break
			if not dependencies_met:
				return
		else:
			print(f"  [Pipeline] Task {task_id} has no dependencies.")

		# 3. æ‰§è¡Œå½“å‰ä»»åŠ¡ (ç¡®ä¿å®ƒçœŸçš„æ˜¯pendingçŠ¶æ€)
		if task_to_run.status != "pending":
			print(
				f"  [Pipeline] Task {task_id} is not pending (status: {task_to_run.status}). Skipping execution phase for this task.")
			return

		executor = self.executors.get(task_to_run.executor_name)
		if not executor:
			error_msg = f"Executor '{task_to_run.executor_name}' not found for task '{task_to_run.task_description}'."
			context.update_task_status(task_to_run.id, "failed", result=error_msg, thought=error_msg)
			print(f"  [Pipeline Error] {error_msg}")
			return

		print(
			f"\nâ–¶ï¸ Executing Task: {task_to_run.id} - \"{task_to_run.task_description}\" (using {task_to_run.executor_name})")
		context.update_task_status(task_to_run.id, "running", thought=f"å¼€å§‹æ‰§è¡Œ: {task_to_run.task_description}")
		try:
			result = await executor.execute(task_to_run, context)
			context.update_task_status(task_to_run.id, "completed", result=result, thought=task_to_run.thought)
			print(
				f"âœ… Task {task_to_run.id} Result : {str(result)}")
		except ExecutorError as e:
			error_str = f"æ‰§è¡Œå™¨é”™è¯¯ for task {task_to_run.id}: {e}"
			final_thought = (task_to_run.thought or "") + f"\næ‰§è¡Œå™¨é”™è¯¯: {e}"
			context.update_task_status(task_to_run.id, "failed", result=error_str, thought=final_thought)
			print(f"ğŸ›‘ {error_str}")
		except Exception as e:
			error_str = f"æ‰§è¡Œä»»åŠ¡æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ {task_to_run.id}: {e}"
			import traceback
			tb_str = traceback.format_exc()
			print(f"ğŸ›‘ {error_str}\n{tb_str}")
			final_thought = (task_to_run.thought or "") + f"\næ„å¤–ç³»ç»Ÿé”™è¯¯: {e}"
			context.update_task_status(task_to_run.id, "failed", result=error_str, thought=final_thought)

	async def run(self, user_query: str) -> str:
		print(f"\nğŸš€ Pipeline starting for query: \"{user_query}\"")
		context = ContextManager(user_query)
		available_executors_schemas = [ex.get_schema() for ex in self.executors.values()]

		final_answer = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†ä¸€äº›éº»çƒ¦ã€‚"  # Default error

		# 1. è§„åˆ’
		print(f"\nğŸ“ Planning phase...")
		try:
			planned_tasks_data_list = await self.planner.create_plan(user_query, context, available_executors_schemas)
			if not planned_tasks_data_list:
				print("  [Pipeline] Planner did not generate any tasks. Cannot proceed.")
				return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä¸ºæ‚¨çš„é—®é¢˜åˆ¶å®šæœ‰æ•ˆçš„æ‰§è¡Œè®¡åˆ’ã€‚"

			# æ¸…ç©ºæ—§ä»»åŠ¡ï¼Œå¹¶å°†æ–°è®¡åˆ’åŠ å…¥ä¸Šä¸‹æ–‡
			context.tasks.clear()
			context.execution_order.clear()
			print("\nğŸ“‹ Plan Received from LLM (before adding to context):")
			for i, task_data_item in enumerate(planned_tasks_data_list):
				print(f"  Raw Task Data {i}: {task_data_item}")
				# ä½¿ç”¨ context.user_query ç”Ÿæˆä¸€ä¸ªåŸºç¡€IDå‰ç¼€ï¼Œç¡®ä¿å…¶å¯¹æ–‡ä»¶åå‹å¥½
				base_id_prefix_for_task = re.sub(r'[^\w\s-]', '', context.user_query[:15]).strip().replace(' ',
																										   '_') or "query"
				context.add_task_from_planner(task_data_item, base_id_prefix_for_task, i)


		except ValueError as ve:
			print(f"  [Pipeline Error] Planning phase failed due to invalid LLM response: {ve}")
			return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨è§„åˆ’å¦‚ä½•è§£å†³æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†é”™è¯¯ï¼ˆLLMè¿”å›æ ¼å¼é—®é¢˜ï¼‰ï¼š{ve}"
		except RuntimeError as rte:
			print(f"  [Pipeline Error] Planning phase LLM call failed: {rte}")
			return f"æŠ±æ­‰ï¼Œè¿æ¥åˆ°è¯­è¨€æ¨¡å‹è¿›è¡Œè§„åˆ’æ—¶å¤±è´¥ï¼š{rte}"
		except Exception as e:
			print(f"  [Pipeline Error] Unexpected error during planning: {e}")
			import traceback;
			traceback.print_exc()
			return f"æŠ±æ­‰ï¼Œè§„åˆ’é˜¶æ®µå‡ºç°æ„å¤–é”™è¯¯ï¼š{e}"

		print("\nğŸ“‹ Tasks in Context after planning:")
		for task_id_in_order in context.execution_order:
			task_obj = context.get_task(task_id_in_order)
			if task_obj:
				print(
					f"  - ID: {task_obj.id}, Executor: {task_obj.executor_name}, Desc: \"{task_obj.task_description}\", Deps: {task_obj.dependencies}")
			else:
				print(f"  - Error: Task ID {task_id_in_order} found in execution_order but not in tasks dict.")

		# 2. æ‰§è¡Œè®¡åˆ’ (å¤„ç†DAG)
		print("\nâš™ï¸ Execution phase...")
		executed_tasks_this_run = set()

		# æŒ‰ä»»åŠ¡åœ¨ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºï¼ˆå³LLMè§„åˆ’çš„é¡ºåºï¼‰å°è¯•æ‰§è¡Œ
		# _execute_task_with_dependencies ä¼šé€’å½’å¤„ç†å®é™…ä¾èµ–é¡ºåº
		for task_id_to_process in context.execution_order:
			# åªæœ‰å½“ä»»åŠ¡æ˜¯pendingçš„æ—¶å€™æ‰å¯åŠ¨å®ƒï¼Œå› ä¸ºä¾èµ–æ‰§è¡Œå¯èƒ½å·²ç»å¤„ç†äº†å®ƒ
			task_obj_to_process = context.get_task(task_id_to_process)
			if task_obj_to_process and task_obj_to_process.status == "pending":
				await self._execute_task_with_dependencies(task_id_to_process, context, executed_tasks_this_run)

		failed_or_skipped_tasks = [t for t in context.tasks.values() if t.status in ["failed", "skipped"]]
		if failed_or_skipped_tasks:
			print("\nâš ï¸ Some tasks failed or were skipped during execution:")
			for ft in failed_or_skipped_tasks:
				print(f"  - Task ID: {ft.id}, Status: {ft.status}, Reason/Result: {str(ft.result)[:200]}")
		else:
			print("  All planned tasks appear to have completed successfully or were appropriately handled.")

		# 3. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
		print("\nğŸ’¬ Generation phase...")
		try:
			final_answer = await self.generator.generate_final_answer(user_query, context)
		except RuntimeError as rte:
			print(f"  [Pipeline Error] Generation phase LLM call failed: {rte}")
			final_answer = f"æŠ±æ­‰ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶è¿æ¥è¯­è¨€æ¨¡å‹å¤±è´¥ï¼š{rte}"
		except Exception as e:
			print(f"  [Pipeline Error] Unexpected error during generation: {e}")
			import traceback;
			traceback.print_exc()
			final_answer = f"æŠ±æ­‰ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆé˜¶æ®µå‡ºç°æ„å¤–é”™è¯¯ï¼š{e}"

		print(f"\nğŸ’¡ Final Answer: {final_answer}")
		return final_answer


# --- ä¸»ç¨‹åºå…¥å£ ---
async def run_main_logic_with_user_data():
	# 1. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
	api_key = 'sk-af4423da370c478abaf68b056f547c6e'
	base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
	model_name = os.getenv("LLM_MODEL_NAME", "qwen-plus")

	if not api_key:
		print("é”™è¯¯ï¼šæœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·è®¾ç½®åå†è¿è¡Œã€‚")
		return

	llm_client = OpenAIChatLLM(model_name=model_name, api_key=api_key, base_url=base_url)

	# 2. å‡†å¤‡ç”¨æˆ·æ•°æ®å¹¶åˆå§‹åŒ–ChromaçŸ¥è¯†åº“
	# ä½¿ç”¨æ‚¨æä¾›çš„DashScope Embedding Key
	# è¯·ç¡®ä¿æ‚¨å·²å®‰è£… langchain-community å’Œ tongyiembedding
	# pip install langchain-community langchain-chroma tongyiembedding
	try:
		# ä½¿ç”¨æ‚¨ä»£ç ä¸­æä¾›çš„ QwenEmbeddingFunction
		# è¿™é‡Œå‡è®¾æ‚¨çš„ DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ä¹Ÿé€‚ç”¨äº QwenEmbeddingFunction
		# å¦‚æœQwenEmbeddingFunctionéœ€è¦ä¸åŒçš„keyï¼Œè¯·ç›¸åº”è°ƒæ•´

		from tongyiembedding import QwenEmbeddingFunction

		embedding_function = QwenEmbeddingFunction(api_key='sk-af4423da370c478abaf68b056f547c6e')

		print(f"  [Embedding] ä½¿ç”¨ QwenEmbeddingFunction åˆå§‹åŒ– embedding_functionã€‚")
	except Exception as e:
		print(f"  [Embedding Error] åˆå§‹åŒ– QwenEmbeddingFunction å¤±è´¥: {e}")
		print("  è¯·ç¡®ä¿ 'tongyiembedding' åº“å·²å®‰è£…ä¸”API Keyæœ‰æ•ˆã€‚")
		return

	# å°†æ‚¨æä¾›çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºLangchain Documentå¯¹è±¡
	initial_user_docs_as_dicts = [
		{"page_content": "ã€ä¸€æé»„èŠ±ã€‘æ€§çŠ¶:æœ¬å“é•¿30ï½100cm",
		 "metadata": {"id": "doc_yzyh", "source_name": "ä¸€æé»„èŠ±è¯´æ˜"}},
		{"page_content": "ã€æ­£æŸ´èƒ¡é¥®é¢—ç²’ã€‘æ£€æŸ¥: åº”ç¬¦åˆé¢—ç²’å‰‚é¡¹ä¸‹æœ‰å…³çš„å„é¡¹è§„å®šï¼ˆé€šåˆ™0104)ã€‚",
		 "metadata": {"id": "doc_zchyk", "source_name": "æ­£æŸ´èƒ¡é¥®é¢—ç²’è¯´æ˜"}},
		{"page_content": "0104é¢—ç²’å‰‚é™¤å¦æœ‰è§„å®šå¤–ï¼Œé¢—ç²’å‰‚åº”è¿›è¡Œä»¥ä¸‹ç›¸åº”æ£€æŸ¥ã€‚ã€ç²’åº¦ã€‘",
		 "metadata": {"id": "doc_tongze0104_part1", "source_name": "è¯å…¸é€šåˆ™0104"}},
		{"page_content": "é€šåˆ™0104ç»§ç»­ï¼šã€å¹²ç‡¥å¤±é‡ã€‘...",
		 "metadata": {"id": "doc_tongze0104_part2", "source_name": "è¯å…¸é€šåˆ™0104"}}
	]
	initial_langchain_docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in
							  initial_user_docs_as_dicts]
	initial_langchain_docs = [
		Document(
			page_content="ã€ä¸€æé»„èŠ±ã€‘æ€§çŠ¶:æœ¬å“é•¿30ï½100cmã€‚æ ¹èŒçŸ­ç²—ï¼Œç°‡ç”Ÿæ·¡é»„è‰²ç»†æ ¹ã€‚èŒåœ†æŸ±å½¢ï¼Œç›´å¾„0.2ï½0.5cmï¼›è¡¨é¢é»„ç»¿è‰²ã€ç°æ£•è‰²æˆ–æš—ç´«çº¢è‰²ï¼Œæœ‰æ£±çº¿ï¼Œä¸Šéƒ¨è¢«æ¯›ï¼›è´¨è„†ï¼Œæ˜“æŠ˜æ–­ï¼Œæ–­é¢çº¤ç»´æ€§ï¼Œæœ‰é«“ã€‚å•å¶äº’ç”Ÿï¼Œå¤šçš±ç¼©ã€ç ´ç¢ï¼Œå®Œæ•´å¶ç‰‡å±•å¹³åå‘ˆåµå½¢æˆ–æŠ«é’ˆå½¢ï¼Œé•¿1ï½9cmï¼Œå®½0.3ï½1.5cmï¼›å…ˆç«¯ç¨å°–æˆ–é’ï¼Œå…¨ç¼˜æˆ–æœ‰ä¸è§„åˆ™çš„ç–é”¯é½¿ï¼ŒåŸºéƒ¨ä¸‹å»¶æˆæŸ„ã€‚å¤´çŠ¶èŠ±åºç›´å¾„çº¦0.7cmï¼Œæ’æˆæ€»çŠ¶ï¼Œå¶æœ‰é»„è‰²èˆŒçŠ¶èŠ±æ®‹ç•™ï¼Œå¤šçš±ç¼©æ‰­æ›²ï¼Œè‹ç‰‡3å±‚ï¼ŒåµçŠ¶æŠ«é’ˆå½¢ã€‚ç˜¦æœç»†å°ï¼Œå† æ¯›é»„ç™½è‰²ã€‚æ°”å¾®é¦™ï¼Œå‘³å¾®è‹¦è¾›ã€‚",
			metadata={"year": 1993, "rating": 7.7, "genre": "science fiction", "id": "movie_1"},
			# Added an 'id' for clarity
		),
		# åŠ å…¥ä¹‹å‰çŸ¥è¯†åº“çš„æ–‡æ¡£ï¼Œè½¬æ¢ä¸ºDocumentæ ¼å¼
		Document(page_content="ã€æ­£æŸ´èƒ¡é¥®é¢—ç²’ã€‘æ£€æŸ¥: åº”ç¬¦åˆé¢—ç²’å‰‚é¡¹ä¸‹æœ‰å…³çš„å„é¡¹è§„å®šï¼ˆé€šåˆ™0104)ã€‚",
				 metadata={"topic": "programming", "language": "Python", "id": "tech_py"}),
		Document(page_content="""0104é¢—ç²’å‰‚é™¤å¦æœ‰è§„å®šå¤–ï¼Œé¢—ç²’å‰‚åº”è¿›è¡Œä»¥ä¸‹ç›¸åº”æ£€æŸ¥ã€‚
								ã€ç²’åº¦ã€‘é™¤å¦æœ‰è§„å®šå¤–ï¼Œç…§ç²’åº¦å’Œç²’åº¦åˆ†å¸ƒæµ‹å®šæ³•ï¼ˆé€šåˆ™0982ç¬¬äºŒæ³• åŒç­›åˆ†æ³•ï¼‰æµ‹å®šï¼Œä¸èƒ½é€šè¿‡ä¸€å·ç­›ä¸èƒ½é€šè¿‡äº”å·ç­›çš„æ€»å’Œä¸å¾—è¶…è¿‡15ï¼…ã€‚
								ã€æ°´åˆ†ã€‘ä¸­è¯é¢—ç²’å‰‚ç…§æ°´åˆ†æµ‹å®šæ³•ï¼ˆé€šåˆ™0832ï¼‰æµ‹å®šï¼Œé™¤å¦æœ‰è§„å®šå¤–ï¼Œæ°´åˆ†ä¸å¾—è¶…è¿‡8.0ï¼…ã€‚
								ã€å¹²ç‡¥å¤±é‡ã€‘é™¤å¦æœ‰è§„å®šå¤–ï¼ŒåŒ–å­¦è¯å“å’Œç”Ÿç‰©åˆ¶å“é¢—ç²’å‰‚ç…§å¹²ç‡¥å¤±é‡æµ‹å®šæ³•ï¼ˆé€šåˆ™0831ï¼‰æµ‹å®šï¼Œäº105â„ƒå¹²ç‡¥ï¼ˆå«ç³–é¢—ç²’åº”åœ¨80â„ƒå‡å‹å¹²ç‡¥ï¼‰è‡³æ’é‡ï¼Œå‡å¤±é‡é‡ä¸å¾—è¶…è¿‡2.0%ã€‚
								ã€æº¶åŒ–æ€§ã€‘é™¤å¦æœ‰è§„å®šå¤–ï¼Œé¢—ç²’å‰‚ç…§ä¸‹è¿°æ–¹æ³•æ£€æŸ¥ï¼Œæº¶åŒ–æ€§åº”ç¬¦åˆè§„å®šã€‚å«ä¸­è¯åŸç²‰çš„é¢—ç²’å‰‚ä¸è¿›è¡Œæº¶åŒ–æ€§æ£€æŸ¥ã€‚
								å¯æº¶é¢—ç²’æ£€æŸ¥æ³• å–ä¾›è¯•å“10gï¼ˆä¸­è¯å•å‰‚é‡åŒ…è£…å–1è¢‹ï¼‰ï¼ŒåŠ çƒ­æ°´200mlï¼Œæ…æ‹Œ5åˆ†é’Ÿï¼Œç«‹å³è§‚å¯Ÿï¼Œå¯æº¶é¢—ç²’åº”å…¨éƒ¨æº¶åŒ–æˆ–è½»å¾®æµ‘æµŠã€‚
								æ³¡è…¾é¢—ç²’æ£€æŸ¥æ³• å–ä¾›è¯•å“3è¢‹ï¼Œå°†å†…å®¹ç‰©åˆ†åˆ«è½¬ç§»è‡³ç››æœ‰200mlæ°´çš„çƒ§æ¯ä¸­ï¼Œæ°´æ¸©ä¸º15ï½25â„ƒï¼Œåº”è¿…é€Ÿäº§ç”Ÿæ°”ä½“è€Œå‘ˆæ³¡è…¾çŠ¶ï¼Œ5åˆ†é’Ÿå†…é¢—ç²’å‡åº”å®Œå…¨åˆ†æ•£æˆ–æº¶è§£åœ¨æ°´ä¸­ã€‚
								é¢—ç²’å‰‚æŒ‰ä¸Šè¿°æ–¹æ³•æ£€æŸ¥ï¼Œå‡ä¸å¾—æœ‰å¼‚ç‰©ï¼Œä¸­è¯é¢—ç²’è¿˜ä¸å¾—æœ‰ç„¦å±‘ã€‚
								æ··æ‚¬é¢—ç²’ä»¥åŠå·²è§„å®šæ£€æŸ¥æº¶å‡ºåº¦æˆ–é‡Šæ”¾åº¦çš„é¢—ç²’å‰‚å¯ä¸è¿›è¡Œæº¶åŒ–æ€§æ£€æŸ¥ã€‚
								ã€è£…é‡å·®å¼‚ã€‘å•å‰‚é‡åŒ…è£…çš„é¢—ç²’å‰‚æŒ‰ä¸‹è¿°æ–¹æ³•æ£€æŸ¥ï¼Œåº”ç¬¦åˆè§„å®šã€‚
								æ£€æŸ¥æ³• å–ä¾›è¯•å“10è¢‹ï¼ˆç“¶ï¼‰ï¼Œé™¤å»åŒ…è£…ï¼Œåˆ†åˆ«ç²¾å¯†ç§°å®šæ¯è¢‹ï¼ˆç“¶ï¼‰å†…å®¹ç‰©çš„é‡é‡ï¼Œæ±‚å‡ºæ¯è¢‹ï¼ˆç“¶ï¼‰å†…å®¹ç‰©çš„è£…é‡ä¸å¹³å‡è£…é‡ã€‚æ¯è¢‹ï¼ˆç“¶ï¼‰è£…é‡ä¸å¹³å‡è£…é‡ç›¸æ¯”è¾ƒï¼»å‡¡æ— å«é‡æµ‹å®šçš„é¢—ç²’å‰‚æˆ–æœ‰æ ‡ç¤ºè£…é‡çš„é¢—ç²’å‰‚ï¼Œæ¯è¢‹ï¼ˆç“¶ï¼‰è£…é‡åº”ä¸æ ‡ç¤ºè£…é‡æ¯”è¾ƒï¼½ï¼Œè¶…å‡ºè£…é‡å·®å¼‚é™åº¦çš„é¢—ç²’å‰‚ä¸å¾—å¤šäº2è¢‹ï¼ˆç“¶ï¼‰ï¼Œå¹¶ä¸å¾—æœ‰1è¢‹ï¼ˆç“¶ï¼‰è¶…å‡ºè£…é‡å·®å¼‚é™åº¦1å€ã€‚
								<table border="1" ><tr>
								<td colspan="1" rowspan="1">å¹³å‡è£…é‡æˆ–æ ‡ç¤ºè£…é‡</td>
								<td colspan="1" rowspan="1">è£…é‡å·®å¼‚é™åº¦</td>
								</tr><tr>
								<td colspan="1" rowspan="1">1.0gåŠ1.0gä»¥ä¸‹</td>
								<td colspan="1" rowspan="1">Â±10%</td>
								</tr><tr>
								<td colspan="1" rowspan="1">1.0gä»¥ä¸Šè‡³1.5g </td>
								<td colspan="1" rowspan="1">Â±8%</td>
								</tr><tr>
								<td colspan="1" rowspan="1">1.5gä»¥ä¸Šè‡³6.0g </td>
								<td colspan="1" rowspan="1">Â±7%</td>
								</tr><tr>
								<td colspan="1" rowspan="1">6.0gä»¥ä¸Š</td>
								<td colspan="1" rowspan="1">Â±5%</td>
								</tr></table>
								å‡¡è§„å®šæ£€æŸ¥å«é‡å‡åŒ€åº¦çš„é¢—ç²’å‰‚ï¼Œä¸€èˆ¬ä¸å†è¿›è¡Œè£…é‡å·®å¼‚æ£€æŸ¥ã€‚
								ã€è£…é‡ã€‘å¤šå‰‚é‡åŒ…è£…çš„é¢—ç²’å‰‚ï¼Œç…§æœ€ä½è£…é‡æ£€æŸ¥æ³•ï¼ˆé€šåˆ™0942ï¼‰æ£€æŸ¥ï¼Œåº”ç¬¦åˆè§„å®šã€‚
								ã€å¾®ç”Ÿç‰©é™åº¦ã€‘ä»¥åŠ¨ç‰©ã€æ¤ç‰©ã€çŸ¿ç‰©è´¨æ¥æºçš„éå•ä½“æˆåˆ†åˆ¶æˆçš„é¢—ç²’å‰‚ï¼Œç”Ÿç‰©åˆ¶å“é¢—ç²’å‰‚ï¼Œç…§éæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ï¼šå¾®ç”Ÿç‰©è®¡æ•°æ³•ï¼ˆé€šåˆ™1105ï¼‰å’Œæ§åˆ¶èŒæ£€æŸ¥æ³•ï¼ˆé€šåˆ™1106ï¼‰åŠéæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ï¼ˆé€šåˆ™1107ï¼‰æ£€æŸ¥ï¼Œåº”ç¬¦åˆè§„å®šã€‚è§„å®šæ£€æŸ¥æ‚èŒçš„ç”Ÿç‰©åˆ¶å“é¢—ç²’å‰‚ï¼Œå¯ä¸è¿›è¡Œå¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ã€‚0104é¢—ç²’å‰‚ï¼ˆæ›´æ˜”æ´›éŸ¦ï¼‰""",
				 metadata={"topic": "AI", "type": "LLM", "id": "tech_llm"}),
		Document(page_content="""1107éæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡† 3ï¼éæ— èŒåŒ–å­¦è¯å“åˆ¶å‰‚ã€ç”Ÿç‰©åˆ¶å“åˆ¶å‰‚ã€ä¸å«è¯æåŸç²‰çš„ä¸­è¯åˆ¶å‰‚çš„å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†è§è¡¨1ã€‚
									è¡¨1 éæ— èŒåŒ–å­¦è¯å“åˆ¶å‰‚ã€ç”Ÿç‰©åˆ¶å“åˆ¶å‰‚ã€ä¸å«è¯æåŸç²‰çš„ä¸­è¯åˆ¶å‰‚çš„å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†
									<table border="1">

										<tr>

											<th>ç»™è¯é€”å¾„</th>

											<th>åˆ¶å‰‚ç±»å‹</th>

											<th>éœ€æ°§èŒæ€»æ•°ï¼ˆcfuï¼gã€cfuï¼mlæˆ–cfuï¼10cã¡ï¼‰</th>

											<th>éœ‰èŒå’Œé…µæ¯èŒæ€»æ•°ï¼ˆcfuï¼gã€cfuï¼mlæˆ–cfuï¼10cã¡ï¼‰</th>

											<th>æ§åˆ¶èŒ</th>

										</tr>

										<tr>

											<td rowspan="2">å£æœç»™è¯</td>

											<td>å›ºä½“åˆ¶å‰‚</td>

											<td>10Â³</td>

											<td>10Â²</td>

											<td>ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒï¼ˆ1gæˆ–1mlï¼‰ï¼›å«è„å™¨æå–ç‰©çš„åˆ¶å‰‚è¿˜ä¸å¾—æ£€å‡ºæ²™é—¨èŒï¼ˆ10gæˆ–10mlï¼‰</td>

										</tr>

										<tr>

											<td>æ¶²ä½“åŠåŠå›ºä½“åˆ¶å‰‚</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒï¼ˆ1gæˆ–1mlï¼‰ï¼›å«è„å™¨æå–ç‰©çš„åˆ¶å‰‚è¿˜ä¸å¾—æ£€å‡ºæ²™é—¨èŒï¼ˆ10gæˆ–10mlï¼‰</td>

										</tr>

										<tr>

											<td rowspan="3">å£è…”é»è†œç»™è¯åˆ¶å‰‚<br>é½¿é¾ˆç»™è¯åˆ¶å‰‚<br>é¼»ç”¨åˆ¶å‰‚</td>

											<td>-</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒã€é‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰</td>

										</tr>

										<tr>

											<td>è€³ç”¨åˆ¶å‰‚</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰</td>

										</tr>

										<tr>

											<td>çš®è‚¤ç»™è¯åˆ¶å‰‚</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰</td>

										</tr>

										<tr>

											<td>å‘¼å¸é“å¸å…¥ç»™è¯åˆ¶å‰‚</td>

											<td>-</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒã€é‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒã€è€èƒ†ç›é©å…°é˜´æ€§èŒï¼ˆ1gæˆ–1mlï¼‰</td>

										</tr>

										<tr>

											<td>é˜´é“ã€å°¿é“ç»™è¯åˆ¶å‰‚</td>

											<td>-</td>

											<td>10Â²</td>

											<td>10Â¹</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒã€ç™½è‰²å¿µç èŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰ï¼›ä¸­è¯åˆ¶å‰‚è¿˜ä¸å¾—æ£€å‡ºæ¢­èŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰</td>

										</tr>

										<tr>

											<td rowspan="2">ç›´è‚ ç»™è¯</td>

											<td>å›ºä½“åŠåŠå›ºä½“åˆ¶å‰‚</td>

											<td>10Â³</td>

											<td>10Â²</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1g æˆ–1mlï¼‰</td>

										</tr>

										<tr>

											<td>æ¶²ä½“åˆ¶å‰‚</td>

											<td>10Â²</td>

											<td>10Â²</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1g æˆ–1mlï¼‰</td>

										</tr>

										<tr>

											<td>å…¶ä»–å±€éƒ¨ç»™è¯åˆ¶å‰‚</td>

											<td>-</td>

											<td>10Â²</td>

											<td>10Â²</td>

											<td>ä¸å¾—æ£€å‡ºé‡‘é»„è‰²è‘¡è„çƒèŒã€é“œç»¿å‡å•èƒèŒï¼ˆ1gã€1mlæˆ–10cã¡ï¼‰</td>

										</tr>

									</table>
									æ³¨ï¼šâ‘ åŒ–å­¦è¯å“åˆ¶å‰‚å’Œç”Ÿç‰©åˆ¶å“åˆ¶å‰‚è‹¥å«æœ‰æœªç»æå–çš„åŠ¨æ¤ç‰©æ¥æºçš„æˆåˆ†åŠçŸ¿ç‰©è´¨ï¼Œè¿˜ä¸å¾—æ£€å‡ºæ²™é—¨èŒï¼ˆ10gæˆ–10mlï¼‰ã€‚""",
				 metadata={"topic": "technology", "product": "iPhone", "id": "tech_iphone"}),
		Document(page_content="åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯ä¸­å›½çš„æ”¿æ²»ã€æ–‡åŒ–ã€ç§‘æŠ€åˆ›æ–°å’Œå›½é™…äº¤å¾€ä¸­å¿ƒã€‚",
				 metadata={"topic": "geography", "city": "Beijing", "id": "geo_beijing"}),
		Document(page_content="FAISS (Facebook AI Similarity Search) æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œå¯†é›†å‘é‡èšç±»çš„åº“ã€‚",
				 metadata={"topic": "technology", "library": "FAISS", "id": "tech_faiss"})
	]
	from md2Document import read_md_file, create_document_from_md
	initial_langchain_docs = []
	folder_path = r'D:\Master\llm\database\kag\æµ‹è¯•æ•°æ®é›†'  # Windowsè·¯å¾„
	# è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶
	files = os.listdir(folder_path)
	# ç­›é€‰å‡ºæ‰€æœ‰ .md æ–‡ä»¶
	md_files = [file for file in files if file.endswith('.md')]
	# åˆ›å»ºDocumentå¯¹è±¡åˆ—è¡¨

	for file in md_files:
		file_path = os.path.join(folder_path, file)  # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
		document = create_document_from_md(file_path)  # åˆ›å»ºDocumentå¯¹è±¡
		initial_langchain_docs.append(document)

	try:
		# force_rebuild=True ç¡®ä¿æ¯æ¬¡è¿è¡Œæ—¶éƒ½é‡æ–°æ„å»ºç´¢å¼•ï¼Œä¾¿äºæµ‹è¯•
		# åœ¨ç”Ÿäº§ä¸­ï¼Œé€šå¸¸ä¼šè®¾ä¸º False ä»¥åŠ è½½ç°æœ‰ç´¢å¼•
		chroma_kb = ChromaKnowledgeBase(
			initial_documents=initial_langchain_docs,
			embedding_function=embedding_function,  # Langchain-compatible embedding function
			persist_directory=CHROMA_PERSIST_DIRECTORY,
			collection_name=CHROMA_COLLECTION_NAME,
			force_rebuild=True
		)
	except Exception as e:
		print(f"åˆ›å»ºæˆ–åŠ è½½ChromaçŸ¥è¯†åº“å¤±è´¥: {e}")
		import traceback;
		traceback.print_exc()
		return

	# 3. åˆå§‹åŒ– Prompts
	planner_prompt = PlannerPrompt()
	deduce_prompt = DeducePrompt()
	code_exec_prompt = CodeExecutionPrompt()
	generation_prompt = GenerationPrompt()

	# 4. åˆå§‹åŒ– Executors
	retrieval_executor = RetrievalExecutor(kb=chroma_kb)  # ä½¿ç”¨ChromaKB
	deduce_executor = DeduceExecutor(llm_client, deduce_prompt)
	code_executor = CodeExecutor(llm_client, code_exec_prompt)

	executors_map = {
		"RetrievalExecutor": retrieval_executor,
		"DeduceExecutor": deduce_executor,
		"CodeExecutor": code_executor,
	}

	# 5. åˆå§‹åŒ– Planner
	planner = Planner(llm_client, planner_prompt)

	# 6. åˆå§‹åŒ– Generator
	generator = AnswerGenerator(llm_client, generation_prompt)

	# 7. åˆå§‹åŒ– Pipeline
	pipeline = IterativePipeline(planner, executors_map, generator, max_iterations=1)

	# 8. è¿è¡Œç”¨æˆ·æŒ‡å®šçš„æŸ¥è¯¢
	user_query_to_run = "æ­£æŸ´èƒ¡é¥®é¢—ç²’çš„æ£€æŸ¥å†…å®¹æœ‰å“ªäº›æ–¹é¢ï¼Ÿè¯·è¯¦ç»†è¯´æ˜ã€‚"
	# user_query_to_run = "ä¸€åªhuanghuaçš„æ€§çŠ¶"
	print(f"\nğŸš€ Running user query: \"{user_query_to_run}\"")
	final_answer = await pipeline.run(user_query_to_run)
	print(f"\nğŸ Final Answer for user query: \n{final_answer}")


if __name__ == "__main__":
	# ç¡®ä¿åœ¨è¿è¡Œæ­¤è„šæœ¬å‰è®¾ç½® OPENAI_API_KEY, OPENAI_BASE_URL (å¦‚æœéœ€è¦), å’Œ LLM_MODEL_NAME ç¯å¢ƒå˜é‡
	# ä¾‹å¦‚:
	# export OPENAI_API_KEY="sk-yourdashscopekeyoropenaikey"
	# export OPENAI_BASE_URL="[https://dashscope.aliyuncs.com/compatible-mode/v1](https://dashscope.aliyuncs.com/compatible-mode/v1)" # (å¦‚æœç”¨DashScope)
	# export LLM_MODEL_NAME="qwen-plus"
	# export DASHSCOPE_API_KEY_FOR_EMBEDDING="your_dashscope_key_if_different" (å¦‚æœembedding keyä¸åŒ)

	# æˆ–è€…åœ¨ä»£ç ä¸­ç›´æ¥ä¿®æ”¹ OpenAIChatLLM å’Œ QwenEmbeddingFunction çš„API Key (ä¸æ¨èç”¨äºç”Ÿäº§)

	print("å¼€å§‹æ‰§è¡Œä¸»é€»è¾‘...")
	print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")


	asyncio.run(run_main_logic_with_user_data())