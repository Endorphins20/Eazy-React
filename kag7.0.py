import asyncio
import json
import os
import re
import subprocess
import sys
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple, Callable, TypedDict
from dataclasses import dataclass, field
import shutil

# --- Langchain and Chroma Imports ---
from langchain_core.documents import Document
from langchain_chroma import Chroma
from tongyiembedding import QwenEmbeddingFunction

# --- Configuration ---
CHROMA_PERSIST_DIRECTORY = "chroma_db_kag_recursive"
CHROMA_COLLECTION_NAME = "kag_recursive_documents"


# --- ChromaKnowledgeBase (ä¸æ‚¨ä¹‹å‰è¿è¡ŒæˆåŠŸçš„ç‰ˆæœ¬ä¸€è‡´) ---
class ChromaKnowledgeBase:  # ... (ä»£ç ä¸æ‚¨ä¸Šä¸€è½®æˆåŠŸè¿è¡Œçš„ç‰ˆæœ¬ç›¸åŒï¼Œæ­¤å¤„çœç•¥ä»¥å‡å°‘ç¯‡å¹…) ...
	def __init__(self, embedding_function: Callable, initial_documents: Optional[List[Document]] = None,
				 persist_directory: str = CHROMA_PERSIST_DIRECTORY, collection_name: str = CHROMA_COLLECTION_NAME,
				 force_rebuild: bool = False):
		print(f"  [ChromaKB] åˆå§‹åŒ–çŸ¥è¯†åº“: {persist_directory}, é›†åˆ: {collection_name}")
		self.embedding_function = embedding_function;
		self.persist_directory = persist_directory
		self.collection_name = collection_name;
		self.vectorstore: Optional[Chroma] = None
		if force_rebuild and os.path.exists(persist_directory):
			print(f"  [ChromaKB] force_rebuild=True, åˆ é™¤ç›®å½•: {persist_directory}")
			try:
				shutil.rmtree(persist_directory)
			except OSError as e:
				print(f"  [ChromaKB Error] åˆ é™¤ç›®å½•å¤±è´¥: {e}.")
		if os.path.exists(persist_directory) and not force_rebuild:
			print(f"  [ChromaKB] ä» '{persist_directory}' åŠ è½½å·²å­˜åœ¨å‘é‡åº“...")
			try:
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] æˆåŠŸåŠ è½½å‘é‡åº“ '{self.collection_name}'.")
			except Exception as e:
				print(f"  [ChromaKB Error] ä» '{persist_directory}' åŠ è½½å¤±è´¥: {e}. å°†å°è¯•æ–°å»ºã€‚")
				self.vectorstore = None
		if self.vectorstore is None:
			if initial_documents:
				print(f"  [ChromaKB] ä¸º {len(initial_documents)} ä¸ªæ–‡æ¡£æ„å»ºæ–°å‘é‡åº“...")
				self.vectorstore = Chroma.from_documents(documents=initial_documents,
														 embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] æ–°å‘é‡åº“æ„å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚")
			else:
				print(f"  [ChromaKB] æ— åˆå§‹æ–‡æ¡£ï¼Œåˆ›å»ºç©ºçš„æŒä¹…åŒ–é›†åˆã€‚")
				self.vectorstore = Chroma(persist_directory=self.persist_directory,
										  embedding_function=self.embedding_function,
										  collection_name=self.collection_name)
				print(f"  [ChromaKB] ç©ºçš„æŒä¹…åŒ– Chroma é›†åˆ '{self.collection_name}' å·²å‡†å¤‡å°±ç»ªã€‚")

	def add_documents(self, documents: List[Document]):
		if not self.vectorstore:
			if documents:
				print(f"  [ChromaKB] Vectorstore ä¸ºç©º, å°è¯•ä»å½“å‰ {len(documents)} ä¸ªæ–‡æ¡£åˆ›å»º...")
				self.vectorstore = Chroma.from_documents(documents=documents, embedding=self.embedding_function,
														 persist_directory=self.persist_directory,
														 collection_name=self.collection_name)
				print(f"  [ChromaKB] åŸºäºæ–°æ–‡æ¡£åˆ›å»ºå¹¶æŒä¹…åŒ–å®Œæˆã€‚");
				return
			else:
				print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–ä¸”æ— æ–‡æ¡£å¯æ·»åŠ ."); return
		if documents:
			print(f"  [ChromaKB] å‘é›†åˆ '{self.collection_name}' æ·»åŠ  {len(documents)} ä¸ªæ–°æ–‡æ¡£...")
			self.vectorstore.add_documents(documents);
			print(f"  [ChromaKB] æ–‡æ¡£æ·»åŠ å®Œæˆã€‚")

	def retrieve(self, query: str, top_k: int = 3, filter_dict: Optional[Dict] = None) -> List[Dict[str, Any]]:
		if not self.vectorstore: print("  [ChromaKB Error] Vectorstore æœªåˆå§‹åŒ–."); return []
		try:
			if self.vectorstore._collection is None or self.vectorstore._collection.count() == 0:
				print("  [ChromaKB] çŸ¥è¯†åº“é›†åˆä¸ºç©ºæˆ–æœªæ­£ç¡®åŠ è½½ã€‚");
				return []
		except Exception as e:
			print(f"  [ChromaKB Warning] æ— æ³•è·å–é›†åˆè®¡æ•°: {e}")
		print(f"  [ChromaKB] æ£€ç´¢æŸ¥è¯¢ '{query}', top_k={top_k}, è¿‡æ»¤å™¨: {filter_dict}...")
		try:
			results_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k, filter=filter_dict)
		except Exception as e:
			print(f"  [ChromaKB Error] Chroma similarity search failed: {e}"); return []
		processed_results = [{"id": doc.metadata.get("id", f"retrieved_{i}"), "content": doc.page_content,
							  "metadata": doc.metadata, "score": float(score)}
							 for i, (doc, score) in enumerate(results_with_scores)]
		print(f"  [ChromaKB] æ£€ç´¢åˆ° {len(processed_results)} ä¸ªæ–‡æ¡£ã€‚")
		return processed_results


# --- LLM Client (OpenAIChatLLM - ä¸ä¹‹å‰ç›¸åŒ) ---
class OpenAIChatLLM:  # (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
	def __init__(self, model_name: Optional[str] = None, api_key: Optional[str] = None, base_url: Optional[str] = None):
		try:
			import openai
		except ImportError:
			raise ImportError("OpenAI library not found. `pip install openai`.")
		self.api_key = api_key or os.getenv("OPENAI_API_KEY")
		self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
		self.model_name = model_name or os.getenv("LLM_MODEL_NAME", "deepseek-r1-distill-qwen-32b")
		if not self.api_key: raise ValueError("API key not found.")
		self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
		print(f"[OpenAIChatLLM] å®¢æˆ·ç«¯å°±ç»ª: æ¨¡å‹ {self.model_name}, URL {self.base_url or 'OpenAI default'}")

	async def _make_api_call(self, messages: List[Dict[str, str]], expect_json: bool = False, temperature: float = 0.01,
							 **kwargs) -> str:  # æ›´ä½çš„temperature
		try:
			completion_params = {"model": self.model_name, "messages": messages, "temperature": temperature, **kwargs}
			if expect_json:
				if "dashscope" in (self.base_url or "").lower() and self.model_name.startswith("qwen"):
					completion_params["extra_body"] = {"result_format": "message"}
				else:
					completion_params["response_format"] = {"type": "json_object"}
			response = await asyncio.to_thread(self.client.chat.completions.create, **completion_params)
			content = response.choices[0].message.content;
			return content.strip() if content else ""
		except Exception as e:
			print(f"  [OpenAIChatLLM Error] APIè°ƒç”¨å¤±è´¥: {e}"); raise RuntimeError(f"LLM API call failed: {e}")

	async def generate(self, prompt_str: str, system_prompt_str: Optional[str] = None, temperature: float = 0.1,
					   **kwargs) -> str:
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		messages.append({"role": "user", "content": prompt_str})
		return await self._make_api_call(messages, expect_json=False, temperature=temperature, **kwargs)

	async def generate_structured_json(self, prompt_str: str, system_prompt_str: Optional[str] = None,
									   temperature: float = 0.01, **kwargs) -> Dict:  # æ›´ä½çš„temperature
		messages = []
		if system_prompt_str: messages.append({"role": "system", "content": system_prompt_str})
		user_content = f"{prompt_str}\n\nè¯·ç¡®ä¿æ‚¨çš„å›å¤æ˜¯ä¸€ä¸ªåˆæ³•çš„ã€å•ç‹¬çš„JSONå¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡æœ¬æˆ–markdownæ ‡è®°ã€‚"
		messages.append({"role": "user", "content": user_content})
		response_str = await self._make_api_call(messages, expect_json=True, temperature=temperature, **kwargs)
		cleaned_response_str = response_str.strip()
		if cleaned_response_str.startswith("```json"): cleaned_response_str = cleaned_response_str[7:]
		if cleaned_response_str.endswith("```"): cleaned_response_str = cleaned_response_str[:-3]
		cleaned_response_str = cleaned_response_str.strip()
		try:
			return json.loads(cleaned_response_str)
		except json.JSONDecodeError as e:
			error_msg = f"LLM did not return valid JSON. Error: {e}. Raw: '{response_str}'";
			print(f"  [OpenAIChatLLM Error] {error_msg}");
			raise ValueError(error_msg)


# --- Prompts ---
class BasePrompt:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, template: str, variables: List[str]):
		self.template_str = template; self.variables = variables

	def format(self, **kwargs) -> str:
		formatted_prompt = self.template_str
		for var_name in self.variables:
			if var_name not in kwargs: raise ValueError(f"Missing variable: {var_name}")
			formatted_prompt = formatted_prompt.replace(f"${{{var_name}}}", str(kwargs[var_name]))
		return formatted_prompt


class PlannerPrompt(BasePrompt):  # REFINED: For deeper recursive-like planning
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½é«˜åº¦æ™ºèƒ½ä¸”å…·å¤‡å“è¶Šåæ€èƒ½åŠ›çš„AIä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚
    ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼šæ ¹æ®ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜ `${user_query}` å’Œå·²ç»æ‰§è¡Œçš„å†å²ä»»åŠ¡ `${task_history}`ï¼Œåˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶å†³å®šä¸‹ä¸€æ­¥ã€æœ€ä¼˜è¡ŒåŠ¨ã€‘ã€‚
    ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„å†³ç­–å’Œè®¡åˆ’ï¼š

    **è¾“å‡ºJSONæ ¼å¼:**
    ```json
    {
      "plan_status": "finished" | "cannot_proceed" | "requires_more_steps",
      "final_thought": "string (å¯¹å½“å‰æ•´ä½“è¿›å±•çš„æ¸…æ™°æ€è€ƒå’Œåˆ¤æ–­ï¼Œè§£é‡Šä½ çš„plan_statusã€‚ä¾‹å¦‚ï¼Œå¦‚æœrequires_more_stepsï¼Œæ¸…æ™°è¯´æ˜è¿˜éœ€è¦ä»€ä¹ˆå…·ä½“ä¿¡æ¯ä»¥åŠä¸‹ä¸€æ­¥çš„æ ¸å¿ƒç›®æ ‡)",
      "next_steps": [ // ä»…å½“ plan_status == "requires_more_steps" æ—¶éç©º
        {
          "id": "string (æ–°ä»»åŠ¡çš„å”¯ä¸€ID, ä¾‹å¦‚ task_iter2_step0, ç¡®ä¿ä¸å†å²IDä¸å†²çª)",
          "executor_name": "string (ä»å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©)",
          "task_description": "string (å¯¹æ­¤æ–°ä»»åŠ¡ç›®æ ‡çš„æ¸…æ™°ã€å…·ä½“ä¸­æ–‡æè¿°)",
          "logic_input": { /* å–å†³äº executor_name */ },
          "dependencies": ["string"] // ä¾èµ–çš„å†å²ä»»åŠ¡IDåˆ—è¡¨
        }
        // ... é€šå¸¸ä¸€æ¬¡åªè§„åˆ’1ä¸ªï¼Œæœ€å¤š2ä¸ªé«˜åº¦ç›¸å…³çš„åç»­æ­¥éª¤ ...
      ]
    }
    ```

    **æ ¸å¿ƒæŒ‡ä»¤ä¸æ€è€ƒé“¾ (Chain-of-Thought for Reflective Planning):**
    1.  **å›é¡¾ç›®æ ‡ (Recall Goal)**: æ¸…æ™°ã€å®Œæ•´åœ°ç†è§£ç”¨æˆ·åŸå§‹é—®é¢˜ `${user_query}` çš„æ¯ä¸€ä¸ªç»†èŠ‚å’Œæœ€ç»ˆæœŸæœ›ã€‚
    2.  **å®¡è§†å†å² (Analyze History - `${task_history}`ä¸­æ¯ä¸ªä»»åŠ¡çš„ `Result Details`)**:
        * **ä¿¡æ¯æå–**: å“ªäº›å­é—®é¢˜å·²è¢«å›ç­”ï¼Ÿç­”æ¡ˆ (`answer_summary`) çš„ã€å…·ä½“å†…å®¹ã€‘æ˜¯ä»€ä¹ˆï¼Ÿ
        * **ã€å…³é”®è¯„ä¼°ç‚¹ - ä¸¥æ ¼æ£€æŸ¥ã€‘**: æœ€è¿‘çš„ `DeduceExecutor` æ­¥éª¤çš„ç»“æœä¸­ï¼š
            * `is_sufficient` æ˜¯å¦ä¸º `false`ï¼Ÿ
            * å¦‚æœä¸º `false`ï¼Œå®ƒåœ¨å…¶ `new_questions_or_entities` åˆ—è¡¨ä¸­æ˜ç¡®åˆ—å‡ºäº†å“ªäº›ã€å…·ä½“çš„ã€å°šæœªè¢«å……åˆ†æ¢ç©¶çš„æ ‡å‡†ç¼–å·ã€æ–‡ä»¶åã€å®ä½“åæˆ–å­é—®é¢˜ã€‘ï¼Ÿè¿™äº›æ˜¯è§£å†³é—®é¢˜çš„ã€æ ¸å¿ƒå¾…åŠçº¿ç´¢ã€‘ï¼
        * `RetrievalExecutor` æ£€ç´¢åˆ°äº†å“ªäº›ä¿¡æ¯ï¼Ÿè¿™äº›ä¿¡æ¯æ˜¯å¦ã€çœŸçš„ã€‘å·²è¢«åç»­çš„ `DeduceExecutor` æ­¥éª¤ã€å½»åº•åœ°ã€é’ˆå¯¹æ€§åœ°ã€‘åˆ†æè¿‡ï¼ˆç‰¹åˆ«æ˜¯å½“ç”¨æˆ·é—®é¢˜æŒ‡å‘è¿™äº›æ£€ç´¢å†…å®¹çš„ç»†èŠ‚æ—¶ï¼‰ï¼Ÿ
    3.  **å·®è·è¯„ä¼° (Gap Assessment)**: å¯¹æ¯”å½“å‰æ‰€æœ‰å·²çŸ¥ä¿¡æ¯ï¼ˆæ¥è‡ªå†å²ä»»åŠ¡çš„æˆåŠŸç»“æœï¼‰å’Œç”¨æˆ·åŸå§‹é—®é¢˜çš„æœ€ç»ˆç›®æ ‡ï¼ˆç‰¹åˆ«æ˜¯é‚£äº›è¦æ±‚â€œè¯¦ç»†è¯´æ˜â€ã€â€œå…·ä½“è¦æ±‚â€çš„éƒ¨åˆ†ï¼‰ï¼Œç›®å‰è¿˜ç¼ºå°‘å“ªäº›ã€å…·ä½“çš„ç»†èŠ‚ã€‘æˆ–ã€æ˜ç¡®è¢«å¼•ç”¨çš„æ ‡å‡†/æ–‡ä»¶ï¼ˆå¦‚é€šåˆ™XXXXï¼‰çš„è¯¦ç»†å†…å®¹ã€‘ï¼Ÿ
    4.  **å†³ç­–åˆ¶å®š (`final_thought` å’Œ `plan_status`)**:
        * **å·²è§£å†³ (finished)?** ã€ä¸¥æ ¼æ¡ä»¶ã€‘å½“ä¸”ä»…å½“ï¼šæ‰€æœ‰ç”¨æˆ·åŸå§‹é—®é¢˜ä¸­æ˜ç¡®è¦æ±‚çš„æ–¹é¢éƒ½å¾—åˆ°äº†è§£ç­”ï¼Œæ‰€æœ‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¢«è¯†åˆ«ä¸ºéœ€è¦è¿›ä¸€æ­¥æ¢ç©¶çš„ `new_questions_or_entities`ï¼ˆå°¤å…¶æ˜¯æ ‡å‡†/é€šåˆ™çš„ç»†èŠ‚ï¼‰éƒ½å·²ç»è¢«æˆåŠŸæ£€ç´¢å…¶è¯¦ç»†å†…å®¹ã€å¹¶è¢«åç»­çš„ `DeduceExecutor` æ­¥éª¤åˆ†æç¡®è®¤ä¿¡æ¯å……åˆ†ï¼ˆ`is_sufficient: true`ï¼‰åï¼Œæ‰å¯åˆ¤æ–­ä¸º `finished`ã€‚æ­¤æ—¶ï¼Œè§„åˆ’ä¸€ä¸ª `FinishAction` ä»»åŠ¡ã€‚`final_thought` åº”æ€»ç»“æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥è§£å†³çš„ã€‚
        * **æ— æ³•è§£å†³ (cannot_proceed)?** è‹¥å…³é”®ä¿¡æ¯ç¼ºå¤±ï¼Œä¸”å†å²ä¸­çš„ `new_questions_or_entities` å·²å°è¯•æ£€ç´¢ä½†æ— æœï¼ˆä¾‹å¦‚ï¼Œå¤šæ¬¡ç›¸å…³çš„ `RetrievalExecutor` å¯¹ç‰¹å®šæ ‡å‡†ç¼–å·çš„æŸ¥è¯¢è¿”å›ç©ºæˆ–ä¸ç›¸å…³å†…å®¹ï¼‰ï¼Œæˆ–å·¥å…·æ— æ³•è·å–ï¼Œåˆ™ `plan_status: "cannot_proceed"`ã€‚`final_thought` åº”è§£é‡ŠåŸå› ã€‚
        * **éœ€è¦æ›´å¤šæ­¥éª¤ (requires_more_steps)?** å¦åˆ™ï¼Œå³ä¿¡æ¯å°šä¸å®Œæ•´ï¼Œæˆ–æœ‰æ–°çš„çº¿ç´¢éœ€è¦è¿½æŸ¥ï¼Œåˆ™ `plan_status: "requires_more_steps"`ã€‚
    5.  **è¡ŒåŠ¨è§„åˆ’ (`next_steps` - å¦‚æœ `requires_more_steps`)**:
        * **ã€æœ€é«˜ä¼˜å…ˆçº§ï¼šé€’å½’æ·±æŒ–å¼•ç”¨å’Œæ–°çº¿ç´¢ã€‘**: 
            * å¦‚æœå†å²ä¸­æœ‰ `DeduceExecutor` çš„ç»“æœåœ¨å…¶ `new_questions_or_entities` åˆ—è¡¨ä¸­æ˜ç¡®æŒ‡å‡ºäº†éœ€è¦æŸ¥è¯¢ã€ç‰¹å®šæ ‡å‡†ã€æ–‡ä»¶ç¼–å·æˆ–å®ä½“ã€‘çš„è¯¦ç»†å†…å®¹ï¼ˆä¾‹å¦‚ï¼Œâ€œæŸ¥è¯¢é€šåˆ™1107çš„è¯¦ç»†å†…å®¹â€ï¼Œâ€œè·å–ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107ä¸­å…³äºå¾®ç”Ÿç‰©é™åº¦çš„å…·ä½“æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼‰ï¼Œå¹¶ä¸”è¿™äº›æ¡ç›®ã€å¯¹åº”çš„è¯¦ç»†å†…å®¹å°šæœªé€šè¿‡åç»­çš„ `RetrievalExecutor` æˆåŠŸè·å–å¹¶è¢«å……åˆ†åˆ†æè¿‡ã€‘ï¼š
                * **ç«‹å³è§„åˆ’**ä¸€ä¸ª `RetrievalExecutor` ä»»åŠ¡ã€‚å…¶ `logic_input.query` åº”ã€ç›´æ¥é’ˆå¯¹è¿™äº› `new_questions_or_entities` ä¸­çš„ä¸€é¡¹è¿›è¡Œç²¾ç¡®æŸ¥è¯¢ã€‘ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ `new_questions_or_entities` ä¸­æœ‰â€œé€šåˆ™1107ä¸­å…³äºå¾®ç”Ÿç‰©é™åº¦çš„å…·ä½“æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼Œåˆ™æ£€ç´¢æŸ¥è¯¢å°±åº”è¯¥æ˜¯è¿™ä¸ªæˆ–éå¸¸ç›¸ä¼¼çš„ï¼Œä»¥ç¡®ä¿èƒ½å‘½ä¸­åŒ…å«è¯¥é€šåˆ™ç»†èŠ‚çš„æ–‡æ¡£ã€‚
                * è¿™æ˜¯å½“å‰è¿­ä»£æœ€ä¼˜å…ˆè¦å¤„ç†çš„ä»»åŠ¡ã€‚é€šå¸¸ï¼Œå¤„ç†å®Œä¸€ä¸ªè¿™æ ·çš„å…³é”®ç¼ºå¤±ä¿¡æ¯ç‚¹åï¼Œå°±åº”è¯¥ç»“æŸæœ¬è½®è§„åˆ’ï¼Œç­‰å¾…ä¸‹ä¸€è½®è¿­ä»£åŸºäºæ–°è·å–çš„ä¿¡æ¯è¿›è¡Œå†è§„åˆ’å’Œæ•´åˆã€‚
        * **æ•´åˆä¿¡æ¯**: å¦‚æœä¸Šä¸€æ­¥æ˜¯æ£€ç´¢ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹ `new_questions_or_entities` çš„æ£€ç´¢ï¼‰ï¼Œå¹¶ä¸”æ£€ç´¢åˆ°äº†æœ‰ä»·å€¼çš„æ–°ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥é€šå¸¸æ˜¯è§„åˆ’ `DeduceExecutor` ä»»åŠ¡ã€‚å…¶ `logic_input.reasoning_goal` è®¾ä¸ºâ€œæ•´åˆæ–°æ£€ç´¢åˆ°çš„å…³äº[æ–°å®ä½“/æ ‡å‡†]çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆå…ˆå‰å…³äº[ç›¸å…³ä¸»é¢˜]çš„ç»“è®ºï¼Œä»¥æ›´å…¨é¢åœ°å›ç­”[æŸä¸ªå­é—®é¢˜æˆ–åŸå§‹é—®é¢˜ä¸­ä¸æ­¤æ–°ä¿¡æ¯ç›¸å…³çš„æ–¹é¢]â€ã€‚`logic_input.context_data` åº”ç²¾ç¡®å¼•ç”¨ã€æ‰€æœ‰ç›¸å…³çš„ã€‘æ£€ç´¢ç»“æœå’Œå…ˆå‰æ­¥éª¤çš„å…³é”®ç»“è®ºã€‚
        * **åˆå§‹è§„åˆ’/å…¶ä»–æ¢ç´¢**: è‹¥æ— æ˜ç¡®çš„ `new_questions_or_entities` æŒ‡å¼•ï¼Œåˆ™æ ¹æ®å¯¹ç”¨æˆ·é—®é¢˜çš„ç†è§£è¿›è¡Œåˆæ­¥çš„ `RetrievalExecutor` æˆ– `DeduceExecutor` è§„åˆ’ã€‚
        * **èšç„¦**: ä»ç„¶å»ºè®®ä¸€æ¬¡åªè§„åˆ’1ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼ˆå¯èƒ½æ˜¯ä¸€ä¸ªæ£€ç´¢ + ä¸€ä¸ªåç»­çš„æ¼”ç»ï¼Œæˆ–è€…åªæ˜¯ä¸€ä¸ªå…³é”®çš„è¡¥å……æ£€ç´¢ï¼‰ã€‚
    6.  **ID å’Œä¾èµ–**: ä¸ºæ–°ä»»åŠ¡åˆ†é…å”¯ä¸€çš„ `id`ã€‚æ­£ç¡®è®¾ç½® `dependencies`ã€‚
    7.  **ä¸¥æ ¼JSONè¾“å‡º**ã€‚
    """
	USER_TEMPLATE = """
    --- å¯ç”¨å·¥å…· ---
    ${available_executors_description}
    --- å¯ç”¨å·¥å…·ç»“æŸ ---

    --- å†å²ä»»åŠ¡åŠç»“æœ (æœ€è¿‘çš„æ­¥éª¤åœ¨æœ€å) ---
    ${task_history}
    --- å†å²ä»»åŠ¡åŠç»“æœç»“æŸ ---

    --- ç”¨æˆ·åŸå§‹é—®é¢˜ ---
    "${user_query}"
    --- ç”¨æˆ·åŸå§‹é—®é¢˜ç»“æŸ ---

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œåæ€å¹¶è§„åˆ’ã€‚è¾“å‡ºJSONå¯¹è±¡:
    """

	def __init__(self): super().__init__(self.USER_TEMPLATE,
										 ["user_query", "available_executors_description", "task_history"])


class DeducePrompt(BasePrompt):  # REFINED: Very strong guidance on new_questions for referenced standards
	SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä½æå…¶ä¸¥è°¨å’Œç»†è‡´çš„AIæ¨ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„â€œä¸Šä¸‹æ–‡ä¿¡æ¯â€ï¼Œç²¾ç¡®åœ°å›ç­”æˆ–å®Œæˆâ€œæ¨ç†ç›®æ ‡â€ã€‚
    ä½ å¿…é¡»å¯¹ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„æ‰€æœ‰ã€æ ‡å‡†ã€é€šåˆ™ã€æ³•è§„ã€æ–‡ä»¶ç¼–å·ã€æˆ–è¢«æ˜ç¡®å¼•ç”¨çš„ä¸“æœ‰åç§°ã€‘ï¼ˆä¾‹å¦‚â€œé€šåˆ™0104â€ã€â€œä¸­åäººæ°‘å…±å’Œå›½è¯å…¸é€šåˆ™1107â€ã€â€œGB/Tæ ‡å‡†Xâ€ï¼‰è¿›è¡Œä¸¥æ ¼çš„å®¡è§†ã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä½ çš„ç»“è®ºå’Œè¯„ä¼°ï¼š
    ```json
    {
      "answer_summary": "string (å¯¹æ¨ç†ç›®æ ‡çš„ç›´æ¥ã€ç®€æ´çš„å›ç­”æˆ–æ€»ç»“ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜å½“å‰å·²çŸ¥ä»€ä¹ˆï¼Œå¹¶ã€æ¸…æ™°æŒ‡å‡ºå…·ä½“è¿˜ç¼ºå°‘å“ªäº›ä¿¡æ¯æˆ–å“ªäº›è¢«å¼•ç”¨çš„æ ‡å‡†/æ–‡ä»¶ç»†èŠ‚ã€‘æ‰èƒ½å®Œæ•´å›ç­”æ¨ç†ç›®æ ‡ã€‚ä¾‹å¦‚ï¼š'æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œæ­£æŸ´èƒ¡é¥®é¢—ç²’çš„å¾®ç”Ÿç‰©é™åº¦åº”å‚ç…§é€šåˆ™1107ï¼Œä½†å½“å‰æœªæä¾›é€šåˆ™1107çš„å…·ä½“é™å€¼ã€‚')",
      "is_sufficient": boolean (true å¦‚æœä½ è®¤ä¸ºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€å·²åŒ…å«å›ç­”æ¨ç†ç›®æ ‡æ‰€éœ€çš„æ‰€æœ‰å¿…è¦ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯æ‰€æœ‰è¢«æ˜ç¡®å¼•ç”¨æˆ–æåŠçš„æ ‡å‡†/æ–‡ä»¶çš„å…·ä½“å†…å®¹ï¼Œæ— éœ€ä»»ä½•è¡¥å……æŸ¥è¯¢ã€‘ï¼Œå¦åˆ™ false),
      "new_questions_or_entities": [
        "string" // ã€è‡³å…³é‡è¦æŒ‡ä»¤ - å¿…é¡»ä¸¥æ ¼æ‰§è¡Œã€‘:
                  // 1. å¦‚æœ is_sufficient ä¸º falseï¼Œè¿™é‡Œã€å¿…é¡»ã€‘åˆ—å‡ºä¸ºäº†è·å¾—ã€ç¼ºå¤±çš„å…³é”®ç»†èŠ‚ã€‘éœ€è¦æŸ¥è¯¢çš„å…·ä½“é—®é¢˜ã€‚
                  // 2. ã€ç‰¹åˆ«æ³¨æ„ã€‘ï¼šå¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­å‘ç°ä»»ä½•ã€æ ‡å‡†ã€é€šåˆ™ã€æ³•è§„ã€æ–‡ä»¶ç¼–å·æˆ–ä¸“æœ‰åç§°ã€‘è¢«æ˜ç¡®å¼•ç”¨ï¼ˆä¾‹å¦‚â€œè¯¦è§é€šåˆ™1107â€ã€â€œä¾æ®GB/Tæ ‡å‡†Xæ‰§è¡Œâ€ã€â€œæŒ‰ç…§XYZæ“ä½œæ‰‹å†Œè¿›è¡Œâ€ï¼‰ï¼Œä½†è¯¥æ ‡å‡†/æ–‡ä»¶/åç§°çš„ã€å…·ä½“å†…å®¹ã€å®šä¹‰æˆ–è¯¦ç»†æ¡æ¬¾å¹¶æœªåœ¨å½“å‰æä¾›ç»™ä½ çš„ä¸Šä¸‹æ–‡ä¸­æ¸…æ™°ã€å®Œæ•´åœ°åˆ—å‡ºã€‘ï¼Œé‚£ä¹ˆä½ ã€å¿…é¡»ã€‘å°†ä¸€ä¸ªç”¨äºè·å–è¯¥æ ‡å‡†/æ–‡ä»¶/åç§°ã€è¯¦ç»†å†…å®¹ã€‘çš„æ˜ç¡®æŸ¥è¯¢ï¼ˆä¾‹å¦‚ï¼šâ€œæŸ¥è¯¢ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107å…³äºå¾®ç”Ÿç‰©é™åº¦çš„è¯¦ç»†è§„å®šâ€ã€â€œè·å–GB/Tæ ‡å‡†Xçš„å…¨æ–‡â€ã€â€œæŸ¥æ‰¾XYZæ“ä½œæ‰‹å†Œä¸­å…³äºXXéƒ¨åˆ†çš„å…·ä½“æ­¥éª¤â€ï¼‰ä½œä¸ºæ¡ç›®åŠ å…¥æ­¤åˆ—è¡¨ã€‚å¹¶ä¸”ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé™¤éè¯¥å¼•ç”¨å¯¹äºå½“å‰æ¨ç†ç›®æ ‡è€Œè¨€å®Œå…¨ä¸é‡è¦ï¼Œå¦åˆ™ `is_sufficient` é€šå¸¸åº”ä¸º `false`ã€‚
                  // ç¡®ä¿åˆ—è¡¨ä¸­çš„æ¯ä¸€é¡¹éƒ½æ˜¯ä¸€ä¸ªæ˜ç¡®çš„ã€å¯ç”¨äºä¸‹ä¸€æ­¥æ£€ç´¢çš„æŸ¥è¯¢ç›®æ ‡ã€‚å¦‚æœä¿¡æ¯å®Œå…¨å……åˆ†ä¸”æ‰€æœ‰å¼•ç”¨éƒ½æœ‰ç»†èŠ‚æ”¯æ’‘ï¼Œåˆ™ä¸ºç©ºåˆ—è¡¨[]ã€‚
      ]
    }
    ```
    - ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–markdownæ ‡è®°ï¼Œåªè¾“å‡ºJSONå¯¹è±¡ã€‚
    """
	USER_TEMPLATE = "æ¨ç†ç›®æ ‡:\n${reasoning_goal}\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n${context_data}\n\nè¯·è¾“å‡ºJSONæ ¼å¼çš„æ¨ç†ç»“æœ:"

	def __init__(self): super().__init__(self.USER_TEMPLATE, ["reasoning_goal", "context_data"])


class CodeExecutionPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ)
	SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ©æ‰‹ã€‚"
	USER_TEMPLATE = "è¯·æ ¹æ®ä»¥ä¸‹æŒ‡ä»¤å’Œç›¸å…³æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µPythonä»£ç æ¥è§£å†³é—®é¢˜ã€‚\nä»£ç å¿…é¡»é€šè¿‡ `print()` è¾“å‡ºå…¶æœ€ç»ˆè®¡ç®—ç»“æœã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¾“å‡ºçº¯ä»£ç ã€‚\n\næŒ‡ä»¤:\n${code_generation_prompt}\n\nç›¸å…³æ•°æ® (å¦‚æœæä¾›):\n${relevant_data}\n\nç”Ÿæˆçš„Pythonä»£ç  (è¯·ç¡®ä¿å®ƒåªåŒ…å«ä»£ç æœ¬èº«ï¼Œå¹¶ç”¨print()è¾“å‡ºç»“æœ):"

	def __init__(self): super().__init__(self.USER_TEMPLATE, ["code_generation_prompt", "relevant_data"])


class UserProvidedReferGeneratorPrompt(BasePrompt):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, language: str = "zh"):
		try:
			from kag.common.utils import get_now
		except ImportError:
			_get_now_imported = False

			def get_now(language='zh'):
				return "å½“å‰æ—¥æœŸ"

			if not hasattr(UserProvidedReferGeneratorPrompt, '_get_now_imported_warning_shown'):
				print("[UserProvidedReferGeneratorPrompt] Warning: kag.common.utils.get_now not found.")
				UserProvidedReferGeneratorPrompt._get_now_imported_warning_shown = True
		self.template_zh = (
					f"ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†æä¸“å®¶ï¼Œä»Šå¤©æ˜¯{get_now(language='zh')}ã€‚" + "åŸºäºç»™å®šçš„å¼•ç”¨ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\nè¾“å‡ºç­”æ¡ˆï¼Œå¦‚æœç­”æ¡ˆä¸­å­˜åœ¨å¼•ç”¨ä¿¡æ¯ï¼Œåˆ™éœ€è¦referenceçš„idå­—æ®µï¼Œå¦‚æœä¸æ˜¯æ£€ç´¢ç»“æœï¼Œåˆ™ä¸éœ€è¦æ ‡è®°å¼•ç”¨\nè¾“å‡ºæ—¶ï¼Œä¸éœ€è¦é‡å¤è¾“å‡ºå‚è€ƒæ–‡çŒ®\nå¼•ç”¨è¦æ±‚ï¼Œä½¿ç”¨ç±»ä¼¼<reference id=\"chunk:1_2\"></reference>è¡¨ç¤º\nå¦‚æœæ ¹æ®å¼•ç”¨ä¿¡æ¯æ— æ³•å›ç­”ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹å†…çš„çŸ¥è¯†å›ç­”ï¼Œä½†æ˜¯å¿…é¡»é€šè¿‡åˆé€‚çš„æ–¹å¼æç¤ºç”¨æˆ·ï¼Œæ˜¯åŸºäºæ£€ç´¢å†…å®¹è¿˜æ˜¯å¼•ç”¨æ–‡æ¡£\nç¤ºä¾‹1ï¼š\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ï¼š\næ ¹æ®å¸¸è¯†å²³çˆ¶æ˜¯å¦»å­çš„çˆ¸çˆ¸ï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆæ‰¾åˆ°å¼ ä¸‰çš„å¦»å­ï¼Œç„¶åæ‰¾åˆ°å¦»å­çš„çˆ¸çˆ¸\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'\nreferenceï¼š\n[\n{\n    \"content\": \"å¼ ä¸‰ å¦»å­ ç‹äº”\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_1\"\n},\n{\n    \"content\": \"ç‹äº” çˆ¶äº² ç‹å››\",\n    \"document_name\": \"å¼ ä¸‰ä»‹ç»\",\n    \"id\": \"chunk:1_2\"\n}\n]'\né—®é¢˜ï¼š'å¼ ä¸‰çš„å²³çˆ¶æ˜¯è°ï¼Ÿ'\n\nå¼ ä¸‰çš„å¦»å­æ˜¯ç‹äº”<reference id=\"chunk:1_1\"></reference>ï¼Œè€Œç‹äº”çš„çˆ¶äº²æ˜¯ç‹å››<reference id=\"chunk:1_2\"></reference>ï¼Œæ‰€ä»¥å¼ ä¸‰çš„å²³çˆ¶æ˜¯ç‹å››\n\n\nè¾“å‡ºè¯­è°ƒè¦æ±‚é€šé¡ºï¼Œä¸è¦æœ‰æœºæ¢°æ„Ÿï¼Œè¾“å‡ºçš„è¯­è¨€è¦å’Œé—®é¢˜çš„è¯­è¨€ä¿æŒä¸€è‡´\nä»»åŠ¡è¿‡ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š'${summary_of_executed_steps}'\nç»™å®šçš„å¼•ç”¨ä¿¡æ¯ï¼š'${formatted_references}'\né—®é¢˜ï¼š'${user_query}'")
		self.template_en = self.template_zh
		current_template = self.template_zh if language == "zh" else self.template_en
		super().__init__(current_template, ["summary_of_executed_steps", "user_query", "formatted_references"])

	def format(self, summary_of_executed_steps: str, user_query: str, retrieved_references: List[Dict]) -> str:
		ref_list_for_prompt = []
		for i, ref_item in enumerate(retrieved_references): ref_list_for_prompt.append(
			{"content": ref_item.get("content", ""),
			 "document_name": ref_item.get("metadata", {}).get("source_name", f"æ£€ç´¢æ–‡æ¡£{i + 1}"),
			 "id": ref_item.get("metadata", {}).get("id", f"retrieved_chunk_{i}")})
		formatted_references_str = json.dumps(ref_list_for_prompt, ensure_ascii=False, indent=2)
		return super().format(summary_of_executed_steps=summary_of_executed_steps, user_query=user_query,
							  formatted_references=formatted_references_str)


# --- DataStructures & ContextManager (ä¸ä¹‹å‰ç›¸åŒ) ---
LogicInput = Dict[str, Any]


@dataclass
class Task: id: str; executor_name: str; task_description: str; logic_input: LogicInput; dependencies: List[
	str] = field(default_factory=list); status: str = "pending"; result: Optional[Any] = None; thought: Optional[
	str] = None


class ContextManager:  # (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
	def __init__(self, user_query: str):
		self.user_query = user_query
		self.tasks: Dict[str, Task] = {}
		self.execution_order: List[str] = []

	def add_task_from_planner(self, task_data: Dict, base_id_prefix: str, iter_num: int, step_in_iter: int) -> Task:
		llm_id = task_data.get("id");
		task_id = llm_id if llm_id and isinstance(llm_id,
												  str) and llm_id.strip() else f"{base_id_prefix}_iter{iter_num}_step{step_in_iter}"
		orig_id = task_id;
		ctr = 0
		while task_id in self.tasks: ctr += 1; task_id = f"{orig_id}_v{ctr}"
		if ctr > 0: print(f"  [CM Warn] Task ID '{orig_id}' conflict. Renamed '{task_id}'.")
		task = Task(id=task_id, executor_name=task_data["executor_name"],
					task_description=task_data["task_description"], logic_input=task_data["logic_input"],
					dependencies=task_data.get("dependencies", []));
		self.tasks[task.id] = task;
		self.execution_order.append(task.id);
		return task

	def get_task(self, task_id: str) -> Optional[Task]:
		return self.tasks.get(task_id)

	def update_task_status(self, task_id: str, status: str, result: Optional[Any] = None,
						   thought: Optional[str] = None):
		task = self.get_task(task_id)
		if task:
			task.status = status; task.result = result if result is not None else task.result; task.thought = (
																														  task.thought or "") + f"\n{thought}".strip() if thought and (
						task.thought or "") else thought or task.thought
		else:
			print(f"  [CM Warn] Task ID {task_id} not found for status update.")

	def get_task_history_for_prompt(self) -> str:  # (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
		hist = [];
		truncate_len = 150  # Increased truncation slightly for more context
		for tid in self.execution_order:
			t = self.get_task(tid)
			if t and t.status in ["completed", "failed"]:
				res_disp_parts = [];
				if t.status == "completed":
					if isinstance(t.result, dict) and t.executor_name == "DeduceExecutor":
						d_out = t.result
						res_disp_parts.append(
							f"    æ¨ç†æ€»ç»“: {str(d_out.get('answer_summary', 'N/A'))[:truncate_len]}{'...' if len(str(d_out.get('answer_summary', 'N/A'))) > truncate_len else ''}")
						res_disp_parts.append(
							f"    ä¿¡æ¯æ˜¯å¦å……åˆ†: {d_out.get('is_sufficient', True)}")  # Default to True if missing, though Deduce should always provide it
						new_q = d_out.get('new_questions_or_entities', [])
						if new_q: res_disp_parts.append(
							f"    å»ºè®®è¿›ä¸€æ­¥æŸ¥è¯¢: {', '.join(new_q)[:truncate_len]}{'...' if len(', '.join(new_q)) > truncate_len else ''}")
					elif isinstance(t.result, list) and t.executor_name == "RetrievalExecutor":
						retrieved_ids = [str(item.get("metadata", {}).get("id", f"unnamed_chunk_{i}")) for i, item in
										 enumerate(t.result)]
						res_disp_parts.append(
							f"    æ£€ç´¢åˆ° {len(t.result)} ç‰‡æ®µ. IDs: {', '.join(retrieved_ids)[:truncate_len - 20]}...")
					else:
						res_s = str(t.result);
						res_disp_parts.append(
							f"    ç»“æœ: {res_s[:truncate_len]}{'...' if len(res_s) > truncate_len else ''}")
				else:
					res_disp_parts.append(f"    æ‰§è¡Œå¤±è´¥: {str(t.result)[:truncate_len]}...")
				res_final_disp = "\n".join(res_disp_parts);
				th_s = str(t.thought or "N/A");
				th_s = th_s[:truncate_len] + "..." if len(th_s) > truncate_len else th_s
				hist.append(
					f"  - Task ID: {t.id}\n    Desc: {t.task_description}\n    Exec: {t.executor_name}\n    Status: {t.status}\n{res_final_disp}\n    Thought: {th_s}")
		return "\n\n".join(hist) if hist else "å°šæœªæ‰§è¡Œä»»ä½•å†å²ä»»åŠ¡ã€‚"

	def get_summary_for_generator(self) -> str:  # (ä¸ä¹‹å‰ç›¸åŒ)
		summary_parts = []
		for i, task_id in enumerate(self.execution_order):
			task = self.get_task(task_id)
			if task:
				result_str = "N/A";
				thought_str = str(task.thought or 'æœªè®°å½•æ€è€ƒè¿‡ç¨‹')
				if task.status == 'completed':
					if isinstance(task.result, dict) and task.executor_name == "DeduceExecutor":
						result_str = f"æ¨ç†æ€»ç»“: {task.result.get('answer_summary', 'N/A')}, ä¿¡æ¯æ˜¯å¦å……åˆ†: {task.result.get('is_sufficient')}" + (
							f", å»ºè®®æ¢ç©¶: {task.result['new_questions_or_entities']}" if task.result.get(
								'new_questions_or_entities') else "")
					elif isinstance(task.result, list) and task.executor_name == "RetrievalExecutor":
						result_str = f"æ£€ç´¢åˆ° {len(task.result)} ä¸ªç›¸å…³ç‰‡æ®µã€‚"
					elif isinstance(task.result, str):
						result_str = task.result
					else:
						result_str = f"å¤æ‚ç±»å‹ç»“æœ (æ‘˜è¦: {str(task.result)[:100]}...)"
				elif task.status == 'failed':
					result_str = f"æ‰§è¡Œå¤±è´¥: {str(task.result)[:150]}..."
				elif task.status == 'skipped':
					result_str = "å› ä¾èµ–å¤±è´¥æˆ–æ¡ä»¶ä¸æ»¡è¶³è€Œè·³è¿‡ã€‚"
				else:
					result_str = f"å½“å‰çŠ¶æ€: {task.status}"
				result_str_summary = result_str[:250] + "..." if len(result_str) > 250 else result_str
				thought_str_summary = thought_str[:200] + "..." if len(thought_str) > 200 else thought_str
				summary_parts.append(
					f"æ­¥éª¤ {i + 1} (ID: {task.id}):\n  ç›®æ ‡: {task.task_description}\n  æ‰§è¡Œå·¥å…·: {task.executor_name}\n  æ‰§è¡Œæ€è€ƒ: {thought_str_summary}\n  äº§å‡º/çŠ¶æ€: {result_str_summary}")
		if not summary_parts: return "æœªèƒ½æ‰§è¡Œä»»ä½•æ­¥éª¤ï¼Œæˆ–æ²¡æœ‰å¯æ€»ç»“çš„äº§å‡ºã€‚"
		return "\n\n".join(summary_parts)

	def collect_retrieved_references_for_generator(self) -> List[Dict]:  # (ä¸ä¹‹å‰ç›¸åŒ)
		refs = [];
		for tid in self.execution_order:
			t = self.get_task(tid)
			if t and t.executor_name == "RetrievalExecutor" and t.status == "completed" and isinstance(t.result, list):
				for item in t.result:
					if isinstance(item, dict) and "content" in item: refs.append({"content": item["content"],
																				  "document_name": item.get("metadata",
																											{}).get(
																					  "source_name", f"æº_{t.id}"),
																				  "id": item.get("metadata", {}).get(
																					  "id", f"ref_{t.id}_{len(refs)}")})
		return refs


# --- Executors (ä¸ä¹‹å‰ç›¸åŒï¼ŒDeduceExecutor è¿”å›æ–°ç»“æ„ä½“) ---
class ExecutorError(Exception): pass


class ExecutorBase(ABC):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: Optional[OpenAIChatLLM] = None):
		self.llm_client = llm_client

	@abstractmethod
	async def execute(self, task: Task, context: ContextManager) -> Any:
		pass

	@abstractmethod
	def get_schema(self) -> Dict[str, Any]:
		pass

	def _resolve_references(self, data_template: Any,
							context: ContextManager) -> Any:  # (ä¸ä¹‹å‰ç›¸åŒï¼Œå·²åŒ…å«å¯¹DeduceExecutorç»“æ„åŒ–ç»“æœçš„answer_summaryçš„å¼•ç”¨)
		if isinstance(data_template, str):
			def replace_match(match):
				ref_full = match.group(1).strip();
				task_id_ref, attr_ref = ref_full.split('.', 1) if '.' in ref_full else (ref_full, "result")
				ref_task = context.get_task(task_id_ref)
				if ref_task and ref_task.status == "completed":
					target_obj = ref_task.result
					if attr_ref == "result":
						if isinstance(target_obj, dict) and "answer_summary" in target_obj: return str(
							target_obj["answer_summary"])
						if isinstance(target_obj, list): return "\n".join(
							[f"- {item}" for item in map(str, target_obj)])
						return str(target_obj)
					elif isinstance(target_obj, dict) and attr_ref in target_obj:
						return str(target_obj[attr_ref])
					else:
						print(f"  [Exec Warn] Unsupported attr '{attr_ref}' or not found for {{ {ref_full} }}.")
				else:
					print(
						f"  [Exec Warn] Cannot resolve ref {{ {ref_full} }}. Task '{task_id_ref}' status: {ref_task.status if ref_task else 'not_found'}.")
				return f"{{å¼•ç”¨é”™è¯¯: {ref_full}}}"

			return re.sub(r"\{\{([\w_\-\d\.]+)\}\}", replace_match, data_template)
		elif isinstance(data_template, dict):
			return {k: self._resolve_references(v, context) for k, v in data_template.items()}
		elif isinstance(data_template, list):
			return [self._resolve_references(item, context) for item in data_template]
		return data_template


class RetrievalExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, kb: ChromaKnowledgeBase):
		super().__init__(); self.kb = kb

	async def execute(self, task: Task, context: ContextManager) -> List[Dict[str, Any]]:
		logic_input = task.logic_input;
		query_to_retrieve = logic_input.get("query");
		filter_param = logic_input.get("filter")
		if not query_to_retrieve or not isinstance(query_to_retrieve, str): raise ExecutorError(
			"RetrievalExecutor: 'query' (string) is required.")
		resolved_query = self._resolve_references(query_to_retrieve, context);
		actual_filter = None
		if filter_param and isinstance(filter_param, dict) and filter_param:
			actual_filter = filter_param
		elif filter_param:
			print(f"  [RetrievalExec Warn] Invalid filter for task '{task.id}': {filter_param}. Ignoring.")
		task.thought = (task.thought or "") + f"KBæ£€ç´¢æŸ¥è¯¢: '{resolved_query}', è¿‡æ»¤å™¨: {actual_filter}.".strip()
		retrieved_docs_with_meta = self.kb.retrieve(resolved_query, top_k=3, filter_dict=actual_filter)
		if not retrieved_docs_with_meta: task.thought += "\næœªæ£€ç´¢åˆ°ä»»ä½•åŒ¹é…æ–‡æ¡£."; return []
		task.thought += f"\næ£€ç´¢åˆ° {len(retrieved_docs_with_meta)} ä¸ªæ–‡æ¡£å¯¹è±¡.";
		return retrieved_docs_with_meta

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "RetrievalExecutor",
				"description": "ä»å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚å¯æŒ‡å®šå…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚",
				"logic_input_schema": {"query": "string (æ£€ç´¢æŸ¥è¯¢è¯­å¥, å¯å¼•ç”¨ {{task_id.result}})",
									   "filter": "dict (å¯é€‰, ChromaDBå…ƒæ•°æ®è¿‡æ»¤å™¨)"}}


class DeduceExecutorOutput(TypedDict, total=False):  # (ä¸ä¹‹å‰ç›¸åŒ)
	answer_summary: str;
	is_sufficient: bool;
	new_questions_or_entities: List[str];
	raw_llm_response: str


class DeduceExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨æ–°çš„DeducePrompt)
	def __init__(self, llm_client: OpenAIChatLLM, prompt_template: DeducePrompt,
				 specialized_prompts: Optional[Dict[str, BasePrompt]] = None):
		super().__init__(llm_client);
		self.default_prompt_template = prompt_template;
		self.specialized_prompts = specialized_prompts or {}

	async def execute(self, task: Task, context: ContextManager) -> DeduceExecutorOutput:
		logic_input = task.logic_input;
		goal = logic_input.get("reasoning_goal");
		raw_ctx_data = logic_input.get("context_data");
		op_type = logic_input.get("operation_type")
		if not goal or not isinstance(goal, str) or raw_ctx_data is None: raise ExecutorError(
			"DeduceExecutor: 'reasoning_goal'(str) & 'context_data' required.")
		res_ctx_data = self._resolve_references(raw_ctx_data, context)
		ctx_data_str = json.dumps(res_ctx_data, ensure_ascii=False, indent=2) if isinstance(res_ctx_data,
																							(list, dict)) else str(
			res_ctx_data)
		prompt_to_use = self.specialized_prompts.get(op_type,
													 self.default_prompt_template) if op_type else self.default_prompt_template
		sys_prompt = getattr(prompt_to_use, "SYSTEM_PROMPT", DeducePrompt.SYSTEM_PROMPT)
		if op_type and prompt_to_use != self.default_prompt_template: print(
			f"  [DeduceExecutor] Using specialized prompt for op_type: {op_type}")
		prompt_str = prompt_to_use.format(reasoning_goal=goal, context_data=ctx_data_str)
		task.thought = (
								   task.thought or "") + f"æ¼”ç»ç›®æ ‡({op_type or 'default'}): {goal}. ä¸Šä¸‹æ–‡(æ‘˜è¦): {ctx_data_str[:100]}...".strip()
		resp_json = await self.llm_client.generate_structured_json(prompt_str, system_prompt_str=sys_prompt,
																   temperature=0.0)
		summary = resp_json.get("answer_summary", "æœªèƒ½ä»LLMå“åº”ä¸­è§£æå‡ºç­”æ¡ˆæ€»ç»“ã€‚");
		is_suff = bool(resp_json.get("is_sufficient", False))
		new_qs_raw = resp_json.get("new_questions_or_entities", []);
		new_qs = [str(item).strip() for item in new_qs_raw if
				  isinstance(item, str) and str(item).strip()] if isinstance(new_qs_raw, list) else [
			str(new_qs_raw).strip()] if isinstance(new_qs_raw, str) and str(new_qs_raw).strip() else []
		task.thought += f"\nLLMæ¼”ç»å“åº”(ç»“æ„åŒ–): sufficient={is_suff}, new_qs={new_qs}, summary_preview='{summary[:50]}...'"
		output: DeduceExecutorOutput = {"answer_summary": summary, "is_sufficient": is_suff,
										"new_questions_or_entities": new_qs,
										"raw_llm_response": json.dumps(resp_json, ensure_ascii=False)};
		return output

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "DeduceExecutor",
				"description": "åŸºäºä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ã€æ€»ç»“ã€åˆ¤æ–­æˆ–æŠ½å–ã€‚ä¼šåˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……åˆ†å¹¶ç»™å‡ºä¸‹ä¸€æ­¥æŸ¥è¯¢å»ºè®®ã€‚",
				"logic_input_schema": {"reasoning_goal": "string (å…·ä½“æ¨ç†ç›®æ ‡)",
									   "context_data": "any (æ¨ç†æ‰€éœ€ä¸Šä¸‹æ–‡,å¯å¼•ç”¨ {{task_id.result}})",
									   "operation_type": "string (å¯é€‰, å¦‚ summarize, extract_info)"}}


class CodeExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: CodeExecutionPrompt):
		super().__init__(llm_client); self.prompt_template = prompt

	async def execute(self, task: Task, context: ContextManager) -> str:
		logic_input = task.logic_input;
		code_gen_prompt = logic_input.get("code_generation_prompt");
		rel_data = logic_input.get("relevant_data", "")
		if not code_gen_prompt or not isinstance(code_gen_prompt, str): raise ExecutorError(
			"CodeExecutor: 'code_generation_prompt' required.")
		res_code_prompt = self._resolve_references(code_gen_prompt, context);
		res_rel_data = self._resolve_references(rel_data, context)
		rel_data_prompt = json.dumps(res_rel_data, ensure_ascii=False, indent=2) if isinstance(res_rel_data,
																							   (list, dict)) else str(
			res_rel_data) if res_rel_data else ""
		llm_prompt = self.prompt_template.format(code_generation_prompt=res_code_prompt, relevant_data=rel_data_prompt)
		task.thought = (task.thought or "") + f"ä»£ç ç”Ÿæˆç›®æ ‡: {res_code_prompt}. ".strip()
		code_block = await self.llm_client.generate(llm_prompt, system_prompt_str=CodeExecutionPrompt.SYSTEM_PROMPT)
		code = code_block.strip();
		if code.startswith("```python"): code = code[9:]
		if code.startswith("```"): code = code[3:]
		if code.endswith("```"): code = code[:-3]
		code = code.strip()
		if not code: task.thought += "\nLLMæœªèƒ½ç”Ÿæˆä»£ç ."; raise ExecutorError("CodeExecutor: LLM no code.")
		task.thought += f"\nç”Ÿæˆçš„ä»£ç :\n---\n{code}\n---"
		try:
			with open("t.py", "w", encoding="utf-8") as f:
				f.write(code)
			p = await asyncio.to_thread(subprocess.run, [sys.executable, "t.py"], capture_output=True, text=True,
										timeout=10, check=False)
			if p.returncode != 0: task.thought += f"\nä»£ç é”™è¯¯ç {p.returncode}. stderr:\n{p.stderr or 'æ— '}"; raise ExecutorError(
				f"Code exec err {p.returncode}:\n{p.stderr or 'æ— '}")
			out = p.stdout.strip();
			task.thought += f"\nä»£ç è¾“å‡º: {out}";
			return out
		except subprocess.TimeoutExpired:
			task.thought += "\nä»£ç è¶…æ—¶."; raise ExecutorError("Code timeout.")
		except Exception as e:
			task.thought += f"\nä»£ç æœ¬åœ°æ‰§è¡Œé”™è¯¯: {e}"; raise ExecutorError(f"Code local exec err: {e}")
		finally:
			if os.path.exists("t.py"): os.remove("t.py")

	def get_schema(self) -> Dict[str, Any]:
		return {"name": "CodeExecutor",
				"description": "ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ã€‚ä»£ç åº”print()ç»“æœã€‚è¾“å…¥å¯å¼•ç”¨ {{task_id.result}}ã€‚",
				"logic_input_schema": {"code_generation_prompt": "string (ä»£ç ç”ŸæˆæŒ‡ä»¤)",
									   "relevant_data": "any (å¯é€‰, ä»£ç æ‰€éœ€æ•°æ®)"}}


class FinishExecutor(ExecutorBase):  # (ä¸ä¹‹å‰ç›¸åŒ)
	async def execute(self, task: Task,
					  context: ContextManager) -> str: task.thought = "æ”¶åˆ°FinishæŒ‡ä»¤ï¼Œæµç¨‹ç»“æŸã€‚"; print(
		f"  [FinishExecutor] Task {task.id} executed."); return "å·²å®Œæˆæ‰€æœ‰å¿…è¦æ­¥éª¤ã€‚"

	def get_schema(self) -> Dict[str, Any]: return {"name": "FinishAction",
													"description": "å½“é—®é¢˜å·²è§£å†³æˆ–æ— æ³•ç»§ç»­æ—¶è°ƒç”¨æ­¤åŠ¨ä½œç»“æŸè§„åˆ’ã€‚",
													"logic_input_schema": {"reason": "string (å¯é€‰ï¼Œç»“æŸåŸå› )"}}


# --- Planner (ä¸ä¹‹å‰ç›¸åŒ) ---
class Planner:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM, prompt: PlannerPrompt):
		self.llm_client = llm_client; self.prompt_template = prompt

	async def plan_next_steps(self, user_query: str, context: ContextManager, available_executors: List[Dict]) -> Tuple[
		str, str, List[Dict]]:
		exec_desc_parts = [
			f"  - åç§°: \"{s['name']}\"\n    æè¿°: \"{s['description']}\"\n    è¾“å…¥å‚æ•°æ¨¡å¼ (logic_input_schema): {json.dumps(s.get('logic_input_schema', 'N/A'), ensure_ascii=False)}"
			for s in available_executors]
		exec_desc = "\n".join(exec_desc_parts);
		history_str = context.get_task_history_for_prompt()
		user_prompt_str = self.prompt_template.format(user_query=user_query, available_executors_description=exec_desc,
													  task_history=history_str)
		response_json = await self.llm_client.generate_structured_json(user_prompt_str,
																	   system_prompt_str=PlannerPrompt.SYSTEM_PROMPT,
																	   temperature=0.01)  # Very low temp for planner
		plan_status = response_json.get("plan_status", "error");
		final_thought = response_json.get("final_thought", "LLMæœªèƒ½æä¾›è§„åˆ’æ€è€ƒã€‚");
		next_steps_data = response_json.get("next_steps", [])
		if not isinstance(next_steps_data, list): print(
			f"  [Planner Err] LLM 'next_steps' not list. Got: {next_steps_data}. Assuming none."); next_steps_data = []
		valid_steps = [td for td in next_steps_data if isinstance(td, dict) and all(
			k in td for k in ["id", "executor_name", "task_description", "logic_input"])]
		if len(valid_steps) != len(next_steps_data): print(
			f"  [Planner Warn] Some planned steps were invalid and skipped by planner output validation.")
		return plan_status, final_thought, valid_steps


# --- AnswerGenerator (ä¸ä¹‹å‰ç›¸åŒ) ---
class AnswerGenerator:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, llm_client: OpenAIChatLLM,
				 prompt: UserProvidedReferGeneratorPrompt): self.llm_client = llm_client; self.prompt_template = prompt

	async def generate_final_answer(self, user_query: str, context: ContextManager) -> str:
		summary = context.get_summary_for_generator();
		refs = context.collect_retrieved_references_for_generator()
		prompt_str = self.prompt_template.format(summary_of_executed_steps=summary, user_query=user_query,
												 retrieved_references=refs)
		return await self.llm_client.generate(prompt_str, temperature=0.2)  # Slightly higher temp for generation


# --- Pipeline (ä¸ä¹‹å‰ç›¸åŒï¼Œä½¿ç”¨è¿­ä»£é€»è¾‘) ---
class IterativePipeline:  # (ä¸ä¹‹å‰ç›¸åŒ)
	def __init__(self, planner: Planner, executors: Dict[str, ExecutorBase], generator: AnswerGenerator,
				 max_iterations: int = 5):  # Default to 5 iterations
		self.planner = planner;
		self.executors = executors;
		self.generator = generator;
		self.max_iterations = max_iterations

	async def _execute_task_dag_segment(self, tasks_data: List[Dict], ctx: ContextManager,
										iter_num: int) -> bool:  # (ä¸ä¹‹å‰ç›¸åŒ)
		if not tasks_data: return True
		prefix = re.sub(r'[^\w\s-]', '', ctx.user_query[:10]).strip().replace(' ', '_') or "q"
		added_ids = [ctx.add_task_from_planner(td, prefix, iter_num, i).id for i, td in enumerate(tasks_data)]
		cache = set();
		success = True
		for tid in added_ids:
			task = ctx.get_task(tid)
			if task and task.status == "pending":
				await self._execute_task_with_dependencies(tid, ctx, cache)
				if ctx.get_task(tid).status != "completed": success = False
		return success

	async def _execute_task_with_dependencies(self, task_id: str, ctx: ContextManager, cache: set):  # (ä¸ä¹‹å‰ç›¸åŒ)
		task = ctx.get_task(task_id)
		if not task: print(f"  [Pipe Err] Task {task_id} def not found for exec."); return
		if task.status != "pending": return
		cache.add(task_id)
		for dep_id in task.dependencies:
			dep_task = ctx.get_task(dep_id)
			if not dep_task: emsg = f"T {task.id} undef dep ID: {dep_id}. Failed."; ctx.update_task_status(task.id,
																										   "failed",
																										   result=emsg,
																										   thought=emsg); print(
				f"  [Pipe Err] {emsg}"); return
			if dep_task.status == "pending": await self._execute_task_with_dependencies(dep_id, ctx, cache)
		deps_met = True
		if task.dependencies:
			for dep_id in task.dependencies:
				dep_task = ctx.get_task(dep_id)
				if not dep_task or dep_task.status != "completed":
					deps_met = False;
					ts = (
									 task.thought or "") + f"Dep {dep_id} not met (status: {dep_task.status if dep_task else 'N/A'}). Skip."
					ctx.update_task_status(task.id, "skipped", thought=ts);
					print(f"  [Pipe] Task {task.id} skip, dep {dep_id} not met.");
					break
			if not deps_met: return
		if task.status != "pending": return
		executor = self.executors.get(task.executor_name)
		if not executor: emsg = f"Exec '{task.executor_name}' not found for task '{task.task_description}'."; ctx.update_task_status(
			task.id, "failed", result=emsg, thought=emsg); print(f"  [Pipe Err] {emsg}"); return
		ctx.update_task_status(task.id, "running", thought=f"Start: {task.task_description}")
		print(f"\nâ–¶ï¸ Iter Exec Task: {task.id} - \"{task.task_description}\" ({task.executor_name})")
		try:
			result = await executor.execute(task, ctx)
			ctx.update_task_status(task.id, "completed", result=result, thought=task.thought)
		except ExecutorError as e:
			emsg = f"ExecErr T {task.id}: {e}"; ft = (task.thought or "") + f"\nExecErr: {e}"; ctx.update_task_status(
				task.id, "failed", result=emsg, thought=ft); print(f"ğŸ›‘ {emsg}")
		except Exception as e:
			emsg = f"UnexpectedErr T {task.id}: {e}"; import traceback; tb = traceback.format_exc(); print(
				f"ğŸ›‘ {emsg}\n{tb}"); ft = (task.thought or "") + f"\nUnexpectedErr: {e}"; ctx.update_task_status(task.id,
																												"failed",
																												result=emsg,
																												thought=ft)

	async def run(self, user_query: str) -> str:  # (ä¸ä¹‹å‰ç›¸åŒ)
		print(f"\nğŸš€ IterativePipeline for query: \"{user_query}\"")
		ctx = ContextManager(user_query);
		schemas = [ex.get_schema() for ex in self.executors.values()]
		final_ans = "å¤„ç†ä¸­é‡åˆ°é—®é¢˜ï¼Œæœªèƒ½å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚";
		planner_overall_thought = "";
		current_plan_status = "requires_more_steps"
		final_iter_num_for_log = 0
		for i_iter in range(self.max_iterations):
			final_iter_num_for_log = i_iter + 1
			print(f"\n--- Iteration {final_iter_num_for_log} / {self.max_iterations} ---")
			print(f"ğŸ“ Planning phase (Iteration {final_iter_num_for_log})...")
			try:
				status, thought, steps_data = await self.planner.plan_next_steps(user_query, ctx, schemas)
				planner_overall_thought += f"\nIter {final_iter_num_for_log} Planner Thought: {thought}"
				current_plan_status = status
				print(f"  [Planner Out] Status: {current_plan_status}, Thought: {thought}")
				if steps_data:
					print(
						f"  [Planner Out] Next Steps Planned ({len(steps_data)}): {[s.get('task_description', 'N/A') for s in steps_data]}")
				else:
					print("  [Planner Out] No new steps planned.")
				if current_plan_status == "finished": print("  [Pipe] Planner: finished."); break
				if current_plan_status == "cannot_proceed": print(
					"  [Pipe] Planner: cannot_proceed."); final_ans = f"æ— æ³•ç»§ç»­ï¼š{thought}"; break
				if not steps_data:
					if i_iter > 0:
						print("  [Pipe] No new steps & not finished explicitly. Assuming completion."); break
					else:
						print(
							"  [Pipe Err] Planner: no initial steps."); return f"æ— æ³•åˆ¶å®šåˆæ­¥è®¡åˆ’ã€‚Planner Thought: {thought}"
			except Exception as e:
				print(f"  [Pipe Err] Planning error: {e}"); import \
					traceback; traceback.print_exc(); final_ans = f"è§„åˆ’é˜¶æ®µæ„å¤–é”™è¯¯ï¼š{e}"; break
			print(f"\nâš™ï¸ Execution phase (Iteration {final_iter_num_for_log})...")
			await self._execute_task_dag_segment(steps_data, ctx, final_iter_num_for_log)
			finish_executed = any(
				t.executor_name == "FinishAction" and t.status == "completed" for t in ctx.tasks.values() if
				t.id in [s.get("id", "") for s in steps_data])  # Check ID properly
			if finish_executed: print(
				f"  [Pipe] FinishAction completed. Ending iterations."); current_plan_status = "finished"; break
		print(f"\nğŸ’¬ Generation phase after {final_iter_num_for_log} iteration(s)...")
		try:
			if current_plan_status == "cannot_proceed" and "æ— æ³•ç»§ç»­" in final_ans:
				pass
			else:
				final_ans = await self.generator.generate_final_answer(user_query, ctx)
		except Exception as e:
			print(f"  [Pipe Err] Generation error: {e}"); import \
				traceback; traceback.print_exc(); final_ans = f"ç”Ÿæˆç­”æ¡ˆæ„å¤–é”™è¯¯ï¼š{e}"
		print(f"\nğŸ’¡ Final Answer: {final_ans}");
		return final_ans


# --- ä¸»ç¨‹åºå…¥å£ ---
async def run_main_logic_with_user_data_deep_recursive_v2():  # Renamed main function
	# --- LLM and Embedding Setup ---
	api_key = 'sk-af4423da370c478abaf68b056f547c6e'
	base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
	model_name = os.getenv("LLM_MODEL_NAME", "qwen-plus")
	embedding_api_key = 'sk-af4423da370c478abaf68b056f547c6e'

	if not api_key or not embedding_api_key:
		print("é”™è¯¯ï¼šè¯·ç¡®ä¿ OPENAI_API_KEY å’Œ DASHSCOPE_API_KEY_FOR_EMBEDDING (æˆ–å…¶å›é€€ OPENAI_API_KEY) å·²è®¾ç½®ã€‚")
		return
	llm_client = OpenAIChatLLM(model_name=model_name, api_key=api_key, base_url=base_url)
	try:
		embedding_function = QwenEmbeddingFunction(api_key=embedding_api_key)
	except Exception as e:
		print(f"  [Embedding Error] QwenEmbeddingFunction åˆå§‹åŒ–å¤±è´¥: {e}"); return

	# --- REFINED Knowledge Base Data for Multi-Hop Retrieval Test ---
	initial_user_docs_as_dicts = [
		{
			"page_content": "ã€æ­£æŸ´èƒ¡é¥®é¢—ç²’ã€‘è¯å“è¯´æ˜ä¹¦ï¼ˆæ‘˜è¦ï¼‰\nã€æ£€æŸ¥ã€‘åº”ç¬¦åˆé¢—ç²’å‰‚é¡¹ä¸‹æœ‰å…³çš„å„é¡¹è§„å®šï¼ˆè¯¦è§ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104ï¼‰ã€‚å…¶ä½™æŒ‰å“ç§æ ‡å‡†æ‰§è¡Œã€‚",
			"metadata": {"id": "zchy_spec_main_v3", "source_name": "æ­£æŸ´èƒ¡é¥®é¢—ç²’è¯´æ˜ä¹¦æ‘˜è¦"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104 - é¢—ç²’å‰‚ï¼ˆæ¦‚è¿°ï¼‰\næœ¬é€šåˆ™ä¸ºé¢—ç²’å‰‚çš„é€šç”¨è´¨é‡æ§åˆ¶è¦æ±‚ã€‚å…·ä½“æ£€æŸ¥é¡¹ç›®åŒ…æ‹¬ï¼šã€æ€§çŠ¶ã€‘ã€ã€é‰´åˆ«ã€‘ã€ã€æ£€æŸ¥ã€‘ï¼ˆå¦‚ç²’åº¦ã€æ°´åˆ†ã€æº¶åŒ–æ€§ã€è£…é‡å·®å¼‚ã€å¾®ç”Ÿç‰©é™åº¦ç­‰ï¼‰ã€ã€å«é‡æµ‹å®šã€‘ç­‰ã€‚\nå…³äºã€å¾®ç”Ÿç‰©é™åº¦ã€‘ï¼Œé¢—ç²’å‰‚åº”ç¬¦åˆç°è¡Œç‰ˆã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1105ï¼ˆéæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ï¼šå¾®ç”Ÿç‰©è®¡æ•°æ³•ï¼‰ã€é€šåˆ™1106ï¼ˆéæ— èŒäº§å“å¾®ç”Ÿç‰©é™åº¦æ£€æŸ¥ï¼šæ§åˆ¶èŒæ£€æŸ¥æ³•ï¼‰å’Œé€šåˆ™1107ï¼ˆéæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ï¼‰çš„ç›¸å…³è§„å®šã€‚æœ¬é€šåˆ™0104ä¸è¯¦è¿°è¿™äº›å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†çš„å…·ä½“é™å€¼ã€‚",
			"metadata": {"id": "tg0104_overview_v3", "source_name": "è¯å…¸é€šåˆ™0104æ¦‚è¿°"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™1107 - éæ— èŒè¯å“å¾®ç”Ÿç‰©é™åº¦æ ‡å‡†ï¼ˆèŠ‚é€‰ï¼‰\næœ¬æ ‡å‡†è§„å®šäº†å„ç±»éæ— èŒè¯å“æ‰€éœ€æ§åˆ¶çš„å¾®ç”Ÿç‰©é™åº¦ã€‚\nå¯¹äºå£æœå›ºä½“åˆ¶å‰‚ï¼ˆå¦‚é¢—ç²’å‰‚ï¼‰ï¼š\n1. éœ€æ°§èŒæ€»æ•°ï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—è¿‡1000 cfuã€‚\n2. éœ‰èŒå’Œé…µæ¯èŒæ€»æ•°ï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—è¿‡100 cfuã€‚\n3. æ§åˆ¶èŒï¼šæ¯1gï¼ˆæˆ–1mlï¼‰ä¸å¾—æ£€å‡ºå¤§è‚ åŸƒå¸ŒèŒï¼›å¯¹äºå«åŠ¨ç‰©è„å™¨ã€ç»„ç»‡æˆ–è¡€æ¶²æˆåˆ†çš„åˆ¶å‰‚ï¼Œæ¯10gï¼ˆæˆ–10mlï¼‰ä¸å¾—æ£€å‡ºæ²™é—¨èŒã€‚",
			"metadata": {"id": "tg1107_details_v3", "source_name": "è¯å…¸é€šåˆ™1107-å¾®ç”Ÿç‰©é™åº¦ç»†èŠ‚"}},
		{
			"page_content": "ã€Šä¸­å›½è¯å…¸ã€‹é€šåˆ™0104 - é¢—ç²’å‰‚ï¼ˆç²’åº¦ä¸æ°´åˆ†ç»†èŠ‚ï¼‰\nã€ç²’åº¦ã€‘ï¼ˆé€šåˆ™0982ç¬¬äºŒæ³•ï¼‰ä¸èƒ½é€šè¿‡ä¸€å·ç­›ï¼ˆ2.00mmï¼‰ä¸èƒ½é€šè¿‡äº”å·ç­›ï¼ˆ0.250mmï¼‰çš„è¯ç²‰æ€»å’Œä¸å¾—è¶…è¿‡æ€»é‡é‡çš„15ï¼…ã€‚\nã€æ°´åˆ†ã€‘ï¼ˆé€šåˆ™0832ç¬¬ä¸€æ³•ï¼‰ä¸­è¯é¢—ç²’å‰‚ä¸å¾—è¿‡8.0ï¼…ã€‚",
			"metadata": {"id": "tg0104_sizewater_v3", "source_name": "è¯å…¸é€šåˆ™0104-ç²’åº¦ä¸æ°´åˆ†ç»†èŠ‚"}}
	]
	initial_langchain_docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in
							  initial_user_docs_as_dicts]
	from md2Document import read_md_file, create_document_from_md
	initial_langchain_docs = []
	folder_path = r'D:\Master\llm\database\kag\æµ‹è¯•æ•°æ®é›†'  # Windowsè·¯å¾„
	# è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶
	files = os.listdir(folder_path)
	# ç­›é€‰å‡ºæ‰€æœ‰ .md æ–‡ä»¶
	md_files = [file for file in files if file.endswith('.md')]
	# åˆ›å»ºDocumentå¯¹è±¡åˆ—è¡¨

	for file in md_files:
		file_path = os.path.join(folder_path, file)  # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
		document = create_document_from_md(file_path)  # åˆ›å»ºDocumentå¯¹è±¡
		initial_langchain_docs.append(document)
	# md_folder_path = r'D:\Master\llm\database\kag\æµ‹è¯•æ•°æ®é›†'
	# if os.path.exists(md_folder_path): # ... (æ‚¨çš„ md åŠ è½½é€»è¾‘) ...
	# else: print(f"  [Warning] Markdownæ–‡ä»¶å¤¹ {md_folder_path} ä¸å­˜åœ¨ã€‚")

	try:
		chroma_kb = ChromaKnowledgeBase(initial_documents=initial_langchain_docs, embedding_function=embedding_function,
										force_rebuild=False,
										persist_directory = 'chroma_db_kag_recursive_2')
	except Exception as e:
		print(f"åˆ›å»ºChromaçŸ¥è¯†åº“å¤±è´¥: {e}"); import traceback; traceback.print_exc(); return

	# --- Initialize Components (using refined prompts) ---
	planner_prompt = PlannerPrompt()
	deduce_prompt_template = DeducePrompt()
	code_exec_prompt = CodeExecutionPrompt()
	refer_generator_prompt = UserProvidedReferGeneratorPrompt(language="zh")

	retrieval_executor = RetrievalExecutor(kb=chroma_kb)
	deduce_executor = DeduceExecutor(llm_client, deduce_prompt_template)
	code_executor = CodeExecutor(llm_client, code_exec_prompt)
	finish_executor = FinishExecutor()

	executors_map = {
		"RetrievalExecutor": retrieval_executor, "DeduceExecutor": deduce_executor,
		"CodeExecutor": code_executor, "FinishAction": finish_executor
	}
	planner = Planner(llm_client, planner_prompt)
	generator = AnswerGenerator(llm_client, refer_generator_prompt)
	pipeline = IterativePipeline(planner, executors_map, generator, max_iterations=8)  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥å…è®¸æ›´æ·±çš„é€’å½’

	# --- Run Query Designed to Force Multi-Hop Retrieval ---
	user_query_to_run = "ä½ æ˜¯è°"

	print(f"\nğŸš€ Running DEEP RECURSIVE V2 query: \"{user_query_to_run}\"")
	final_answer = await pipeline.run(user_query_to_run)
	print(f"\nğŸğŸğŸğŸğŸ DEEP RECURSIVE V2 FINAL ANSWER (for query: '{user_query_to_run}') ğŸğŸğŸğŸğŸ\n{final_answer}")


if __name__ == "__main__":
	print("å¼€å§‹æ‰§è¡Œâ€œæ·±åº¦ç±»é€’å½’V2â€ä¼˜åŒ–ç‰ˆä¸»é€»è¾‘...")
	# ... (ç¯å¢ƒå˜é‡å’Œä¾èµ–æç¤º - ä¸ä¹‹å‰ç›¸åŒ) ...

	asyncio.run(run_main_logic_with_user_data_deep_recursive_v2())
