"""
ä¸»ç¨‹åºå…¥å£æ¨¡å—
åŒ…å«ç³»ç»Ÿçš„ä¸»è¦è¿è¡Œé€»è¾‘
"""
import asyncio
import os
from typing import List
from langchain_core.documents import Document

from config.settings import (
    API_KEY, EMBEDDING_API_KEY, MODEL_NAME, BASE_URL,
    DOCUMENT_FOLDER_PATH, MAX_ITERATIONS,CHROMA_PERSIST_DIRECTORY,EMBEDDING_BASE_URL,EMBEDDING_MODEL_NAME
)
from model.client import OpenAIChatLLM
from knowledge.knowledge_base import ChromaKnowledgeBase
from prompts.templates import (
    PlannerPrompt, DeducePrompt, CodeExecutionPrompt, UserProvidedReferGeneratorPrompt
)
from executors import (
    RetrievalExecutor, DeduceExecutor, CodeExecutor, FinishExecutor
)
from planning.planner import Planner
from generation.answer_generator import AnswerGenerator
from pipeline.iterative_pipeline import IterativePipeline
from utils.document_loader import load_documents_from_folder, create_default_test_documents

try:
    from model.embedding import QwenEmbeddingFunction
except ImportError:
    print("[Warning] tongyiembedding module not found. Embedding may not work.")
    QwenEmbeddingFunction = None


class KAGSystem:
    """KAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        self.persist_directory = CHROMA_PERSIST_DIRECTORY
        self.llm_client = None
        self.kb = None
        self.pipeline = None
    
    async def initialize(self, force_rebuild_kb: bool = False):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("ğŸ”§ åˆå§‹åŒ–KAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        api_key = API_KEY
        embedding_api_key = EMBEDDING_API_KEY

        model_name = MODEL_NAME
        embedding_model_name = EMBEDDING_MODEL_NAME
        
        base_url = BASE_URL
        embedding_base_url = EMBEDDING_BASE_URL
        
        if not api_key or not embedding_api_key:
            raise ValueError("é”™è¯¯ï¼šè¯·ç¡®ä¿ API keys å·²æ­£ç¡®è®¾ç½®ã€‚")
        
        self.llm_client = OpenAIChatLLM(
            model_name=model_name,
            api_key=api_key,
            base_url=base_url
        )
        
        # åˆå§‹åŒ–åµŒå…¥å‡½æ•°
        if QwenEmbeddingFunction is None:
            raise ImportError("QwenEmbeddingFunction not available. Please install tongyiembedding.")
        
        try:
            embedding_function = QwenEmbeddingFunction(api_key=embedding_api_key,base_url=embedding_base_url, model=embedding_model_name)
        except Exception as e:
            print(f"  [Embedding Error] QwenEmbeddingFunction åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åŠ è½½æ–‡æ¡£
        print("ğŸ“š åŠ è½½æ–‡æ¡£...")
        documents = self._load_documents()
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        print("ğŸ—„ï¸ åˆå§‹åŒ–çŸ¥è¯†åº“...")
        try:
            self.kb = ChromaKnowledgeBase(
                initial_documents=documents,
                embedding_function=embedding_function,
                force_rebuild=force_rebuild_kb,
                persist_directory=self.persist_directory
            )
        except Exception as e:
            print(f"åˆ›å»ºChromaçŸ¥è¯†åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # åˆå§‹åŒ–ç»„ä»¶
        print("âš™ï¸ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        self._initialize_components()
        
        print("âœ… KAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def _load_documents(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        documents = []
        
        # å°è¯•ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£
        if os.path.exists(DOCUMENT_FOLDER_PATH):
            documents = load_documents_from_folder(DOCUMENT_FOLDER_PATH)
        
        # å¦‚æœæ²¡æœ‰åŠ è½½åˆ°æ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æ¡£
        if not documents:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æ¡£")
            documents = create_default_test_documents()
        
        print(f"ğŸ“„ å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        # åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿
        planner_prompt = PlannerPrompt()
        deduce_prompt_template = DeducePrompt()
        code_exec_prompt = CodeExecutionPrompt()
        refer_generator_prompt = UserProvidedReferGeneratorPrompt(language="zh")
        
        # åˆå§‹åŒ–æ‰§è¡Œå™¨
        retrieval_executor = RetrievalExecutor(kb=self.kb)
        deduce_executor = DeduceExecutor(self.llm_client, deduce_prompt_template)
        code_executor = CodeExecutor(self.llm_client, code_exec_prompt)
        finish_executor = FinishExecutor()
        
        executors_map = {
            "RetrievalExecutor": retrieval_executor,
            "DeduceExecutor": deduce_executor,
            "CodeExecutor": code_executor,
            "FinishAction": finish_executor
        }
        
        # åˆå§‹åŒ–è§„åˆ’å™¨å’Œç”Ÿæˆå™¨
        planner = Planner(self.llm_client, planner_prompt)
        generator = AnswerGenerator(self.llm_client, refer_generator_prompt)
        
        # åˆå§‹åŒ–æµæ°´çº¿
        self.pipeline = IterativePipeline(
            planner, executors_map, generator, max_iterations=MAX_ITERATIONS
        )
    
    async def query(self, user_query: str) -> str:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        if not self.pipeline:
            raise RuntimeError("ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize() æ–¹æ³•ã€‚")
        
        print(f"\nğŸš€ å¤„ç†æŸ¥è¯¢: \"{user_query}\"")
        final_answer = await self.pipeline.run(user_query)
        print(f"\nğŸ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
        return final_answer


async def run_main_example():
    """è¿è¡Œä¸»ç¨‹åºç¤ºä¾‹"""
    print("ğŸ¯ å¯åŠ¨KAGç³»ç»Ÿç¤ºä¾‹...")
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    kag_system = KAGSystem()
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    await kag_system.initialize(force_rebuild_kb=False)
    
    # è¿è¡ŒæŸ¥è¯¢
    user_query = "è´¢åŠ¡æŠ¥é”€åˆ¶åº¦è¯´æ˜ä¸­å…³äºæŠ¥é”€å›½å†…æˆ–å›½å¤–ä½å®¿è´¹çš„è§„å®šå…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ"
    final_answer = await kag_system.query(user_query)
    
    print(f"\nğŸ‰ æŸ¥è¯¢å®Œæˆï¼")
    return final_answer


if __name__ == "__main__":
    print("å¼€å§‹æ‰§è¡Œæ·±åº¦ç±»é€’å½’V2ä¼˜åŒ–ç‰ˆä¸»é€»è¾‘...")
    asyncio.run(run_main_example())
